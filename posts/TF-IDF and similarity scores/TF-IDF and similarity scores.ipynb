{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: \"TF-IDF and similarity scores \"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "author: \"kakamana\"\n",
    "date: \"2023-03-30\"\n",
    "categories: [python, datacamp, machine learning, nlp, feature engineering]\n",
    "image: \"tf-idf-1.png\"\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF and similarity scores\n",
    "\n",
    "The cosine similarity score and TF-IDF weights will be computed between two vectors in this course. Using these concepts, you will create a movie recommendation system and a TED Talk recommendation system. In addition, you will learn about word embeddings and use word vector representations to compute similarities between various Pink Floyd songs.\n",
    "\n",
    "This **TF-IDF and similarity scores** is part of [Datacamp course: Introduction to Natural Language Processing in Python] This course teaches techniques for extracting useful information from text and converting it into a format suitable for ML models. You will learn about POS tagging, named entity recognition, readability scores, n-gram and tf-idf models, and how to implement them using scikit-learn and spaCy. Additionally, you will learn how to calculate the similarity between two documents. During this process, you will be able to predict the sentiment of movie reviews and build recommenders for movies and Ted Talks. In the course of this course, you will learn how to engineer critical features from any text and solve some of the most challenging problems in data science.\n",
    "\n",
    "This is my learning experience of data science through DataCamp. These repository contributions are part of my learning journey through my graduate program masters of applied data sciences (MADS) at University Of Michigan, [DeepLearning.AI], [Coursera] & [DataCamp]. You can find my similar articles & more stories at my [medium] & [LinkedIn] profile. I am available at [kaggle] & [github blogs] & [github repos]. Thank you for your motivation, support & valuable feedback.\n",
    "\n",
    "These include projects, coursework & notebook which I learned through my data science journey. They are created for reproducible & future reference purpose only. All source code, slides or screenshot are intellactual property of respective content authors. If you find these contents beneficial, kindly consider learning subscription from [DeepLearning.AI Subscription], [Coursera], [DataCamp]\n",
    "\n",
    "\n",
    "\n",
    "[DeepLearning.AI]: https://www.deeplearning.ai\n",
    "[DeepLearning.AI Subscription]: https://www.deeplearning.ai\n",
    "[Coursera]: https://www.coursera.org\n",
    "[DataCamp]: https://www.datacamp.com\n",
    "[medium]: https://medium.com/@kamig4u\n",
    "[LinkedIn]: https://www.linkedin.com/in/asadenterprisearchitect\n",
    "[kaggle]: https://www.kaggle.com/kakamana\n",
    "[github blogs]: https://kakamana.github.io\n",
    "[github repos]: https://github.com/kakamana\n",
    "[Datacamp course: Introduction to Natural Language Processing in Python]: (https://app.datacamp.com/learn/courses/feature-engineering-for-nlp-in-python)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building tf-idf document vectors\n",
    "\n",
    "    n-gram modeling\n",
    "        Weight of dimension dependent on the frequency of the word corresponding to the dimension\n",
    "    Applications\n",
    "        Automatically detect stopwords\n",
    "        Search\n",
    "        Recommender systems\n",
    "        Better performance in predictive modeling for some cases\n",
    "    Term frequency-inverse document frequency\n",
    "        Proportional to term frequency\n",
    "        Inverse function of the number of documents in which it occurs\n",
    "        Mathematical formula\n",
    "![](tf-idf-1.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tf-idf vectors for TED talks\n",
    "\n",
    "In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          transcript  \\\n0  We're going to talk — my — a new lecture, just...   \n1  This is a representation of your brain, and yo...   \n2  It's a great honor today to share with you The...   \n3  My passions are music, technology and making t...   \n4  It used to be that if you wanted to get a comp...   \n\n                                                 url  \n0  https://www.ted.com/talks/al_seckel_says_our_b...  \n1  https://www.ted.com/talks/aaron_o_connell_maki...  \n2  https://www.ted.com/talks/carter_emmart_demos_...  \n3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n4  https://www.ted.com/talks/jeremy_howard_the_wo...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>transcript</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>We're going to talk — my — a new lecture, just...</td>\n      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This is a representation of your brain, and yo...</td>\n      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>It's a great honor today to share with you The...</td>\n      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>My passions are music, technology and making t...</td>\n      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>It used to be that if you wanted to get a comp...</td>\n      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/ted.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ted = df['transcript']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 29168)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You now know how to generate tf-idf vectors for a given corpus of text. You can use these vectors to perform predictive modeling just like we did with CountVectorizer. In the next few lessons, we will see another extremely useful application of the vectorized form of documents: generating recommendations.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nYou now know how to generate tf-idf vectors for a given corpus of text. You can use these vectors to perform predictive modeling just like we did with CountVectorizer. In the next few lessons, we will see another extremely useful application of the vectorized form of documents: generating recommendations.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cosine similarity\n",
    "\n",
    "![](tf-idf-2.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing dot product\n",
    "\n",
    "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "A = np.array([1, 3])\n",
    "B = np.array([-2, 2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cosine similarity matrix of a corpus\n",
    "\n",
    "In this exercise, you have been given a corpus, which is a list containing five sentences. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
    "\n",
    "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "corpus = ['The sun is the largest celestial body in the solar system',\n",
    "          'The solar system consists of the sun and eight revolving planets',\n",
    "          'Ra was the Egyptian Sun God',\n",
    "          'The Pyramids were the pinnacle of Egyptian architecture',\n",
    "          'The quick brown fox jumps over the lazy dog']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAs you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a plot line based recommender\n",
    "\n",
    "    Steps\n",
    "        Text preprocessing\n",
    "        Generate tf-idf vectors\n",
    "        Generate cosine-similarity matrix\n",
    "    The recommender function\n",
    "        Take a movie title, cosine similarity matrix and indices series as arguments\n",
    "        Extract pairwise cosine similarity scores for the movie\n",
    "        Sort the scores in descending order\n",
    "        Output titles corresponding to the highest scores\n",
    "        Ignore the highest similarity score (of 1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing linear_kernel and cosine_similarity\n",
    "\n",
    "In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel.\n",
    "\n",
    "We will then compare the computation times for both functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.0009968280792236328 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" % (time.time() - start))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.0009999275207519531 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" % (time.time() - start))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. (NOTE: In case, you see linear_kernel taking more time, it's because the dataset we're dealing with is extremely small and Python's time module is incapable of capture such minute time differences accurately)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNotice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. (NOTE: In case, you see linear_kernel taking more time, it's because the dataset we're dealing with is extremely small and Python's time module is incapable of capture such minute time differences accurately)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The recommender function\n",
    "\n",
    "In this exercise, we will build a recommender function get_recommendations(), as discussed in the lesson. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  Unnamed: 0       id                  title  \\\n0          0  49026.0  The Dark Knight Rises   \n1          1    414.0         Batman Forever   \n2          2    268.0                 Batman   \n3          3    364.0         Batman Returns   \n4          4    415.0         Batman & Robin   \n\n                                            overview  \\\n0  Following the death of District Attorney Harve...   \n1  The Dark Knight of Gotham City confronts a das...   \n2  The Dark Knight of Gotham City begins his war ...   \n3  Having defeated the Joker, Batman now faces th...   \n4  Along with crime-fighting partner Robin and ne...   \n\n                                             tagline  \n0                                    The Legend Ends  \n1                       Courage now, truth always...  \n2  Have you ever danced with the devil in the pal...  \n3                     The Bat, the Cat, the Penguin.  \n4             Strength. Courage. Honor. And loyalty.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>title</th>\n      <th>overview</th>\n      <th>tagline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>49026.0</td>\n      <td>The Dark Knight Rises</td>\n      <td>Following the death of District Attorney Harve...</td>\n      <td>The Legend Ends</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>414.0</td>\n      <td>Batman Forever</td>\n      <td>The Dark Knight of Gotham City confronts a das...</td>\n      <td>Courage now, truth always...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>268.0</td>\n      <td>Batman</td>\n      <td>The Dark Knight of Gotham City begins his war ...</td>\n      <td>Have you ever danced with the devil in the pal...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>364.0</td>\n      <td>Batman Returns</td>\n      <td>Having defeated the Joker, Batman now faces th...</td>\n      <td>The Bat, the Cat, the Penguin.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>415.0</td>\n      <td>Batman &amp; Robin</td>\n      <td>Along with crime-fighting partner Robin and ne...</td>\n      <td>Strength. Courage. Honor. And loyalty.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv('dataset/movie_metadata.csv').dropna()\n",
    "metadata.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot recommendation engine\n",
    "\n",
    "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies.\n",
    "\n",
    "You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
    "\n",
    "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "movie_plots = metadata['overview']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                              Batman Forever\n",
      "2                                      Batman\n",
      "8                  Batman: Under the Red Hood\n",
      "3                              Batman Returns\n",
      "9                            Batman: Year One\n",
      "10    Batman: The Dark Knight Returns, Part 1\n",
      "11    Batman: The Dark Knight Returns, Part 2\n",
      "5                Batman: Mask of the Phantasm\n",
      "7                               Batman Begins\n",
      "4                              Batman & Robin\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Generate recommendations\n",
    "print(get_recommendations(\"The Dark Knight Rises\", cosine_sim, indices))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You've just built your very first recommendation system. Notice how the recommender correctly identifies 'The Dark Knight Rises' as a Batman movie and recommends other Batman movies as a result. This sytem is, of course, very primitive and there are a host of ways in which it could be improved. One method would be to look at the cast, crew and genre in addition to the plot to generate recommendations\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nYou've just built your very first recommendation system. Notice how the recommender correctly identifies 'The Dark Knight Rises' as a Batman movie and recommends other Batman movies as a result. This sytem is, of course, very primitive and there are a host of ways in which it could be improved. One method would be to look at the cast, crew and genre in addition to the plot to generate recommendations\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TED talk recommender\n",
    "\n",
    "n this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks.\n",
    "\n",
    "You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n",
    "\n",
    "Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "            Unnamed: 0.1                                              title  \\\nUnnamed: 0                                                                    \n0                 1407.0                       10 top time-saving tech tips   \n1                 1524.0                              Who am I? Think again   \n2                 2393.0                                             \"Awoo\"   \n3                 2313.0               What I learned from 2,000 obituaries   \n4                 1633.0  Why giving away our wealth has been the most s...   \n\n                                                          url  \\\nUnnamed: 0                                                      \n0           https://www.ted.com/talks/david_pogue_10_top_t...   \n1           https://www.ted.com/talks/hetain_patel_who_am_...   \n2                https://www.ted.com/talks/sofi_tukker_awoo\\n   \n3           https://www.ted.com/talks/lux_narayan_what_i_l...   \n4           https://www.ted.com/talks/bill_and_melinda_gat...   \n\n                                                   transcript  \nUnnamed: 0                                                     \n0           I've noticed something interesting about socie...  \n1           Hetain Patel: (In Chinese)Yuyu Rau: Hi, I'm He...  \n2           (Music)Sophie Hawley-Weld: OK, you don't have ...  \n3           Joseph Keller used to jog around the Stanford ...  \n4           Chris Anderson: So, this is an interview with ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0.1</th>\n      <th>title</th>\n      <th>url</th>\n      <th>transcript</th>\n    </tr>\n    <tr>\n      <th>Unnamed: 0</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1407.0</td>\n      <td>10 top time-saving tech tips</td>\n      <td>https://www.ted.com/talks/david_pogue_10_top_t...</td>\n      <td>I've noticed something interesting about socie...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1524.0</td>\n      <td>Who am I? Think again</td>\n      <td>https://www.ted.com/talks/hetain_patel_who_am_...</td>\n      <td>Hetain Patel: (In Chinese)Yuyu Rau: Hi, I'm He...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2393.0</td>\n      <td>\"Awoo\"</td>\n      <td>https://www.ted.com/talks/sofi_tukker_awoo\\n</td>\n      <td>(Music)Sophie Hawley-Weld: OK, you don't have ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2313.0</td>\n      <td>What I learned from 2,000 obituaries</td>\n      <td>https://www.ted.com/talks/lux_narayan_what_i_l...</td>\n      <td>Joseph Keller used to jog around the Stanford ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1633.0</td>\n      <td>Why giving away our wealth has been the most s...</td>\n      <td>https://www.ted.com/talks/bill_and_melinda_gat...</td>\n      <td>Chris Anderson: So, this is an interview with ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted = pd.read_csv('dataset/ted_clean.csv', index_col=0)\n",
    "ted.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    talk_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return ted['title'].iloc[talk_indices]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "indices = pd.Series(ted.index, index=ted['title']).drop_duplicates()\n",
    "transcripts = ted['transcript']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m tfidf \u001B[38;5;241m=\u001B[39m TfidfVectorizer(stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Construct the TF-IDF matrix\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m tfidf_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mtfidf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtranscripts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Generate the cosine similarity matrix\u001B[39;00m\n\u001B[0;32m      7\u001B[0m cosine_sim \u001B[38;5;241m=\u001B[39m linear_kernel(tfidf_matrix, tfidf_matrix)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   2072\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[0;32m   2073\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[0;32m   2074\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[0;32m   2075\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[0;32m   2076\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[0;32m   2077\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[0;32m   2078\u001B[0m )\n\u001B[1;32m-> 2079\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2080\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[0;32m   2081\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[0;32m   2082\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1330\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1331\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1332\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1333\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1334\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1335\u001B[0m             )\n\u001B[0;32m   1336\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1338\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1341\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1207\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[0;32m   1208\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1209\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1210\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1211\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001B[0m, in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001B[39;00m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decoder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 106\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m analyzer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    108\u001B[0m     doc \u001B[38;5;241m=\u001B[39m analyzer(doc)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:234\u001B[0m, in \u001B[0;36m_VectorizerMixin.decode\u001B[1;34m(self, doc)\u001B[0m\n\u001B[0;32m    231\u001B[0m     doc \u001B[38;5;241m=\u001B[39m doc\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_error)\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m doc \u001B[38;5;129;01mis\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan:\n\u001B[1;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    236\u001B[0m     )\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "\u001B[1;31mValueError\u001B[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Generate recommendations\n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Beyond n-grams: word embeddings\n",
    "\n",
    "    Word embeddings\n",
    "        Mapping words into an n-dimensional vector space\n",
    "        Produced using deep learning and huge amounts of data\n",
    "        Discern how similar two words are to each other\n",
    "        Used to detect synonyms and antonyms\n",
    "        Captures complex relationships\n",
    "        Dependent on spacy model; independent of dataset you use\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dghr201\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dghr201\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.23.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (57.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dghr201\\appdata\\roaming\\python\\python39\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.5.0\n",
      "\u001B[38;5;2m[+] Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 14:08:26.663141: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-03-31 14:08:26.663196: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-31 14:08:51.185043: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2023-03-31 14:08:51.185081: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-31 14:08:51.317771: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DHR0201L\n",
      "2023-03-31 14:08:51.318117: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DHR0201L\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!spacy download en_core_web_lg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating word vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I like 0.3184410631656647\n",
      "I apples 0.1975560337305069\n",
      "I and -0.0979200005531311\n",
      "I orange 0.06804359704256058\n",
      "like I 0.3184410631656647\n",
      "like like 1.0\n",
      "like apples 0.29574331641197205\n",
      "like and 0.24359610676765442\n",
      "like orange 0.3216366171836853\n",
      "apples I 0.1975560337305069\n",
      "apples like 0.29574331641197205\n",
      "apples apples 1.0\n",
      "apples and 0.24472734332084656\n",
      "apples orange 0.5736395120620728\n",
      "and I -0.0979200005531311\n",
      "and like 0.24359610676765442\n",
      "and apples 0.24472734332084656\n",
      "and and 1.0\n",
      "and orange 0.2520448565483093\n",
      "orange I 0.06804359704256058\n",
      "orange like 0.3216366171836853\n",
      "orange apples 0.5736395120620728\n",
      "orange and 0.2520448565483093\n",
      "orange orange 1.0\n"
     ]
    }
   ],
   "source": [
    "sent = 'I like apples and orange'\n",
    "\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing similarity of Pink Floyd songs\n",
    "\n",
    "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as hopes, hey and mother respectively.\n",
    "\n",
    "Your task is to compute the pairwise similarity between mother and hopes, and mother and hey."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "with open('dataset/mother.txt', 'r') as f:\n",
    "    mother = f.read()\n",
    "\n",
    "with open('dataset/hopes.txt', 'r') as f:\n",
    "    hopes = f.read()\n",
    "\n",
    "with open('dataset/hey.txt', 'r') as f:\n",
    "    hey = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5779929666352768\n",
      "0.9465446706762218\n"
     ]
    }
   ],
   "source": [
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
