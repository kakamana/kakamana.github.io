{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: \"Going Deeper convolutions in keras\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "author: \"kakamana\"\n",
    "date: \"2023-04-08\"\n",
    "categories: [python, datacamp, machine learning, deep learning, tensorflow, keras, neural network]\n",
    "image: \"goingDeeper.png\"\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Going Deeper\n",
    "\n",
    "Convolutional neural networks gain a great deal of power when they are constructed with multiple layers (deep networks). You will learn how to stack multiple convolutional layers into a deep network in this chapter. As the network grows, you will also learn how to keep track of the number of parameters and how to control it.\n",
    "\n",
    "This **Going Deeper** is part of [Datacamp course: Image Processing with Keras in Python] Deep learning methods are used in image processing to train neural network algorithms to perform various machine learning tasks. Using convolutional neural networks (CNNs), you will be able to classify different types of objects for the analysis of images. In this four-hour course, you will be taught how to construct, train, and evaluate CNNs using Keras.\n",
    "\n",
    "It is a challenging task to turn images into data and train neural networks to classify them using deep learning. It has extensive applications in business and research ranging from helping eCommerce sites manage inventory more efficiently to assisting cancer researchers to identify dangerous melanoma quickly.\n",
    "\n",
    "In the first chapter of this course, you will learn how images can be viewed as data and how you can use Keras to train a neural network to classify objects within an image.\n",
    "\n",
    "A fundamental component of CNNs is convolutions, which will be discussed in the second chapter. Through the use of test data, you will learn how Keras CNNs operate on image data and how to train and tweak your Keras CNN. In later chapters, you will learn how to create a deep learning network in greater detail.\n",
    "\n",
    "In this course, you will learn how to track the performance of a CNN and how to improve it. By this point, you will be able to create Keras neural networks, optimize them, and visualize their responses across a variety of applications.\n",
    "\n",
    "This is my learning experience of data science through DataCamp. These repository contributions are part of my learning journey through my graduate program masters of applied data sciences (MADS) at University Of Michigan, [DeepLearning.AI], [Coursera] & [DataCamp]. You can find my similar articles & more stories at my [medium] & [LinkedIn] profile. I am available at [kaggle] & [github blogs] & [github repos]. Thank you for your motivation, support & valuable feedback.\n",
    "\n",
    "These include projects, coursework & notebook which I learned through my data science journey. They are created for reproducible & future reference purpose only. All source code, slides or screenshot are intellactual property of respective content authors. If you find these contents beneficial, kindly consider learning subscription from [DeepLearning.AI Subscription], [Coursera], [DataCamp]\n",
    "\n",
    "\n",
    "\n",
    "[DeepLearning.AI]: https://www.deeplearning.ai\n",
    "[DeepLearning.AI Subscription]: https://www.deeplearning.ai\n",
    "[Coursera]: https://www.coursera.org\n",
    "[DataCamp]: https://www.datacamp.com\n",
    "[medium]: https://medium.com/@kamig4u\n",
    "[LinkedIn]: https://www.linkedin.com/in/asadenterprisearchitect\n",
    "[kaggle]: https://www.kaggle.com/kakamana\n",
    "[github blogs]: https://kakamana.github.io\n",
    "[github repos]: https://github.com/kakamana\n",
    "[Datacamp course: Image Processing with Keras in Python]: (https://app.datacamp.com/learn/courses/image-processing-with-keras-in-python)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Going Deeper\n",
    "Creating a deep learning network\n",
    "A deep convolutional neural network is a network that has more than one layer. Each layer in a deep network receives its input from the preceding layer, with the very first layer receiving its input from the images used as training or test data.\n",
    "\n",
    "Here, you will create a network that has two convolutional layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 27, 27, 15)        75        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 5)         305       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3380)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 10143     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,523\n",
      "Trainable params: 10,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer (15 units)\n",
    "model.add(Conv2D(15, kernel_size=2, input_shape=(img_rows, img_cols, 1), activation='relu'))\n",
    "\n",
    "# Add another convolutional layer (5 units)\n",
    "model.add(Conv2D(5, kernel_size=2, activation='relu'))\n",
    "\n",
    "# Flatten and feed to output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a deep CNN to classify clothing images\n",
    "Training a deep learning model is very similar to training a single layer network. Once the model is constructed (as you have done in the previous exercise), the model needs to be compiled with the right set of parameters. Then, the model is fit by providing it with training data, as well as training labels. After training is done, the model can be evaluated on test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = train_data[(train_labels >= 0) & (train_labels < 3)][0:50].reshape(-1, 28, 28, 1)\n",
    "train_labels = train_labels[(train_labels >= 0) & (train_labels < 3)][0:50]\n",
    "train_labels = pd.get_dummies(train_labels).to_numpy()\n",
    "\n",
    "test_data = test_data[(test_labels >= 0) & (test_labels < 3)][0:10].reshape(-1, 28, 28, 1)\n",
    "test_labels = test_labels[(test_labels >= 0) & (test_labels < 3)][0:10]\n",
    "test_labels = pd.get_dummies(test_labels).to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 23:51:04.303747: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 43ms/step - loss: 27.9144 - accuracy: 0.3750 - val_loss: 3.2749 - val_accuracy: 0.6000\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 6.7234 - accuracy: 0.7000 - val_loss: 2.3842e-08 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.0786 - accuracy: 0.9500 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0657 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.06566821038722992, 1.0]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to training data\n",
    "model.fit(train_data, train_labels, validation_split=0.2, epochs=3, batch_size=10)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "model.evaluate(test_data, test_labels, batch_size=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How many parameters?\n",
    "## How many parameters in a CNN?\n",
    "We need to know how many parameters a CNN has, so we can adjust the model architecture, to reduce this number or shift parameters from one part of the network to another. How many parameters would a network have if its inputs are images with 28-by-28 pixels, there is one convolutional layer with 10 units kernels of 3-by-3 pixels, using zero padding (input has the same size as the output), and one densely connected layer with 2 units?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "15782"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 * (3 * 3) + 10 + 784 * 10 * 2 + 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How many parameters in a deep CNN?\n",
    "In this exercise, you will use Keras to calculate the total number of parameters along with the number of parameters in each layer of the network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 27, 27, 10)        50        \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 10)        410       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6760)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 20283     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,743\n",
      "Trainable params: 20,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(10, kernel_size=2, activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(10, kernel_size=2, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pooling operations\n",
    "\n",
    "## Write your own pooling operation\n",
    "As we have seen before, CNNs can have a lot of parameters. Pooling layers are often added between the convolutional layers of a neural network to summarize their outputs in a condensed manner, and reduce the number of parameters in the next layer in the network. This can help us if we want to train the network more rapidly, or if we don't have enough data to learn a very large number of parameters.\n",
    "\n",
    "A pooling layer can be described as a particular kind of convolution. For every window in the input it finds the maximal pixel value and passes only this pixel through. In this exercise, you will write your own max pooling operation, based on the code that you previously used to write a two-dimensional convolution operation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transform\n\u001B[1;32m      3\u001B[0m im \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset/brick_bw.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m imt \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(imt\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skimage/transform/_warps.py:147\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001B[0m\n\u001B[1;32m    145\u001B[0m image, output_shape \u001B[38;5;241m=\u001B[39m _preprocess_resize_output_shape(image, output_shape)\n\u001B[1;32m    146\u001B[0m input_shape \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m--> 147\u001B[0m input_type \u001B[38;5;241m=\u001B[39m \u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_type \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mfloat16:\n\u001B[1;32m    150\u001B[0m     image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5575\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   5568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   5569\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[1;32m   5570\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[1;32m   5571\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[1;32m   5572\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[1;32m   5573\u001B[0m ):\n\u001B[1;32m   5574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[0;32m-> 5575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "from skimage import transform\n",
    "\n",
    "im = pd.read_csv('dataset/brick_bw.csv')\n",
    "imt = transform.resize(im, (128, 128))\n",
    "print(imt.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((\u001B[43mimt\u001B[49m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m, imt\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Pooling operation\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ii \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(result\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'imt' is not defined"
     ]
    }
   ],
   "source": [
    "result = np.zeros((imt.shape[0] // 2, imt.shape[1] // 2))\n",
    "\n",
    "# Pooling operation\n",
    "for ii in range(result.shape[0]):\n",
    "    for jj in range(result.shape[1]):\n",
    "        result[ii, jj] = np.max(imt[ii * 2:ii * 2 + 2, jj * 2:jj * 2 + 2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m     ax2\u001B[38;5;241m.\u001B[39mset_title(img_title_filtered)\n\u001B[1;32m      8\u001B[0m     ax2\u001B[38;5;241m.\u001B[39maxis(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moff\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m plot_comparison(\u001B[43mimt\u001B[49m, result, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAfter MaxPooling\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'imt' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_comparison(img_original, img_filtered, img_title_filtered):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 8))\n",
    "    ax1.imshow(img_original, cmap=plt.cm.gray)\n",
    "    ax1.set_title('Original')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(img_filtered, cmap=plt.cm.gray)\n",
    "    ax2.set_title(img_title_filtered)\n",
    "    ax2.axis('off')\n",
    "\n",
    "plot_comparison(imt, result, 'After MaxPooling')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 27, 27, 15)        75        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 15)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 5)         305       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 720)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 2163      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,543\n",
      "Trainable params: 2,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MaxPool2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer\n",
    "model.add(Conv2D(15, kernel_size=2, activation='relu', input_shape=(img_rows, img_cols, 1)))\n",
    "\n",
    "# Add a pooling operation\n",
    "model.add(MaxPool2D(2))\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(5, kernel_size=2, activation='relu'))\n",
    "\n",
    "# Flatten and feed to output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras pooling layers\n",
    "Keras implements a pooling operation as a layer that can be added to CNNs between other layers. In this exercise, you will construct a convolutional neural network similar to the one you have constructed before:\n",
    "\n",
    "Convolution => Convolution => Flatten => Dense\n",
    "\n",
    "However, you will also add a pooling layer. The architecture will add a single max-pooling layer between the convolutional layer and the dense layer with a pooling of 2x2:\n",
    "\n",
    "Convolution => Max pooling => Convolution => Flatten => Dense"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 1s 41ms/step - loss: 6.0605 - accuracy: 0.2250 - val_loss: 3.5208 - val_accuracy: 0.4000\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.3224 - accuracy: 0.6500 - val_loss: 1.6885 - val_accuracy: 0.6000\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.9483 - accuracy: 0.8000 - val_loss: 1.1728 - val_accuracy: 0.8000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0369 - accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit to training data\n",
    "model.fit(train_data, train_labels, epochs=3, validation_split=0.2, batch_size=10);\n",
    "\n",
    "# Evaluate on test data\n",
    "model.evaluate(test_data, test_labels, batch_size=10);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
