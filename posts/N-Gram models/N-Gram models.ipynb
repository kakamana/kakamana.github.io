{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: \"N-Gram models\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "author: \"kakamana\"\n",
    "date: \"2023-03-30\"\n",
    "categories: [python, datacamp, machine learning, nlp, feature engineering]\n",
    "image: \"ngram.png\"\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# N-Gram models\n",
    "\n",
    "Learn about n-gram modeling and use it to perform sentiment analysis on movie reviews\n",
    "\n",
    "This **N-Gram models** is part of [Datacamp course: Introduction to Natural Language Processing in Python] This course teaches techniques for extracting useful information from text and converting it into a format suitable for ML models. You will learn about POS tagging, named entity recognition, readability scores, n-gram and tf-idf models, and how to implement them using scikit-learn and spaCy. Additionally, you will learn how to calculate the similarity between two documents. During this process, you will be able to predict the sentiment of movie reviews and build recommenders for movies and Ted Talks. In the course of this course, you will learn how to engineer critical features from any text and solve some of the most challenging problems in data science.\n",
    "\n",
    "This is my learning experience of data science through DataCamp. These repository contributions are part of my learning journey through my graduate program masters of applied data sciences (MADS) at University Of Michigan, [DeepLearning.AI], [Coursera] & [DataCamp]. You can find my similar articles & more stories at my [medium] & [LinkedIn] profile. I am available at [kaggle] & [github blogs] & [github repos]. Thank you for your motivation, support & valuable feedback.\n",
    "\n",
    "These include projects, coursework & notebook which I learned through my data science journey. They are created for reproducible & future reference purpose only. All source code, slides or screenshot are intellactual property of respective content authors. If you find these contents beneficial, kindly consider learning subscription from [DeepLearning.AI Subscription], [Coursera], [DataCamp]\n",
    "\n",
    "\n",
    "\n",
    "[DeepLearning.AI]: https://www.deeplearning.ai\n",
    "[DeepLearning.AI Subscription]: https://www.deeplearning.ai\n",
    "[Coursera]: https://www.coursera.org\n",
    "[DataCamp]: https://www.datacamp.com\n",
    "[medium]: https://medium.com/@kamig4u\n",
    "[LinkedIn]: https://www.linkedin.com/in/asadenterprisearchitect\n",
    "[kaggle]: https://www.kaggle.com/kakamana\n",
    "[github blogs]: https://kakamana.github.io\n",
    "[github repos]: https://github.com/kakamana\n",
    "[Datacamp course: Introduction to Natural Language Processing in Python]: (https://app.datacamp.com/learn/courses/feature-engineering-for-nlp-in-python)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dghr201\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dghr201\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dghr201\\appdata\\roaming\\python\\python39\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a bag of words model\n",
    "\n",
    "* Bag of words model\n",
    "    * Extract word tokens\n",
    "    * Compute frequency of word tokens\n",
    "    * Construct a word vector out of these frequencies and vocabulary of corpus\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BoW model for movie taglines\n",
    "\n",
    "In this exercise, you have been provided with a corpus of more than 7000 movie tag lines. Your job is to generate the bag of words representation bow_matrix for these taglines. For this exercise, we will ignore the text preprocessing step and generate bow_matrix directly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "      id                        title  \\\n1   8844                      Jumanji   \n2  15602             Grumpier Old Men   \n3  31357            Waiting to Exhale   \n4  11862  Father of the Bride Part II   \n5    949                         Heat   \n\n                                            overview  \\\n1  When siblings Judy and Peter discover an encha...   \n2  A family wedding reignites the ancient feud be...   \n3  Cheated on, mistreated and stepped on, the wom...   \n4  Just when George Banks has recovered from his ...   \n5  Obsessive master thief, Neil McCauley leads a ...   \n\n                                             tagline  \n1          roll the dice and unleash the excitement!  \n2  still yelling. still fighting. still ready for...  \n3  friends are the people who let you be yourself...  \n4  just when his world is back to normal... he's ...  \n5                           a los angeles crime saga  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>overview</th>\n      <th>tagline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>8844</td>\n      <td>Jumanji</td>\n      <td>When siblings Judy and Peter discover an encha...</td>\n      <td>roll the dice and unleash the excitement!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15602</td>\n      <td>Grumpier Old Men</td>\n      <td>A family wedding reignites the ancient feud be...</td>\n      <td>still yelling. still fighting. still ready for...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31357</td>\n      <td>Waiting to Exhale</td>\n      <td>Cheated on, mistreated and stepped on, the wom...</td>\n      <td>friends are the people who let you be yourself...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11862</td>\n      <td>Father of the Bride Part II</td>\n      <td>Just when George Banks has recovered from his ...</td>\n      <td>just when his world is back to normal... he's ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>949</td>\n      <td>Heat</td>\n      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n      <td>a los angeles crime saga</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('dataset/movie_overviews.csv').dropna()\n",
    "movies['tagline'] = movies['tagline'].str.lower()\n",
    "movies.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "corpus = movies['tagline']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 6614)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You now know how to generate a bag of words representation for a given corpus of documents. Notice that the word vectors created have more than 6600 dimensions. However, most of these dimensions have a value of zero since most words do not occur in a particular tagline.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nYou now know how to generate a bag of words representation for a given corpus of documents. Notice that the word vectors created have more than 6600 dimensions. However, most of these dimensions have a value of zero since most words do not occur in a particular tagline.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyzing dimensionality and preprocessing\n",
    "\n",
    "You have been provided with a lem_corpus that contains lowercased, lemmatized, and stopword-free versions of the movie taglines from the previous exercise.\n",
    "\n",
    "In this exercise, you are required to generate the bag of words representation bow_lem_matrix for these lemmatized taglines and to compare its shape to that of the bow_matrix obtained in the previous exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "lem_corpus = corpus.apply(lambda row: ' '.join([t.lemma_ for t in nlp(row)\n",
    "                                                if t.lemma_ not in stopwords\n",
    "                                                and t.lemma_.isalpha()]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "1                            roll dice unleash excitement\n2                                   yell fight ready love\n3                            friend people let let forget\n4                              world normal surprise life\n5                                  los angeles crime saga\n                              ...                        \n9091                         kingsglaive final fantasy xv\n9093                       happen vegas stay vegas happen\n9095    decorate officer devote family man defend hono...\n9097                              god incarnate city doom\n9098                                      band know story\nName: tagline, Length: 7033, dtype: object"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 4941)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of how_lem_matrix\n",
    "print(bow_lem_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mapping feature indices with feature names\n",
    "In the previous exercise, we learned that CountVectorizer does not necessarily index the vocabulary alphabetically. In this exercise, we will learn how to map each feature index to its corresponding feature name."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "sentences = ['The lion is the king of the jungle',\n",
    "             'Lions have lifespans of a decade',\n",
    "             'The lion is an endangered species']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   an  decade  endangered  have  is  jungle  king  lifespans  lion  lions  of  \\\n0   0       0           0     0   1       1     1          0     1      0   1   \n1   0       1           0     1   0       0     0          1     0      1   1   \n2   1       0           1     0   1       0     0          0     1      0   0   \n\n   species  the  \n0        0    3  \n1        0    0  \n2        1    1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>an</th>\n      <th>decade</th>\n      <th>endangered</th>\n      <th>have</th>\n      <th>is</th>\n      <th>jungle</th>\n      <th>king</th>\n      <th>lifespans</th>\n      <th>lion</th>\n      <th>lions</th>\n      <th>of</th>\n      <th>species</th>\n      <th>the</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary\n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "bow_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observe that the column names refer to the token whose frequency is being recorded. As an example, since the first column name is an, the first feature represents how often 'an' occurs in a given sentence. As a result of get_feature_names(), we receive a list corresponding to the mapping between the feature indexes and the vocabulary names.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nObserve that the column names refer to the token whose frequency is being recorded. As an example, since the first column name is an, the first feature represents how often 'an' occurs in a given sentence. As a result of get_feature_names(), we receive a list corresponding to the mapping between the feature indexes and the vocabulary names.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building a BoW Naive Bayes classifier\n",
    "\n",
    "* Steps\n",
    "    * Text preprocessing\n",
    "    * Building a bag-of-words model (or representation)\n",
    "    * Machine Learning\n",
    "\n",
    "## BoW vectors for movie reviews\n",
    "\n",
    "You are given two pandas Series, X_train and X_test, which contain movie reviews. They represent the training and testing review data, respectively. Your task is to preprocess the reviews and generate BoW vectors for these two sets using CountVectorizer.\n",
    "\n",
    "After we have generated the BoW vector matrices X_train_bow and X_test_bow, we will be able to apply a machine learning model to them and conduct sentiment analysis.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review  sentiment\n0  this anime series starts out great interesting...        0.0\n1  some may go for a film like this but i most as...        0.0\n2  i ve seen this piece of perfection during the ...        1.0\n3  this movie is likely the worst movie i ve ever...        0.0\n4  it ll soon be 10 yrs since this movie was rele...        1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>this anime series starts out great interesting...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>some may go for a film like this but i most as...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i ve seen this piece of perfection during the ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>this movie is likely the worst movie i ve ever...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>it ll soon be 10 yrs since this movie was rele...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews = pd.read_csv('dataset/movie_reviews_clean.csv')\n",
    "movie_reviews.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "X = movie_reviews['review']\n",
    "y = movie_reviews['sentiment']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(757, 15158)\n",
      "(253, 15158)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting the sentiment of a movie review\n",
    "\n",
    "For the training and test movie review data, you generated bag-of-words representations in the previous exercise. In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy by using this model. This model can only classify a review as either positive (1) or negative (0) since it is a binary classification problem. It is incapable of detecting neutral reviews."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m clf \u001B[38;5;241m=\u001B[39m MultinomialNB()\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Fit the classifier\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_bow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Measure the accuracy\u001B[39;00m\n\u001B[0;32m     10\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mscore(X_test_bow, y_test)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\naive_bayes.py:699\u001B[0m, in \u001B[0;36m_BaseDiscreteNB.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    679\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    680\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001B[39;00m\n\u001B[0;32m    681\u001B[0m \n\u001B[0;32m    682\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    697\u001B[0m \u001B[38;5;124;03m        Returns the instance itself.\u001B[39;00m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 699\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    700\u001B[0m     _, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    702\u001B[0m     labelbin \u001B[38;5;241m=\u001B[39m LabelBinarizer()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\naive_bayes.py:553\u001B[0m, in \u001B[0;36m_BaseDiscreteNB._check_X_y\u001B[1;34m(self, X, y, reset)\u001B[0m\n\u001B[0;32m    551\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_X_y\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:596\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    594\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 596\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    597\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1090\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1070\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1071\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1072\u001B[0m     )\n\u001B[0;32m   1074\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1075\u001B[0m     X,\n\u001B[0;32m   1076\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1087\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1088\u001B[0m )\n\u001B[1;32m-> 1090\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43m_check_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmulti_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_numeric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_numeric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1092\u001B[0m check_consistent_length(X, y)\n\u001B[0;32m   1094\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1112\u001B[0m, in \u001B[0;36m_check_y\u001B[1;34m(y, multi_output, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1110\u001B[0m     estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n\u001B[0;32m   1111\u001B[0m     y \u001B[38;5;241m=\u001B[39m column_or_1d(y, warn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m-> 1112\u001B[0m     \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1113\u001B[0m     _ensure_no_complex_data(y)\n\u001B[0;32m   1114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_numeric \u001B[38;5;129;01mand\u001B[39;00m y\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m allow_nan\n\u001B[0;32m    126\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m estimator_name\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[0;32m    131\u001B[0m             \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m             msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    133\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    144\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    145\u001B[0m             )\n\u001B[1;32m--> 146\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n",
      "\u001B[1;31mValueError\u001B[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = 'The movie was terrible. The music was underwhelming and the acting mediocre.'\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building n-gram models\n",
    "\n",
    "    BoW shortcomings\n",
    "        Example\n",
    "            The movie was good and not boring -> positive\n",
    "            The movie was not good and boring -> negative\n",
    "        Exactly the same BoW representation!\n",
    "        Context of the words is lost.\n",
    "        Sentiment dependent on the position of not\n",
    "    n-grams\n",
    "        Contiguous sequence of n elements (or words) in a given document.\n",
    "        Bi-grams / Tri-grams\n",
    "    n-grams Shortcomings\n",
    "        Increase number of dimension, occurs curse of dimensionality\n",
    "        Higher order n-grams are rare\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## n-gram models for movie tag lines\n",
    "\n",
    "In this exercise, we have been provided with a corpus of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model.\n",
    "\n",
    "We will then compare the number of features generated for each model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1, 1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1, 2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" %\n",
    "      (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Higher order n-grams for sentiment analysis\n",
    "\n",
    "Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "ng_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_ng = ng_vectorizer.fit_transform(X_train)\n",
    "X_test_ng = ng_vectorizer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [25]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m clf_ng \u001B[38;5;241m=\u001B[39m MultinomialNB()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Fit the classifier\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[43mclf_ng\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_ng\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Measure the accuracy\u001B[39;00m\n\u001B[0;32m      7\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m clf_ng\u001B[38;5;241m.\u001B[39mscore(X_test_ng, y_test)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\naive_bayes.py:699\u001B[0m, in \u001B[0;36m_BaseDiscreteNB.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    679\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    680\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001B[39;00m\n\u001B[0;32m    681\u001B[0m \n\u001B[0;32m    682\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    697\u001B[0m \u001B[38;5;124;03m        Returns the instance itself.\u001B[39;00m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 699\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    700\u001B[0m     _, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    702\u001B[0m     labelbin \u001B[38;5;241m=\u001B[39m LabelBinarizer()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\naive_bayes.py:553\u001B[0m, in \u001B[0;36m_BaseDiscreteNB._check_X_y\u001B[1;34m(self, X, y, reset)\u001B[0m\n\u001B[0;32m    551\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_X_y\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:596\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    594\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 596\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    597\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1090\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1070\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1071\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1072\u001B[0m     )\n\u001B[0;32m   1074\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1075\u001B[0m     X,\n\u001B[0;32m   1076\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1087\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1088\u001B[0m )\n\u001B[1;32m-> 1090\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43m_check_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmulti_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_numeric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_numeric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1092\u001B[0m check_consistent_length(X, y)\n\u001B[0;32m   1094\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1112\u001B[0m, in \u001B[0;36m_check_y\u001B[1;34m(y, multi_output, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1110\u001B[0m     estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n\u001B[0;32m   1111\u001B[0m     y \u001B[38;5;241m=\u001B[39m column_or_1d(y, warn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m-> 1112\u001B[0m     \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1113\u001B[0m     _ensure_no_complex_data(y)\n\u001B[0;32m   1114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_numeric \u001B[38;5;129;01mand\u001B[39;00m y\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m allow_nan\n\u001B[0;32m    126\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m estimator_name\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[0;32m    131\u001B[0m             \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m             msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    133\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    144\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    145\u001B[0m             )\n\u001B[1;32m--> 146\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n",
      "\u001B[1;31mValueError\u001B[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = 'The movie was not good. The plot had several holes and the acting lacked panache'\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing performance of n-gram models\n",
    "\n",
    "You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3.\n",
    "\n",
    "We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Splitting the data into training and test sets\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m train_X, test_X, train_y, test_y \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmovie_reviews\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreview\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mmovie_reviews\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msentiment\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mstratify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmovie_reviews\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msentiment\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Generateing ngrams\u001B[39;00m\n\u001B[0;32m     13\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m CountVectorizer(ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2469\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2465\u001B[0m         CVClass \u001B[38;5;241m=\u001B[39m ShuffleSplit\n\u001B[0;32m   2467\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[1;32m-> 2469\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstratify\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   2471\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   2472\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[0;32m   2473\u001B[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n\u001B[0;32m   2474\u001B[0m     )\n\u001B[0;32m   2475\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2050\u001B[0m, in \u001B[0;36mStratifiedShuffleSplit.split\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m   2016\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msplit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   2017\u001B[0m     \u001B[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001B[39;00m\n\u001B[0;32m   2018\u001B[0m \n\u001B[0;32m   2019\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2048\u001B[0m \u001B[38;5;124;03m    to an integer.\u001B[39;00m\n\u001B[0;32m   2049\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2050\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msplit(X, y, groups)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    893\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    894\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    895\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[0;32m    896\u001B[0m         )\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[1;32m--> 899\u001B[0m         \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    900\u001B[0m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    901\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    902\u001B[0m \u001B[43m            \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    903\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    904\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    907\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m allow_nan\n\u001B[0;32m    126\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m estimator_name\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[0;32m    131\u001B[0m             \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m             msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    133\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    144\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    145\u001B[0m             )\n\u001B[1;32m--> 146\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n",
      "\u001B[1;31mValueError\u001B[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(movie_reviews['review'],\n",
    "                                                    movie_reviews['sentiment'],\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=movie_reviews['sentiment'])\n",
    "\n",
    "# Generateing ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print the accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. \" %\n",
    "      (time.time() - start_time, clf.score(test_X, test_y)))\n",
    "print(\"The ngram representation had %i features.\" % (train_X.shape[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [27]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Splitting the data into training and test sets\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m train_X, test_X, train_y, test_y \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmovie_reviews\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreview\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mmovie_reviews\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msentiment\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mstratify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmovie_reviews\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msentiment\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Generateing ngrams\u001B[39;00m\n\u001B[0;32m     11\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m CountVectorizer(ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m3\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2469\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2465\u001B[0m         CVClass \u001B[38;5;241m=\u001B[39m ShuffleSplit\n\u001B[0;32m   2467\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[1;32m-> 2469\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstratify\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   2471\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   2472\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[0;32m   2473\u001B[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n\u001B[0;32m   2474\u001B[0m     )\n\u001B[0;32m   2475\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2050\u001B[0m, in \u001B[0;36mStratifiedShuffleSplit.split\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m   2016\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msplit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   2017\u001B[0m     \u001B[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001B[39;00m\n\u001B[0;32m   2018\u001B[0m \n\u001B[0;32m   2019\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2048\u001B[0m \u001B[38;5;124;03m    to an integer.\u001B[39;00m\n\u001B[0;32m   2049\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2050\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msplit(X, y, groups)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    893\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    894\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    895\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[0;32m    896\u001B[0m         )\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[1;32m--> 899\u001B[0m         \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    900\u001B[0m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    901\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    902\u001B[0m \u001B[43m            \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    903\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    904\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    907\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m allow_nan\n\u001B[0;32m    126\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m estimator_name\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[0;32m    131\u001B[0m             \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m             msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    133\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    144\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    145\u001B[0m             )\n\u001B[1;32m--> 146\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n",
      "\u001B[1;31mValueError\u001B[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(movie_reviews['review'],\n",
    "                                                    movie_reviews['sentiment'],\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=movie_reviews['sentiment'])\n",
    "\n",
    "# Generateing ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print the accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. \" %\n",
    "      (time.time() - start_time, clf.score(test_X, test_y)))\n",
    "print(\"The ngram representation had %i features.\" % (train_X.shape[1]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
