{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: \"Facial Expression Recognition with Keras\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "author: \"kakamana\"\n",
    "date: \"2023-06-02\"\n",
    "categories: [python, deep learning, machine learning, cnn, neural network, keras, tensorflow, face recognition]\n",
    "image: \"model.png\"\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Facial Expression Recognition with Keras\n",
    "\n",
    " This **Facial Expression Recognition with Keras** is part of [Coursera Project: Facial Expression Recognition with Keras]. We will build and train a convolutional neural network (CNN) in Keras from scratch to recognize facial expressions. The data consists of 48x48 pixel grayscale images of faces. The objective is to classify each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). You will use OpenCV to automatically detect faces in images and draw bounding boxes around them. Once you have trained, saved, and exported the CNN, you will directly serve the trained model predictions to a web interface and perform real-time facial expression recognition on video and image data.\n",
    "\n",
    "This is my learning experience of data science through DeepLearning.AI. These repository contributions are part of my learning journey through my graduate program masters of applied data sciences (MADS) at University Of Michigan, [DeepLearning.AI], [Coursera] & [DataCamp]. You can find my similar articles & more stories at my [medium] & [LinkedIn] profile. I am available at [kaggle] & [github blogs] & [github repos]. Thank you for your motivation, support & valuable feedback.\n",
    "\n",
    "These include projects, coursework & notebook which I learned through my data science journey. They are created for reproducible & future reference purpose only. All source code, slides or screenshot are intellectual property of respective content authors. If you find these contents beneficial, kindly consider learning subscription from [DeepLearning.AI Subscription], [Coursera], [DataCamp]\n",
    "\n",
    "\n",
    "\n",
    "[DeepLearning.AI]: https://www.deeplearning.ai\n",
    "[DeepLearning.AI Subscription]: https://www.deeplearning.ai\n",
    "[Coursera]: https://www.coursera.org\n",
    "[DataCamp]: https://www.datacamp.com\n",
    "[medium]: https://medium.com/@kamig4u\n",
    "[LinkedIn]: https://www.linkedin.com/in/asadenterprisearchitect\n",
    "[kaggle]: https://www.kaggle.com/kakamana\n",
    "[github blogs]: https://kakamana.github.io\n",
    "[github repos]: https://github.com/kakamana\n",
    "[Coursera Project: Facial Expression Recognition with Keras]: (https://www.coursera.org/learn/facial-expression-recognition-keras)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2 align=center> Facial Expression Recognition with Keras</h2>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Practice Project: Facial Expression Recognition with Keras\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Task 1: Import Libraries ](#1)\n",
    "- [ 2 - Task 2: Plot Sample Images ](#2)\n",
    "- [ 3 - Task 3: Generate Training and Validation Batches ](#3)\n",
    "- [ 4 - Task 4: Create Convolution Neural Network (CNN) Model ](#4)\n",
    "- [ 5 - Task 5: Train and Evaluate Model ](#5)\n",
    "- [ 6 - Task 6: Represent Model as JSON String ](#6)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"1\"></a>\n",
    "### Task 1: Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(devices)\n",
    "\n",
    "from IPython.display import SVG, Image\n",
    "#from livelossplot import PlotLossesTensorFlowKeras\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify the GPU device to use\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Set the GPU memory growth to True\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"2\"></a>\n",
    "### Task 2: Plot Sample Images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.datasets.fer.plot_example_images(plt).show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for expression in os.listdir(\"train/\"):\n",
    "    print(str(len(os.listdir(\"train/\" + expression))) + \" \" + expression + \" images\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"3\"></a>\n",
    "### Task 3: Generate Training and Validation Batches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_size = 48\n",
    "batch_size = 64\n",
    "\n",
    "datagen_train = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "train_generator = datagen_train.flow_from_directory(\"train/\",\n",
    "                                                    target_size=(img_size,img_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)\n",
    "\n",
    "datagen_validation = ImageDataGenerator(horizontal_flip=True)\n",
    "validation_generator = datagen_validation.flow_from_directory(\"test/\",\n",
    "                                                    target_size=(img_size,img_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"4\"></a>\n",
    "### Task 4: Create Convolution Neural Network (CNN) Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](model.png)\n",
    "Inspired by Goodfellow, I.J., et.al. (2013). Challenged in representation learning: A report of three machine learning contests. *Neural Networks*, 64, 59-63. [doi:10.1016/j.neunet.2014.09.005](https://arxiv.org/pdf/1307.0414.pdf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# 1 - Convolution\n",
    "model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 2nd Convolution layer\n",
    "model.add(Conv2D(128,(5,5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3rd Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 4th Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer 1st layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected layer 2nd layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"5\"></a>\n",
    "### Task 5: Train and Evaluate Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 15\n",
    "steps_per_epoch = train_generator.n//train_generator.batch_size\n",
    "validation_steps = validation_generator.n//validation_generator.batch_size\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=2, min_lr=0.00001, mode='auto')\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n",
    "                             save_weights_only=True, mode='max', verbose=1)\n",
    "callbacks = [PlotLossesKerasTF(), checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = validation_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"6\"></a>\n",
    "### Task 6: Represent Model as JSON String"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <h1>Below sections are for blogging purpose Where I am writing code for model.py, camera.py, main.py  & index.html for flask based application<h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First lets write code for model.py which used above generated model.json and try to predict emotions\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.15\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "set_session(session)\n",
    "\n",
    "\n",
    "class FacialExpressionModel(object):\n",
    "\n",
    "    EMOTIONS_LIST = [\"Angry\", \"Disgust\",\n",
    "                     \"Fear\", \"Happy\",\n",
    "                     \"Neutral\", \"Sad\",\n",
    "                     \"Surprise\"]\n",
    "\n",
    "    def __init__(self, model_json_file, model_weights_file):\n",
    "        # load model from JSON file\n",
    "        with open(model_json_file, \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            self.loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        # load weights into the new model\n",
    "        self.loaded_model.load_weights(model_weights_file)\n",
    "        #self.loaded_model.compile()\n",
    "        #self.loaded_model._make_predict_function()\n",
    "\n",
    "    def predict_emotion(self, img):\n",
    "        global session\n",
    "        set_session(session)\n",
    "        self.preds = self.loaded_model.predict(img)\n",
    "        return FacialExpressionModel.EMOTIONS_LIST[np.argmax(self.preds)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# This code uses camera.py file to process camera to capture video or it can process provided video placed in videos folder\n",
    "## Here if we replace cv2.VideoCapture('videos/facial_exp.mkv') with cv2.VideoCapture(0) then it will capture video stream from provided or connected camera.\n",
    "\n",
    "import cv2\n",
    "from model import FacialExpressionModel\n",
    "import numpy as np\n",
    "\n",
    "facec = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "model = FacialExpressionModel(\"model.json\", \"model_weights.h5\")\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "class VideoCamera(object):\n",
    "    def __init__(self):\n",
    "        #self.video = cv2.VideoCapture('videos/facial_exp.mkv')\n",
    "        self.video = cv2.VideoCapture(0)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.video.release()\n",
    "\n",
    "    # returns camera frames along with bounding boxes and predictions\n",
    "    def get_frame(self):\n",
    "        _, fr = self.video.read()\n",
    "        gray_fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
    "        faces = facec.detectMultiScale(gray_fr, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            fc = gray_fr[y:y+h, x:x+w]\n",
    "\n",
    "            roi = cv2.resize(fc, (48, 48))\n",
    "            pred = model.predict_emotion(roi[np.newaxis, :, :, np.newaxis])\n",
    "\n",
    "            cv2.putText(fr, pred, (x, y), font, 1, (255, 255, 0), 2)\n",
    "            cv2.rectangle(fr,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "        _, jpeg = cv2.imencode('.jpg', fr)\n",
    "        return jpeg.tobytes()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Below is basic flask application html to use to render an html version of emotion capture & its processing\n",
    "\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Face expression recognition</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <img id=\"bg\" width=1200px height=900px src=\"{{ url_for('video_feed') }}\">\n",
    "  </body>\n",
    "</html>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Last but not least is the main.py routine which will initiate through terminal (in mac) or through anacoda shell in windows to launch facial expression routine\n",
    "\n",
    "import socket\n",
    "\n",
    "from flask import Flask, render_template, Response\n",
    "from camera import VideoCamera\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "def gen(camera):\n",
    "    while True:\n",
    "        frame = camera.get_frame()\n",
    "        yield (b'--frame\\r\\n'\n",
    "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n\\r\\n')\n",
    "\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen(VideoCamera()),\n",
    "                    mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "\n",
    "def home():\n",
    "    # Connect to a server\n",
    "    conn = socket.create_connection(('dummy-server.com', 8080))\n",
    "    return 'Connected to server'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        #app.run(host='0.0.0.0:52520', debug=True)\n",
    "    app.run(debug=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
