<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="kakamana">
<meta name="dcterms.date" content="2023-04-23">

<title>Kakamana’s Blogs - Linear regression for multiple variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Kakamana’s Blogs</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://kakamana.github.io"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Nerdy_kakamana"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#linear-regression-for-multiple-variables" id="toc-linear-regression-for-multiple-variables" class="nav-link active" data-scroll-target="#linear-regression-for-multiple-variables">Linear regression for multiple variables</a></li>
  <li><a href="#optional-lab-multiple-variable-linear-regression" id="toc-optional-lab-multiple-variable-linear-regression" class="nav-link" data-scroll-target="#optional-lab-multiple-variable-linear-regression">Optional Lab: Multiple Variable Linear Regression</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Linear regression for multiple variables</h1>
  <div class="quarto-categories">
    <div class="quarto-category">python</div>
    <div class="quarto-category">deep learning.ai</div>
    <div class="quarto-category">machine learning</div>
    <div class="quarto-category">supervised learning</div>
    <div class="quarto-category">Linear regression</div>
    <div class="quarto-category">multiple variables</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>kakamana </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 23, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="linear-regression-for-multiple-variables" class="level1">
<h1>Linear regression for multiple variables</h1>
<p>The objective of this optional lab is to demonstrate how to define a multiple regression model, in code, and how to calculate the prediction, f of x. Additionally, you will learn how to calculate the cost and implement gradient descent for a multiple linear regression model. This lab will utilize Python’s numpy library, so if any of the code looks unfamiliar, please refer to the previous optional lab introducing Numpy and Vectorization for a refresher on Numpy functions.</p>
<p>This <strong>Linear regression for multiple variables</strong> is part of <a href="" title="https://www.deeplearning.ai/courses/machine-learning-specialization/">DeepLearning.AI course: Machine Learning Specialization / Course 1: Supervised Machine Learning: Regression and Classification</a> In this course we will learn the difference between supervised and unsupervised learning and regression and classification tasks. Develop a linear regression model. Understand and implement the purpose of a cost function. Understand and implement gradient descent as a machine learning training method.</p>
<p>This is my learning experience of data science through DeepLearning.AI. These repository contributions are part of my learning journey through my graduate program masters of applied data sciences (MADS) at University Of Michigan, <a href="https://www.deeplearning.ai">DeepLearning.AI</a>, <a href="https://www.coursera.org">Coursera</a> &amp; <a href="https://www.datacamp.com">DataCamp</a>. You can find my similar articles &amp; more stories at my <a href="https://medium.com/@kamig4u">medium</a> &amp; <a href="https://www.linkedin.com/in/asadenterprisearchitect">LinkedIn</a> profile. I am available at <a href="https://www.kaggle.com/kakamana">kaggle</a> &amp; <a href="https://kakamana.github.io">github blogs</a> &amp; <a href="https://github.com/kakamana">github repos</a>. Thank you for your motivation, support &amp; valuable feedback.</p>
<p>These include projects, coursework &amp; notebook which I learned through my data science journey. They are created for reproducible &amp; future reference purpose only. All source code, slides or screenshot are intellectual property of respective content authors. If you find these contents beneficial, kindly consider learning subscription from <a href="https://www.deeplearning.ai">DeepLearning.AI Subscription</a>, <a href="https://www.coursera.org">Coursera</a>, <a href="https://www.datacamp.com">DataCamp</a></p>
</section>
<section id="optional-lab-multiple-variable-linear-regression" class="level1">
<h1>Optional Lab: Multiple Variable Linear Regression</h1>
<p>In this lab, you will extend the data structures and previously developed routines to support multiple features. Several routines are updated making the lab appear lengthy, but it makes minor adjustments to previous routines making it quick to review. # Outline - <a href="#toc_15456_1.1">&nbsp;&nbsp;1.1 Goals</a> - <a href="#toc_15456_1.2">&nbsp;&nbsp;1.2 Tools</a> - <a href="#toc_15456_1.3">&nbsp;&nbsp;1.3 Notation</a> - <a href="#toc_15456_2">2 Problem Statement</a> - <a href="#toc_15456_2.1">&nbsp;&nbsp;2.1 Matrix X containing our examples</a> - <a href="#toc_15456_2.2">&nbsp;&nbsp;2.2 Parameter vector w, b</a> - <a href="#toc_15456_3">3 Model Prediction With Multiple Variables</a> - <a href="#toc_15456_3.1">&nbsp;&nbsp;3.1 Single Prediction element by element</a> - <a href="#toc_15456_3.2">&nbsp;&nbsp;3.2 Single Prediction, vector</a> - <a href="#toc_15456_4">4 Compute Cost With Multiple Variables</a> - <a href="#toc_15456_5">5 Gradient Descent With Multiple Variables</a> - <a href="#toc_15456_5.1">&nbsp;&nbsp;5.1 Compute Gradient with Multiple Variables</a> - <a href="#toc_15456_5.2">&nbsp;&nbsp;5.2 Gradient Descent With Multiple Variables</a> - <a href="#toc_15456_6">6 Congratulations</a></p>
<p><a name="toc_15456_1.1"></a> ## 1.1 Goals - Extend our regression model routines to support multiple features - Extend data structures to support multiple features - Rewrite prediction, cost and gradient routines to support multiple features - Utilize NumPy <code>np.dot</code> to vectorize their implementations for speed and simplicity</p>
<p><a name="toc_15456_1.2"></a> ## 1.2 Tools In this lab, we will make use of: - NumPy, a popular library for scientific computing - Matplotlib, a popular library for plotting data</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:31:01.229583Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:31:02.252898Z&quot;}" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy, math</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'./deeplearning.mplstyle'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(precision<span class="op">=</span><span class="dv">2</span>)  <span class="co"># reduced display precision on numpy arrays</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><a name="toc_15456_1.3"></a> ## 1.3 Notation Here is a summary of some of the notation you will encounter, updated for multiple features.</p>
<p>|General <img width="70/"> <br> Notation <img width="70/"> | Description<img width="350/">| Python (if applicable) | |: ————|: ————————————————————|| | <span class="math inline">\(a\)</span> | scalar, non bold || | <span class="math inline">\(\mathbf{a}\)</span> | vector, bold || | <span class="math inline">\(\mathbf{A}\)</span> | matrix, bold capital || | <strong>Regression</strong> | | | | | <span class="math inline">\(\mathbf{X}\)</span> | training example matrix | <code>X_train</code> | | <span class="math inline">\(\mathbf{y}\)</span> | training example targets | <code>y_train</code> | <span class="math inline">\(\mathbf{x}^{(i)}\)</span>, <span class="math inline">\(y^{(i)}\)</span> | <span class="math inline">\(i_{th}\)</span>Training Example | <code>X[i]</code>, <code>y[i]</code>| | m | number of training examples | <code>m</code>| | n | number of features in each example | <code>n</code>| | <span class="math inline">\(\mathbf{w}\)</span> | parameter: weight, | <code>w</code> | | <span class="math inline">\(b\)</span> | parameter: bias | <code>b</code> | | <span class="math inline">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> | The result of the model evaluation at <span class="math inline">\(\mathbf{x^{(i)}}\)</span> parameterized by <span class="math inline">\(\mathbf{w},b\)</span>: <span class="math inline">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)}+b\)</span> | <code>f_wb</code> |</p>
<p><img src="LinearRegression-1.png" class="img-fluid"></p>
<p><a name="toc_15456_2"></a> # 2 Problem Statement</p>
<p>You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below. Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 23%">
<col style="width: 21%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Size (sqft)</th>
<th>Number of Bedrooms</th>
<th>Number of floors</th>
<th>Age of Home</th>
<th>Price (1000s dollars)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2104</td>
<td>5</td>
<td>1</td>
<td>45</td>
<td>460</td>
</tr>
<tr class="even">
<td>1416</td>
<td>3</td>
<td>2</td>
<td>40</td>
<td>232</td>
</tr>
<tr class="odd">
<td>852</td>
<td>2</td>
<td>1</td>
<td>35</td>
<td>178</td>
</tr>
</tbody>
</table>
<p>You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.</p>
<p>Please run the following code cell to create your <code>X_train</code> and <code>y_train</code> variables.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:34:52.614444Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:34:52.621730Z&quot;}" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.array([[<span class="dv">2104</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">45</span>], [<span class="dv">1416</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">40</span>], [<span class="dv">852</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">35</span>]])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.array([<span class="dv">460</span>, <span class="dv">232</span>, <span class="dv">178</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><a name="toc_15456_2.1"></a> ## 2.1 Matrix X containing our examples Similar to the table above, examples are stored in a NumPy matrix <code>X_train</code>. Each row of the matrix represents one example. When you have <span class="math inline">\(m\)</span> training examples ( <span class="math inline">\(m\)</span> is three in our example), and there are <span class="math inline">\(n\)</span> features (four in our example), <span class="math inline">\(\mathbf{X}\)</span> is a matrix with dimensions (<span class="math inline">\(m\)</span>, <span class="math inline">\(n\)</span>) (m rows, n columns).</p>
<p><span class="math display">\[\mathbf{X} =
\begin{pmatrix}
x^{(0)}_0 &amp; x^{(0)}_1 &amp; \cdots &amp; x^{(0)}_{n-1} \\
x^{(1)}_0 &amp; x^{(1)}_1 &amp; \cdots &amp; x^{(1)}_{n-1} \\
\cdots \\
x^{(m-1)}_0 &amp; x^{(m-1)}_1 &amp; \cdots &amp; x^{(m-1)}_{n-1}
\end{pmatrix}
\]</span> notation: - <span class="math inline">\(\mathbf{x}^{(i)}\)</span> is vector containing example i. <span class="math inline">\(\mathbf{x}^{(i)}\)</span> $ = (x^{(i)}_0, x^{(i)}<em>1, ,x^{(i)}</em>{n-1})$ - <span class="math inline">\(x^{(i)}_j\)</span> is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.</p>
<p>Display the input data.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:36:15.013625Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:36:15.019365Z&quot;}" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data is stored in numpy array/matrix</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X Shape: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, X Type:</span><span class="sc">{</span><span class="bu">type</span>(X_train)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"y Shape: </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, y Type:</span><span class="sc">{</span><span class="bu">type</span>(y_train)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>X Shape: (3, 4), X Type:&lt;class 'numpy.ndarray'&gt;)
[[2104    5    1   45]
 [1416    3    2   40]
 [ 852    2    1   35]]
y Shape: (3,), y Type:&lt;class 'numpy.ndarray'&gt;)
[460 232 178]</code></pre>
</div>
</div>
<p><a name="toc_15456_2.2"></a> ## 2.2 Parameter vector w, b</p>
<ul>
<li><span class="math inline">\(\mathbf{w}\)</span> is a vector with <span class="math inline">\(n\)</span> elements.
<ul>
<li>Each element contains the parameter associated with one feature.</li>
<li>in our dataset, n is 4.</li>
<li>notionally, we draw this as a column vector</li>
</ul></li>
</ul>
<p><span class="math display">\[\mathbf{w} = \begin{pmatrix}
w_0 \\
w_1 \\
\cdots\\
w_{n-1}
\end{pmatrix}
\]</span> * <span class="math inline">\(b\)</span> is a scalar parameter.</p>
<p>For demonstration, <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(b\)</span> will be loaded with some initial selected values that are near the optimal. <span class="math inline">\(\mathbf{w}\)</span> is a 1-D NumPy vector.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:37:34.070807Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:37:34.076497Z&quot;}" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>b_init <span class="op">=</span> <span class="fl">785.1811367994083</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>w_init <span class="op">=</span> np.array([ <span class="fl">0.39133535</span>, <span class="fl">18.75376741</span>, <span class="op">-</span><span class="fl">53.36032453</span>, <span class="op">-</span><span class="fl">26.42131618</span>])</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"w_init shape: </span><span class="sc">{</span>w_init<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, b_init type: </span><span class="sc">{</span><span class="bu">type</span>(b_init)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>w_init shape: (4,), b_init type: &lt;class 'float'&gt;</code></pre>
</div>
</div>
<p><a name="toc_15456_3"></a> # 3 Model Prediction With Multiple Variables The model’s prediction with multiple variables is given by the linear model:</p>
<p><span class="math display">\[ f_{\mathbf{w},b}(\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \tag{1}\]</span> or in vector notation: <span class="math display">\[ f_{\mathbf{w},b}(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b  \tag{2} \]</span> where <span class="math inline">\(\cdot\)</span> is a vector <code>dot product</code></p>
<p>To demonstrate the dot product, we will implement prediction using (1) and (2).</p>
<p><a name="toc_15456_3.1"></a> ## 3.1 Single Prediction element by element Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:38:52.470959Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:38:52.477086Z&quot;}" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_single_loop(x, w, b):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    single predict using linear regression</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">      x (ndarray): Shape (n,) example with multiple features</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">      w (ndarray): Shape (n,) model parameters</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">      b (scalar):  model parameter</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">      p (scalar):  prediction</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        p_i <span class="op">=</span> x[i] <span class="op">*</span> w[i]</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p <span class="op">+</span> p_i</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> p <span class="op">+</span> b</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:39:15.182823Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:39:15.189827Z&quot;}" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get a row from our training data</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x_vec <span class="op">=</span> X_train[<span class="dv">0</span>,:]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_vec shape </span><span class="sc">{</span>x_vec<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, x_vec value: </span><span class="sc">{</span>x_vec<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># make a prediction</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>f_wb <span class="op">=</span> predict_single_loop(x_vec, w_init, b_init)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"f_wb shape </span><span class="sc">{</span>f_wb<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, prediction: </span><span class="sc">{</span>f_wb<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>x_vec shape (4,), x_vec value: [2104    5    1   45]
f_wb shape (), prediction: 459.9999976194083</code></pre>
</div>
</div>
<p>Note the shape of <code>x_vec</code>. It is a 1-D NumPy vector with 4 elements, (4,). The result, <code>f_wb</code> is a scalar.</p>
<p><a name="toc_15456_3.2"></a> ## 3.2 Single Prediction, vector</p>
<p>Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to speed up predictions.</p>
<p>Recall from the Python/Numpy lab that NumPy <code>np.dot()</code>[<a href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html">link</a>] can be used to perform a vector dot product.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:40:04.216970Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:40:04.224098Z&quot;}" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(x, w, b):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    single predict using linear regression</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">      x (ndarray): Shape (n,) example with multiple features</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">      w (ndarray): Shape (n,) model parameters</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">      b (scalar):             model parameter</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">      p (scalar):  prediction</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.dot(x, w) <span class="op">+</span> b</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:40:26.000942Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:40:26.008578Z&quot;}" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get a row from our training data</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x_vec <span class="op">=</span> X_train[<span class="dv">0</span>,:]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_vec shape </span><span class="sc">{</span>x_vec<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, x_vec value: </span><span class="sc">{</span>x_vec<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># make a prediction</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>f_wb <span class="op">=</span> predict(x_vec,w_init, b_init)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"f_wb shape </span><span class="sc">{</span>f_wb<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, prediction: </span><span class="sc">{</span>f_wb<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>x_vec shape (4,), x_vec value: [2104    5    1   45]
f_wb shape (), prediction: 459.9999976194083</code></pre>
</div>
</div>
<p>The results and shapes are the same as the previous version which used looping. Going forward, <code>np.dot</code> will be used for these operations. The prediction is now a single statement. Most routines will implement it directly rather than calling a separate predict routine.</p>
<p><a name="toc_15456_4"></a> # 4 Compute Cost With Multiple Variables The equation for the cost function with multiple variables <span class="math inline">\(J(\mathbf{w},b)\)</span> is: <span class="math display">\[J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 \tag{3}\]</span> where: <span class="math display">\[ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  \tag{4} \]</span></p>
<p>In contrast to previous labs, <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{x}^{(i)}\)</span> are vectors rather than scalars supporting multiple features.</p>
<p>Below is an implementation of equations (3) and (4). Note that this uses a <em>standard pattern for this course</em> where a for loop over all <code>m</code> examples is used.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:42:14.700113Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:42:14.712231Z&quot;}" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_cost(X, y, w, b):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    compute cost</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">      X (ndarray (m,n)): Data, m examples with n features</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">      y (ndarray (m,)) : target values</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">      w (ndarray (n,)) : model parameters</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">      b (scalar)       : model parameter</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">      cost (scalar): cost</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    cost <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        f_wb_i <span class="op">=</span> np.dot(X[i], w) <span class="op">+</span> b           <span class="co">#(n,)(n,) = scalar (see np.dot)</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> cost <span class="op">+</span> (f_wb_i <span class="op">-</span> y[i])<span class="op">**</span><span class="dv">2</span>       <span class="co">#scalar</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    cost <span class="op">=</span> cost <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> m)                      <span class="co">#scalar</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:43:22.829061Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:43:22.837875Z&quot;}" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and display cost using our pre-chosen optimal parameters.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>cost <span class="op">=</span> compute_cost(X_train, y_train, w_init, b_init)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Cost at optimal w : </span><span class="sc">{</span>cost<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Cost at optimal w : 1.5578904045996674e-12</code></pre>
</div>
</div>
<p><a name="toc_15456_5"></a> # 5 Gradient Descent With Multiple Variables Gradient descent for multiple variables:</p>
<p><span class="math display">\[\begin{align*} \text{repeat}&amp;\text{ until convergence:} \; \lbrace \newline\;
&amp; w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{5}  \; &amp; \text{for j = 0..n-1}\newline
&amp;b\ \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline \rbrace
\end{align*}\]</span></p>
<p>where, n is the number of features, parameters <span class="math inline">\(w_j\)</span>, <span class="math inline">\(b\)</span>, are updated simultaneously and where</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \tag{6}  \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &amp;= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{7}
\end{align}
\]</span> * m is the number of training examples in the data set</p>
<ul>
<li><span class="math inline">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> is the model’s prediction, while <span class="math inline">\(y^{(i)}\)</span> is the target value</li>
</ul>
<p><a name="toc_15456_5.1"></a> ## 5.1 Compute Gradient with Multiple Variables An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an - outer loop over all m examples. - <span class="math inline">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> for the example can be computed directly and accumulated - in a second loop over all n features: - <span class="math inline">\(\frac{\partial J(\mathbf{w},b)}{\partial w_j}\)</span> is computed for each <span class="math inline">\(w_j\)</span>.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:46:05.138381Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:46:05.152883Z&quot;}" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_gradient(X, y, w, b):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the gradient for linear regression</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">      X (ndarray (m,n)): Data, m examples with n features</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">      y (ndarray (m,)) : target values</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">      w (ndarray (n,)) : model parameters</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">      b (scalar)       : model parameter</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    m,n <span class="op">=</span> X.shape           <span class="co">#(number of examples, number of features)</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    dj_dw <span class="op">=</span> np.zeros((n,))</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    dj_db <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        err <span class="op">=</span> (np.dot(X[i], w) <span class="op">+</span> b) <span class="op">-</span> y[i]</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>            dj_dw[j] <span class="op">=</span> dj_dw[j] <span class="op">+</span> err <span class="op">*</span> X[i, j]</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        dj_db <span class="op">=</span> dj_db <span class="op">+</span> err</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    dj_dw <span class="op">=</span> dj_dw <span class="op">/</span> m</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    dj_db <span class="op">=</span> dj_db <span class="op">/</span> m</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dj_db, dj_dw</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:46:13.271357Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:46:13.286602Z&quot;}" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Compute and display gradient</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>tmp_dj_db, tmp_dj_dw <span class="op">=</span> compute_gradient(X_train, y_train, w_init, b_init)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'dj_db at initial w,b: </span><span class="sc">{</span>tmp_dj_db<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'dj_dw at initial w,b: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>tmp_dj_dw<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>dj_db at initial w,b: -1.6739251122999121e-06
dj_dw at initial w,b: 
 [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]</code></pre>
</div>
</div>
<p><a name="toc_15456_5.2"></a> ## 5.2 Gradient Descent With Multiple Variables The routine below implements equation (5) above.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:46:41.219643Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:46:41.222760Z&quot;}" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Performs batch gradient descent to learn w and b. Updates w and b by taking</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">    num_iters gradient steps with learning rate alpha</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">      X (ndarray (m,n))   : Data, m examples with n features</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">      y (ndarray (m,))    : target values</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">      w_in (ndarray (n,)) : initial model parameters</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co">      b_in (scalar)       : initial model parameter</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">      cost_function       : function to compute cost</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">      gradient_function   : function to compute the gradient</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">      alpha (float)       : Learning rate</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">      num_iters (int)     : number of iterations to run gradient descent</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co">      w (ndarray (n,)) : Updated values of parameters</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co">      b (scalar)       : Updated value of parameter</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co">      """</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># An array to store cost J and w's at each iteration primarily for graphing later</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    J_history <span class="op">=</span> []</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> copy.deepcopy(w_in)  <span class="co">#avoid modifying global w within function</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> b_in</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iters):</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the gradient and update the parameters</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        dj_db,dj_dw <span class="op">=</span> gradient_function(X, y, w, b)   <span class="co">##None</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update Parameters using w, b, alpha and gradient</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> w <span class="op">-</span> alpha <span class="op">*</span> dj_dw               <span class="co">##None</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> b <span class="op">-</span> alpha <span class="op">*</span> dj_db               <span class="co">##None</span></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save cost J at each iteration</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i<span class="op">&lt;</span><span class="dv">100000</span>:      <span class="co"># prevent resource exhaustion</span></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>            J_history.append( cost_function(X, y, w, b))</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i<span class="op">%</span> math.ceil(num_iters <span class="op">/</span> <span class="dv">10</span>) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>i<span class="sc">:4d}</span><span class="ss">: Cost </span><span class="sc">{</span>J_history[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:8.2f}</span><span class="ss">   "</span>)</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w, b, J_history <span class="co">#return final w,b and J history for graphing</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the next cell you will test the implementation.</p>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:47:30.951290Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:47:30.968774Z&quot;}" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize parameters</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>initial_w <span class="op">=</span> np.zeros_like(w_init)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>initial_b <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># some gradient descent settings</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">5.0e-7</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># run gradient descent</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>w_final, b_final, J_hist <span class="op">=</span> gradient_descent(X_train, y_train, initial_w, initial_b,</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>                                                    compute_cost, compute_gradient,</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>                                                    alpha, iterations)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"b,w found by gradient descent: </span><span class="sc">{</span>b_final<span class="sc">:0.2f}</span><span class="ss">,</span><span class="sc">{</span>w_final<span class="sc">}</span><span class="ss"> "</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>m,_ <span class="op">=</span> X_train.shape</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"prediction: </span><span class="sc">{</span>np<span class="sc">.</span>dot(X_train[i], w_final) <span class="op">+</span> b_final<span class="sc">:0.2f}</span><span class="ss">, target value: </span><span class="sc">{</span>y_train[i]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration    0: Cost  2529.46   
Iteration  100: Cost   695.99   
Iteration  200: Cost   694.92   
Iteration  300: Cost   693.86   
Iteration  400: Cost   692.81   
Iteration  500: Cost   691.77   
Iteration  600: Cost   690.73   
Iteration  700: Cost   689.71   
Iteration  800: Cost   688.70   
Iteration  900: Cost   687.69   
b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] 
prediction: 426.19, target value: 460
prediction: 286.17, target value: 232
prediction: 171.47, target value: 178</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-04-23T18:47:43.209988Z&quot;,&quot;end_time&quot;:&quot;2023-04-23T18:47:43.433682Z&quot;}" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot cost versus iteration</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, constrained_layout<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>ax1.plot(J_hist)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>ax2.plot(<span class="dv">100</span> <span class="op">+</span> np.arange(<span class="bu">len</span>(J_hist[<span class="dv">100</span>:])), J_hist[<span class="dv">100</span>:])</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">"Cost vs. iteration"</span>)<span class="op">;</span>  ax2.set_title(<span class="st">"Cost vs. iteration (tail)"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Cost'</span>)             <span class="op">;</span>  ax2.set_ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'iteration step'</span>)   <span class="op">;</span>  ax2.set_xlabel(<span class="st">'iteration step'</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Linear regression for multiple variable_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><em>These results are not inspiring</em>! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this.</p>
<p><a name="toc_15456_6"></a> # 6 Congratulations! In this lab you: - Redeveloped the routines for linear regression, now with multiple variables. - Utilized NumPy <code>np.dot</code> to vectorize the implementations</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="kakamana/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>