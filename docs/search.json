[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Muhammad Asad Kamran is passionate\nMuhammad Asad Kamran has more than 15 years of experience in Software Engineering profession. Having strong hands on for architect, development, consultation & team building/managing experience of enterprise application & their integration with business critical applications, which includes SharePoint/Project Server, Asp.Net MVC & Biztalk complex applications.\nBeing a certified PMP & Prince 2 practitioner, Asad has been managing & mentoring mission critical team which delivered successful & award wining projects worth millions for clients in Middle East, Europe including Telco Operators, Oil & Gas clients, ministries, Insurance giant & multinational Attorney giants. Having numerous success stories of working with key stakeholders to develop architectural framework that aligns strategy, processes, and IT assets with business goals.\nExcellent communication, presentation, and organizational skills. Involved in successful Digital Transformation & Integration projects which provides G2G, G2B, B2B, B2C & G2C e-commerce & Digital services. His core competencies are Collaboration, Messaging, Enterprise solution architecture, Digitization transformation, project management, Agile methodologies & Data analytics."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity Of Michigan | Michigan, USA Masters in applied data science (MADS) | May 2022 - April 2024\nComsats Institute Of Information Technology | Lahore, Pakistan Bachelors Of Computer Science (Software Engineering) | 2002 - 2005"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kakamana’s Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nIntroduction to Hypothesis Testing\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nhypothesis\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSampling Distribution\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAssessing model fit\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nSampling Methods\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSimple Logistic Regression Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to sampling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPredictions and model objects in linear regression\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSimple Linear Regression Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation in a nutshell\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nclassification\n\n\nmachine learning\n\n\nsupervised\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation and Experimental Design\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nDistribution (pdf, cdf) of iris data\n\n\n\n\n\n\n\npython\n\n\nstatistics\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nRandom Numbers and Probability\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSummary Of Statistics\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Correlation and experimental design/Correlation and experimental design.html",
    "href": "posts/Correlation and experimental design/Correlation and experimental design.html",
    "title": "Correlation and Experimental Design",
    "section": "",
    "text": "We will explore how to quantify the strength of a linear relationship between two variables, and explore how confounding variables can affect the relationship between two other variables. we’ll also see how a study’s design can influence its results, change how the data should be analyzed, and potentially affect the reliability of your conclusions\nThis Correlation and Experimental Design is part of Datacamp course: Introduction to Statistic in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport numpy as np\n\n\n\n\n* Correlation coefficient\n    * Quantifies the linear relationship between two variables\n    * Number between -1 and 1\n    * Magnitude corresponds to strength of relationship\n    * Sign (+ or -) corresponds to direction of relationship\n\n* Pearson product-moment correlation(rr)\n\n\n\nHere we’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\nIn this exercise, you’ll examine the relationship between a country’s life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively.\n\n\nCode\nworld_happiness = pd.read_csv('world_happiness.csv', index_col=0)\nworld_happiness.head()\n\n\n\n\n\n\n  \n    \n      \n      country\n      social_support\n      freedom\n      corruption\n      generosity\n      gdp_per_cap\n      life_exp\n      happiness_score\n    \n  \n  \n    \n      1\n      Finland\n      2.0\n      5.0\n      4.0\n      47.0\n      42400\n      81.8\n      155\n    \n    \n      2\n      Denmark\n      4.0\n      6.0\n      3.0\n      22.0\n      48300\n      81.0\n      154\n    \n    \n      3\n      Norway\n      3.0\n      3.0\n      8.0\n      11.0\n      66300\n      82.6\n      153\n    \n    \n      4\n      Iceland\n      1.0\n      7.0\n      45.0\n      3.0\n      47900\n      83.0\n      152\n    \n    \n      5\n      Netherlands\n      15.0\n      19.0\n      12.0\n      7.0\n      50500\n      81.8\n      151\n    \n  \n\n\n\n\n\n\nCode\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x='life_exp', y='happiness_score',data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create scatterplot of happiness_score vs life_exp with trendline\nsns.lmplot(x='life_exp', y='happiness_score',data=world_happiness, ci=None)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Correlation between life_exp and happiness_score\ncor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n\nprint(cor)\n\n\n0.7802249053272062\n\n\n\n\n\n* Correlation only accounts for linear relationships\n* Transformation\n   *  Certain statistical methods rely on variables having a linear relationship\n        * Correlation coefficient\n        * Linear regression\n* Correlation does not imply causation\n    * x is correlated with yy does not mean xx causes y\n\n\n\nWhile the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it’s far from perfect. Here we’ll explore one of the caveats of the correlation coefficient by examining the relationship between a country’s GDP per capita (gdp_per_cap) and happiness score.\n\n\nCode\n# Scatterplot of gdp_per_cap and life_exp\nsns.scatterplot(x='gdp_per_cap',y='life_exp', data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Correlation between gdp_per_cap and life_exp\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n\nprint(cor)\n\n\n0.7019547642148015\n\n\n\n\n\nWhen variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. Here we’ll perform a transformation yourself\n\n\nCode\n# Scatterplot of happiness_score vs. gdp_per_cap\nsns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n\n\n\n0.7279733012222975\n\n\n\n\nCode\n# Create log_gdp_per_cap column\nworld_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n\n# Scatterplot of log_gdp_per_cap and happiness_score\nsns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness);\nplt.show()\n\n# Calculate correlation\ncor =  world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\nprint(\"\\n The relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP\")\n\n\n\n\n\n0.8043146004918288\n\n The relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP\n\n\n\n\n\nA new column has been added to world_happiness called grams_sugar_per_day, which contains the average amount of sugar eaten per person per day in each country. We’ll examine the effect of a country’s average sugar consumption on its happiness score.\n\n\nCode\nworld_happiness = pd.read_csv('world_happiness_add_sugar.csv', index_col=0)\nworld_happiness\n\n\n\n\n\n\n  \n    \n      \n      country\n      social_support\n      freedom\n      corruption\n      generosity\n      gdp_per_cap\n      life_exp\n      happiness_score\n      grams_sugar_per_day\n    \n    \n      Unnamed: 0\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      Finland\n      2\n      5\n      4.0\n      47\n      42400\n      81.8\n      155\n      86.8\n    \n    \n      2\n      Denmark\n      4\n      6\n      3.0\n      22\n      48300\n      81.0\n      154\n      152.0\n    \n    \n      3\n      Norway\n      3\n      3\n      8.0\n      11\n      66300\n      82.6\n      153\n      120.0\n    \n    \n      4\n      Iceland\n      1\n      7\n      45.0\n      3\n      47900\n      83.0\n      152\n      132.0\n    \n    \n      5\n      Netherlands\n      15\n      19\n      12.0\n      7\n      50500\n      81.8\n      151\n      122.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129\n      Yemen\n      100\n      147\n      83.0\n      155\n      2340\n      68.1\n      5\n      77.9\n    \n    \n      130\n      Rwanda\n      144\n      21\n      2.0\n      90\n      2110\n      69.1\n      4\n      14.1\n    \n    \n      131\n      Tanzania\n      131\n      78\n      34.0\n      49\n      2980\n      67.7\n      3\n      28.0\n    \n    \n      132\n      Afghanistan\n      151\n      155\n      136.0\n      137\n      1760\n      64.1\n      2\n      24.5\n    \n    \n      133\n      Central African Republic\n      155\n      133\n      122.0\n      113\n      794\n      52.9\n      1\n      22.4\n    \n  \n\n133 rows × 9 columns\n\n\n\n\n\nCode\n# Scatterplot of grams_sugar_per_day and happiness_score\nsns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor =  world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n\n\n\n0.6939100021829634\n\n\n\n\n\n\n* Vocabulary\n    * Experiment aims to answer: What is the effect of the treatment on the response?\n        * Treatment: explanatory / independent variable\n        * Response: response / dependent variable\n    * E.g.: What is the effect of an advertisement on the number of products purchased?\n        * Treatment: advertisement\n        * Response: number of products purchased\n            * Controlled experiments\n            * Participants are assigned by researchers to either treatment group or control group\n            * Treatment group sees advertisement\n            * Control group does not\n            * Group should be comparable so that causation can be inferred\n            * If groups are not comparable, this could lead to confounding (bias)\n* Gold standard of experiment\n    * Randomized controlled trial\n        * Participants are assigned to treatment/control randomly, not based on any other characteristics\n        C* hoosing randomly helps ensure that groups are comparable\n    * Placebo\n        * Resembles treatement, but has no effect\n        * Participants will not know which group they're in\n    * Double-blind trial\n        * Person administering the treatment/running the study doesn't know whether the treatment is real or a placebo\n        * Prevents bias in the response and/or analysis of results\n    * Fewopportunities for bias = more reliable conclusion about causation\n* Observational studies\n    * Participants are not assigned randomly to groups\n        * Participants assign themselves, usually based on pre-existing characteristics\n    * Many research questions are not conductive to a controlled experiment\n        * Cannot force someone to smoke or have a disease\n    * Establish association, not causation\n        * Effects can be confounded by factors that got certain people into the control or treatment group\n        * There are ways to control for confounders to get more reliable conclusions about association\n            * Longitudinal vs. cross-sectional studies\n    * Longitudinal study\n        * Participants are followed over a period of time to examine effect of treatment on response\n        * Effect of age on height is not confounded by generation\n        * More expensive, results take longer\n    * Cross-sectional study\n        * Data on participants is collected from a single snapshot in time\n        * Effect of age on height is confounded by generation\n        * Cheaper, fater, more convenient"
  },
  {
    "objectID": "posts/Correlation in a nutshell/Correlation in a nutshell.html",
    "href": "posts/Correlation in a nutshell/Correlation in a nutshell.html",
    "title": "Correlation in a nutshell",
    "section": "",
    "text": "In this article we will explore basically a linear relationship between two variables, its possible quantification (magnitude & direction). We will also touch high level of confounding & caveats of correlation. This article use exploration of study for mammals sleeping habits & world happiness\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nCode\ndf = pd.read_csv('mammals.csv')\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      species\n      body_wt\n      brain_wt\n      non_dreaming\n      dreaming\n      total_sleep\n      life_span\n      gestation\n      predation\n      exposure\n      danger\n    \n  \n  \n    \n      0\n      Africanelephant\n      6654.000\n      5712.0\n      NaN\n      NaN\n      3.3\n      38.6\n      645.0\n      3\n      5\n      3\n    \n    \n      1\n      Africangiantpouchedrat\n      1.000\n      6.6\n      6.3\n      2.0\n      8.3\n      4.5\n      42.0\n      3\n      1\n      3\n    \n    \n      2\n      ArcticFox\n      3.385\n      44.5\n      NaN\n      NaN\n      12.5\n      14.0\n      60.0\n      1\n      1\n      1\n    \n    \n      3\n      Arcticgroundsquirrel\n      0.920\n      5.7\n      NaN\n      NaN\n      16.5\n      NaN\n      25.0\n      5\n      2\n      3\n    \n    \n      4\n      Asianelephant\n      2547.000\n      4603.0\n      2.1\n      1.8\n      3.9\n      69.0\n      624.0\n      3\n      5\n      4\n    \n  \n\n\n\n\n\n\nCode\ndf.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 62 entries, 0 to 61\nData columns (total 11 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   species       62 non-null     object \n 1   body_wt       62 non-null     float64\n 2   brain_wt      62 non-null     float64\n 3   non_dreaming  48 non-null     float64\n 4   dreaming      50 non-null     float64\n 5   total_sleep   58 non-null     float64\n 6   life_span     58 non-null     float64\n 7   gestation     58 non-null     float64\n 8   predation     62 non-null     int64  \n 9   exposure      62 non-null     int64  \n 10  danger        62 non-null     int64  \ndtypes: float64(7), int64(3), object(1)\nmemory usage: 5.5+ KB\n\n\n\n\n\nThe sleep time of 39 species of mammals distributed over 13 orders is analyzed in regards to their distribution over the 13 orders. There are 62 observations across 11 variables.\nspecies : Mammal species\nbody_wt : Mammal’s total body weight (kg)\nbrain_wt : Mammal’s brain weight (kg)\nnon_dreaming : Sleep hours without dreaming\ndreaming : Sleep hours spent dreaming\ntotal_sleep : Total number of hours of sleep\nlife_span : Life span (in years)\ngestation : Days during gestation / pregnancy\nThe likelihood that a mammal will be preyed upon. 1 = least likely to be preyed on. 5 = most likely to be preyed upon.\nexposure : How exposed a mammal is during sleep. 1 = least exposed (e.g., sleeps in a well-protected den). 5 = most exposed.\nA measure of how much danger the mammal faces. This index is based upon Predation and Exposure. 1 = least danger from other animals. 5 = most danger from other animals.\n\n\nCode\ndf.isnull().sum()\n\n\nspecies          0\nbody_wt          0\nbrain_wt         0\nnon_dreaming    14\ndreaming        12\ntotal_sleep      4\nlife_span        4\ngestation        4\npredation        0\nexposure         0\ndanger           0\ndtype: int64\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Body Weight Distribution')\nsns.histplot(df['body_wt'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Body Weight Distribution'}, xlabel='body_wt', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Body Wight')\nsns.barplot(x='body_wt', y='species', data=df.sort_values('body_wt',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Body Wight'}, xlabel='body_wt', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"body_wt\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c10a9430>\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Brain Weight Distribution')\nsns.histplot(df['brain_wt'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Brain Weight Distribution'}, xlabel='brain_wt', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Brain Wight')\nsns.barplot(x='brain_wt', y='species', data=df.sort_values('brain_wt',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Brain Wight'}, xlabel='brain_wt', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"brain_wt\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c5b40bb0>\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Life Span Distribution')\nsns.histplot(df['life_span'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Life Span Distribution'}, xlabel='life_span', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Life Span')\nsns.barplot(x='life_span', y='species', data=df.sort_values('life_span',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Life Span'}, xlabel='life_span', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"life_span\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c6460d60>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Predation Total Sleep Visualization')\nsns.countplot(x='predation',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"predation\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"predation\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"predation\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3907231976.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"predation\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='predation', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Exposure Total Sleep Visualization')\nsns.countplot(x='exposure',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"exposure\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"exposure\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"exposure\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3542283944.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"exposure\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='exposure', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Danger Total Sleep Visualization')\nsns.countplot(x='danger',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"danger\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"danger\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"danger\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3554697531.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"danger\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='danger', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\nx = explanatory / independent variables y = response / dependent variable\n\n\nCode\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x='total_sleep', y='dreaming',data=df)\n\n# Show plot\nplt.title('Sleeping habits')\nplt.ylabel(\"rem sleep per day(hour\")\nplt.xlabel(\"total sleep per day(hour\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.lmplot(x=\"total_sleep\", y=\"dreaming\", data=df, ci=None)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\ndf['total_sleep'].corr(df['dreaming'])\n\n\n0.7270869571641637\n\n\n\n\nCode\ndf['dreaming'].corr(df['total_sleep'])\n\n\n0.7270869571641637\n\n\n\n\n\n\n\nCode\n# Create a scatterplot of gestation vs. total_sleep and show\nsns.scatterplot(x='gestation', y='total_sleep',data=df)\n\n# Show plot\nplt.title('High negative correlation')\nplt.ylabel(\"gestation\")\nplt.xlabel(\"total sleep per day(hour\")\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.scatterplot(x='total_sleep', y='dreaming',data=df)\nplt.title(\"low positive correlation\")\nplt.show()"
  },
  {
    "objectID": "posts/Distribution (pdf, cdf) of iris dataset/Distribution (pdf, cdf) of iris data set.html",
    "href": "posts/Distribution (pdf, cdf) of iris dataset/Distribution (pdf, cdf) of iris data set.html",
    "title": "Distribution (pdf, cdf) of iris data",
    "section": "",
    "text": "Lets explore distribution functions pdf and cdf using Iris data set\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings;\nwarnings.filterwarnings('ignore');\n\n\n\n\nCode\niris=pd.read_csv('iris.csv')\niris.head()\n\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      type\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n    \n  \n\n\n\n\n\n\nCode\niris.shape\n\n\n(150, 5)\n\n\n\n\nCode\niris.columns\n\n\nIndex(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'type'], dtype='object')\n\n\n\n\nCode\niris['type'].value_counts()\n\n\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\nName: type, dtype: int64\n\n\n\n\nCode\nsetosa=iris[iris['type']==\"Iris-setosa\"]\nsetosa['petal_length'].value_counts()\n\n\n1.5    14\n1.4    12\n1.3     7\n1.6     7\n1.7     4\n1.2     2\n1.9     2\n1.1     1\n1.0     1\nName: petal_length, dtype: int64\n\n\n\n\n\n\nCode\niris.plot(kind='scatter',x='sepal_length',y='sepal_width');\nplt.show()\n\n\n\n\n\n\n\nCode\n#here we plot the scatter diagram with colour coding\nsns.set_style('whitegrid')\nsns.FacetGrid(iris,hue=\"type\",aspect = 2).map(plt.scatter,\"sepal_length\",\"sepal_width\").add_legend()\nplt.show()\n\n\n\n\n\n\n\n\nFor cross-referencing\n\n\nCode\nplt.close()\nsns.set_style(\"whitegrid\")\nsns.pairplot(iris,hue=\"type\",size=3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsetosa=iris.loc[iris[\"type\"]==\"Iris-setosa\"]\nversicolor=iris.loc[iris[\"type\"]==\"Iris-versicolor\"]\nvirginica=iris.loc[iris[\"type\"]==\"Iris-virginica\"]\n\n\n\n\nCode\nplt.plot(setosa[\"petal_length\"],np.zeros_like(setosa['petal_length']), 'o')\nplt.plot(versicolor[\"petal_length\"],np.zeros_like(versicolor['petal_length']), 'o')\nplt.plot(virginica[\"petal_length\"],np.zeros_like(virginica['petal_length']), 'o')\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.distplot(iris[iris['type']== 'Iris-setosa']['petal_length'])\n\n\n<AxesSubplot:xlabel='petal_length', ylabel='Density'>\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\",aspect = 2).map(sns.distplot, \"petal_length\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect = 2).map(sns.distplot, \"petal_width\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect=2).map(sns.distplot, \"sepal_length\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect=2).map(sns.distplot, \"sepal_width\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = False)\n\nprint(\"histogram counts\\n\",counts)\npdf = counts/(sum(counts))\nprint(\"Sum of count is\\n\",sum(counts))\nprint(\"bin edges \\n\",bin_edges)\nprint(\"PDF below \\n\",pdf);\n\nplt.gca().legend(('Pdf'))\nplt.title('PDF and PDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.plot(bin_edges[1:],pdf)\n\n\nhistogram counts\n [ 1  1  2  7 12 14  7  4  0  2]\nSum of count is\n 50\nbin edges \n [1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\nPDF below \n [0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n\n\n[<matplotlib.lines.Line2D at 0x1a5ae102100>]\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = False)\n\nprint(\"histogram counts\\n\",counts)\npdf = counts/(sum(counts))\nprint(\"Sum of count is\\n\",sum(counts))\nprint(\"bin edges \\n\",bin_edges)\nprint(\"PDF is below  \\n\",pdf);\n\ncdf = np.cumsum(pdf)\nprint(\"CDF is below\\n\",cdf)\nplt.gca().legend(('Cdf'))\nplt.title('CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\n\nplt.plot(bin_edges[1:],cdf)\n\n\nhistogram counts\n [ 1  1  2  7 12 14  7  4  0  2]\nSum of count is\n 50\nbin edges \n [1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\nPDF is below  \n [0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\nCDF is below\n [0.02 0.04 0.08 0.22 0.46 0.74 0.88 0.96 0.96 1.  ]\n\n\n[<matplotlib.lines.Line2D at 0x1a5ae1721c0>]\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\n\nprint(counts)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\n\n#compute CDF\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\nplt.gca().legend(('Pdf','Cdf'))\nplt.title('PDF and CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.show();\n\n\n[0.22222222 0.22222222 0.44444444 1.55555556 2.66666667 3.11111111\n 1.55555556 0.88888889 0.         0.44444444]\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\n\n\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=20,\n                                 density = True)\npdf = counts/(sum(counts))\nplt.plot(bin_edges[1:],pdf);\nplt.gca().legend(('Pdf','Cdf','bin edges'))\nplt.title('PDF and CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.show();\n\n\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\n\n# virginica\ncounts, bin_edges = np.histogram(virginica['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\n\n#versicolor\ncounts, bin_edges = np.histogram(versicolor['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\nplt.title('PDF and CDF For iris_versicolor')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\n\nplt.show();\n\n\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n[0.02 0.1  0.24 0.08 0.18 0.16 0.1  0.04 0.02 0.06]\n[4.5  4.74 4.98 5.22 5.46 5.7  5.94 6.18 6.42 6.66 6.9 ]\n[0.02 0.04 0.06 0.04 0.16 0.14 0.12 0.2  0.14 0.08]\n[3.   3.21 3.42 3.63 3.84 4.05 4.26 4.47 4.68 4.89 5.1 ]\n\n\n\n\n\n\n\n\n\n\nCode\n#Mean, Variance, Std-deviation,\nprint(\"Means:\")\nprint(np.mean(setosa[\"petal_length\"]))\n#Mean with an outlier.\nprint(np.mean(np.append(setosa[\"petal_length\"],50)));\nprint(np.mean(virginica[\"petal_length\"]))\nprint(np.mean(versicolor[\"petal_length\"]))\n\nprint(\"\\nStd-dev:\");\nprint(np.std(setosa[\"petal_length\"]))\nprint(np.std(virginica[\"petal_length\"]))\nprint(np.std(versicolor[\"petal_length\"]))\n\n\nMeans:\n1.464\n2.4156862745098038\n5.5520000000000005\n4.26\n\nStd-dev:\n0.17176728442867112\n0.546347874526844\n0.4651881339845203\n\n\n\n\n\n\n\nCode\n#Median, Quantiles, Percentiles, IQR.\nprint(\"\\nMedians:\")\nprint(np.median(setosa[\"petal_length\"]))\n#Median with an outlier\nprint(np.median(np.append(setosa[\"petal_length\"],50)));\nprint(np.median(virginica[\"petal_length\"]))\nprint(np.median(versicolor[\"petal_length\"]))\n\n\nprint(\"\\nQuantiles:\")\nprint(np.percentile(setosa[\"petal_length\"],np.arange(0, 100, 25)))\nprint(np.percentile(virginica[\"petal_length\"],np.arange(0, 100, 25)))\nprint(np.percentile(versicolor[\"petal_length\"], np.arange(0, 100, 25)))\n\nprint(\"\\n90th Percentiles:\")\nprint(np.percentile(setosa[\"petal_length\"],90))\nprint(np.percentile(virginica[\"petal_length\"],90))\nprint(np.percentile(versicolor[\"petal_length\"], 90))\n\nfrom statsmodels import robust\nprint (\"\\nMedian Absolute Deviation\")\nprint(robust.mad(setosa[\"petal_length\"]))\nprint(robust.mad(virginica[\"petal_length\"]))\nprint(robust.mad(versicolor[\"petal_length\"]))\n\n\n\nMedians:\n1.5\n1.5\n5.55\n4.35\n\nQuantiles:\n[1.    1.4   1.5   1.575]\n[4.5   5.1   5.55  5.875]\n[3.   4.   4.35 4.6 ]\n\n90th Percentiles:\n1.7\n6.31\n4.8\n\nMedian Absolute Deviation\n0.14826022185056031\n0.6671709983275211\n0.5189107764769602\n\n\n\n\n\n\n\nCode\nsns.boxplot(x='type',y='petal_length', data=iris)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.violinplot(x=\"type\", y=\"petal_length\", data=iris, size=8)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.jointplot(x=\"petal_length\", y=\"petal_width\", data=setosa, kind=\"kde\");\nplt.show();"
  },
  {
    "objectID": "posts/Predictions and model objects/Predictions and model objects.html",
    "href": "posts/Predictions and model objects/Predictions and model objects.html",
    "title": "Predictions and model objects in linear regression",
    "section": "",
    "text": "This article explores how linear regression models can be used to predict Taiwanese house prices and Facebook advert clicks. Our regression skills will also be developed through the use of hands-on model objects, as well as the concept of “regression to the mean” and how to transform variables within a dataset.\nThis Simple Linear Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nPredictions can be made using statistical models like linear regression. In other words, you specify each explanatory variable, feed it into the model, and get a prediction.\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847\n    \n  \n\n\n\n\n\n\nCode\n# Create the explanatory_data\nexplanatory_data = pd.DataFrame({'n_convenience': np.arange(0,10)})\n\n# Print it\nprint(explanatory_data)\n\n\n   n_convenience\n0              0\n1              1\n2              2\n3              3\n4              4\n5              5\n6              6\n7              7\n8              8\n9              9\n\n\n\n\nCode\nmdl_price_vs_conv=ols(\"price_twd_msq ~ n_convenience\",data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Create explanatory_data\nexplanatory_data = pd.DataFrame({'n_convenience': np.arange(0, 11)})\n\n# Use mdl_price_vs_conv to predict with explanatory_data, call it price_twd_msq\nprice_twd_msq = mdl_price_vs_conv.predict(explanatory_data)\n\n# Print it\nprint(price_twd_msq)\n\n\n0      8.224237\n1      9.022317\n2      9.820397\n3     10.618477\n4     11.416556\n5     12.214636\n6     13.012716\n7     13.810795\n8     14.608875\n9     15.406955\n10    16.205035\ndtype: float64\n\n\n\n\nCode\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_conv.predict(explanatory_data))\n\n# Print the result\nprint(prediction_data)\n\n\n    n_convenience  price_twd_msq\n0               0       8.224237\n1               1       9.022317\n2               2       9.820397\n3               3      10.618477\n4               4      11.416556\n5               5      12.214636\n6               6      13.012716\n7               7      13.810795\n8               8      14.608875\n9               9      15.406955\n10             10      16.205035\n\n\n\n\n\nThe prediction DataFrame you created contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values\n\n\nCode\n# Create a new figure, fig\nfig = plt.figure()\n\nsns.regplot(x=\"n_convenience\",\n            y=\"price_twd_msq\",\n            data=taiwan_real_estate,\n            ci=None)\n# Add a scatter plot layer to the regplot\nsns.scatterplot(x='n_convenience',y='price_twd_msq',data=prediction_data,color='red',marker='s')\n\n# Show the layered plot\nplt.show()\nprint(\"\\n the predicted points lie on the trend lin\")\n\n\n\n\n\n\n the predicted points lie on the trend lin\n\n\n\n\n\nThe model object created by ols() contains many elements. In order to perform further analysis on the model results, we need to extract its useful bits. The model coefficients, the fitted values, and the residuals are perhaps the most important pieces of the linear model object.\n\n\nCode\n# Print the model parameters of mdl_price_vs_conv\nprint(mdl_price_vs_conv.params)\n\n\nIntercept        8.224237\nn_convenience    0.798080\ndtype: float64\n\n\n\n\nCode\n# Print the fitted values of mdl_price_vs_conv\nprint(mdl_price_vs_conv.fittedvalues)\n\n\n0      16.205035\n1      15.406955\n2      12.214636\n3      12.214636\n4      12.214636\n         ...    \n409     8.224237\n410    15.406955\n411    13.810795\n412    12.214636\n413    15.406955\nLength: 414, dtype: float64\n\n\n\n\nCode\n# Print the residuals of mdl_price_vs_conv\nprint(mdl_price_vs_conv.resid)\n\n\n0     -4.737561\n1     -2.638422\n2      2.097013\n3      4.366302\n4      0.826211\n         ...   \n409   -3.564631\n410   -0.278362\n411   -1.526378\n412    3.670387\n413    3.927387\nLength: 414, dtype: float64\n\n\n\n\nCode\n# Print a summary of mdl_price_vs_conv\nprint(mdl_price_vs_conv.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          price_twd_msq   R-squared:                       0.326\nModel:                            OLS   Adj. R-squared:                  0.324\nMethod:                 Least Squares   F-statistic:                     199.3\nDate:                Thu, 12 Jan 2023   Prob (F-statistic):           3.41e-37\nTime:                        17:11:19   Log-Likelihood:                -1091.1\nNo. Observations:                 414   AIC:                             2186.\nDf Residuals:                     412   BIC:                             2194.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         8.2242      0.285     28.857      0.000       7.664       8.784\nn_convenience     0.7981      0.057     14.118      0.000       0.687       0.909\n==============================================================================\nOmnibus:                      171.927   Durbin-Watson:                   1.993\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1417.242\nSkew:                           1.553   Prob(JB):                    1.78e-308\nKurtosis:                      11.516   Cond. No.                         8.87\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nUsing the model coefficients, you can manually calculate predictions. It’s better to use .predict() when making predictions in real life, but doing it manually is helpful for reassuring yourself that predictions aren’t magic.\nFor simple linear regressions, the predicted value is the intercept plus the slope times the explanatory variable.\nresponse = intercept + slope * explanatory\n\n\nCode\n# Get the coefficients of mdl_price_vs_conv\ncoeffs = mdl_price_vs_conv.params\n\n# Get the intercept\nintercept = coeffs[0]\n\n# Get the slope\nslope = coeffs[1]\n\n# Manually calculate the predictions\nprice_twd_msq = intercept + slope * explanatory_data\nprint(price_twd_msq)\n\n# Compare to the results from .predict()\nprint(price_twd_msq.assign(predictions_auto=mdl_price_vs_conv.predict(explanatory_data)))\n\n\n    n_convenience\n0        8.224237\n1        9.022317\n2        9.820397\n3       10.618477\n4       11.416556\n5       12.214636\n6       13.012716\n7       13.810795\n8       14.608875\n9       15.406955\n10      16.205035\n    n_convenience  predictions_auto\n0        8.224237          8.224237\n1        9.022317          9.022317\n2        9.820397          9.820397\n3       10.618477         10.618477\n4       11.416556         11.416556\n5       12.214636         12.214636\n6       13.012716         13.012716\n7       13.810795         13.810795\n8       14.608875         14.608875\n9       15.406955         15.406955\n10      16.205035         16.205035\n\n\n\n\n\n\nResponse value = fitted value + residual\n“The stuff one can explain” + “the stuff once couldn’t explain”\nResiduals exist due to problems in model and fundamental randomness\nExtreme cases are often due to randomness\nRegression to mean indicated extreme cases don’t persist over time\n\n\n\nCode\nsp500_yearly_returns=pd.read_csv(\"dataset/sp500_yearly_returns.csv\")\nsp500_yearly_returns.head()\n\n\n\n\n\n\n  \n    \n      \n      symbol\n      return_2018\n      return_2019\n    \n  \n  \n    \n      0\n      AAPL\n      -0.053902\n      0.889578\n    \n    \n      1\n      MSFT\n      0.207953\n      0.575581\n    \n    \n      2\n      AMZN\n      0.284317\n      0.230278\n    \n    \n      3\n      FB\n      -0.257112\n      0.565718\n    \n    \n      4\n      GOOGL\n      -0.008012\n      0.281762\n    \n  \n\n\n\n\n\n\nCode\n# Create a new figure, fig\nfig = plt.figure()\n\n# Plot the first layer: y = x\nplt.axline(xy1=(0,0), slope=1, linewidth=2, color=\"green\")\n\n# Add scatter plot with linear regression trend line\nsns.regplot(x='return_2018',y='return_2019',data=sp500_yearly_returns,ci=None,line_kws={'color':'black'})\n\n# Set the axes so that the distances along the x and y axes look the same\nplt.axis(\"equal\")\n\n# Show the plot\nplt.show()\nprint('\\n The regression trend line looks very different to the y equals x line. As the financial advisors say, \"Past performance is no guarantee of future results.\"')\n\n\n\n\n\n\n The regression trend line looks very different to the y equals x line. As the financial advisors say, \"Past performance is no guarantee of future results.\"\n\n\n\n\n\nLet’s quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019\n\n\nCode\n# Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns\nmdl_returns = ols(\"return_2019 ~ return_2018\",data=sp500_yearly_returns).fit()\n\n# Print the parameters\nprint(mdl_returns.params)\n\n# Create a DataFrame with return_2018 at -1, 0, and 1\nexplanatory_data = pd.DataFrame({'return_2018':[-1,0,1]})\n\n# Use mdl_returns to predict with explanatory_data\nprint(mdl_returns.predict(explanatory_data))\n\nprint(\"\\n Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019\")\n\n\nIntercept      0.321321\nreturn_2018    0.020069\ndtype: float64\n0    0.301251\n1    0.321321\n2    0.341390\ndtype: float64\n\n Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019\n\n\n\n\n\nWhen there is no straight-line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both. Let’s transform the explanatory variable.\nWe’ll look at the Taiwan real estate dataset again, but we’ll use the distance to the nearest MRT (metro) station as the explanatory variable. By taking the square root, you’ll shorten the distance to the metro station for commuters.\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\nplt.figure()\n\n# Plot using the transformed variable\nsns.regplot(x='sqrt_dist_to_mrt_m',y='price_twd_msq',data=taiwan_real_estate)\nplt.show()\n\n\n\n\n\n\n\nCode\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\nprint(mdl_price_vs_dist.params)\n\n\nIntercept             16.709799\nsqrt_dist_to_mrt_m    -0.182843\ndtype: float64\n\n\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\nexplanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n\n# Create prediction_data by adding a column of predictions to explantory_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n)\n\n# Print the result\nprint(prediction_data)\n\n\n   sqrt_dist_to_mrt_m  dist_to_mrt_m  price_twd_msq\n0                 0.0              0      16.709799\n1                10.0            100      14.881370\n2                20.0            400      13.052942\n3                30.0            900      11.224513\n4                40.0           1600       9.396085\n5                50.0           2500       7.567656\n6                60.0           3600       5.739227\n7                70.0           4900       3.910799\n8                80.0           6400       2.082370\n\n\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\n# Use this explanatory data\nexplanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n\n# Use mdl_price_vs_dist to predict explanatory_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n)\n\nfig = plt.figure()\nsns.regplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n\n# Add a layer of your prediction points\nsns.scatterplot(data=prediction_data, x='sqrt_dist_to_mrt_m', y='price_twd_msq', color='red')\nplt.show()\n\nprint(\"\\n By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate mode\")\n\n\n\n\n\n\n By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate mode\n\n\n\n\n\nThe response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. That is, you “back transform” the predictions\n\n\nCode\nad_conversion=pd.read_csv('dataset/ad_conversion.csv')\nad_conversion.head()\n\n\n\n\n\n\n  \n    \n      \n      spent_usd\n      n_impressions\n      n_clicks\n    \n  \n  \n    \n      0\n      1.43\n      7350\n      1\n    \n    \n      1\n      1.82\n      17861\n      2\n    \n    \n      2\n      1.25\n      4259\n      1\n    \n    \n      3\n      1.29\n      4133\n      1\n    \n    \n      4\n      4.77\n      15615\n      3\n    \n  \n\n\n\n\n\n\nCode\n# Create qdrt_n_impressions and qdrt_n_clicks\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion['n_impressions'] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion['n_clicks'] ** 0.25\n\nplt.figure()\n\n# Plot using the transformed variables\nsns.regplot(x='qdrt_n_impressions',y='qdrt_n_clicks',data=ad_conversion,ci=None)\nplt.show()\n\n\n\n\n\n\n\nCode\n# From previous step\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n\n# Run a linear regression of your transformed variables\nmdl_click_vs_impression = ols('qdrt_n_clicks ~ qdrt_n_impressions', data=ad_conversion).fit()\nprint(mdl_click_vs_impression.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          qdrt_n_clicks   R-squared:                       0.945\nModel:                            OLS   Adj. R-squared:                  0.944\nMethod:                 Least Squares   F-statistic:                 1.590e+04\nDate:                Thu, 12 Jan 2023   Prob (F-statistic):               0.00\nTime:                        17:11:20   Log-Likelihood:                 193.90\nNo. Observations:                 936   AIC:                            -383.8\nDf Residuals:                     934   BIC:                            -374.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.0717      0.017      4.171      0.000       0.038       0.106\nqdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n==============================================================================\nOmnibus:                       11.447   Durbin-Watson:                   0.568\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\nSkew:                          -0.216   Prob(JB):                      0.00490\nKurtosis:                       2.707   Cond. No.                         52.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# From previous steps\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n\nmdl_click_vs_impression = ols(\"qdrt_n_clicks ~ qdrt_n_impressions\", data=ad_conversion, ci=None).fit()\n\n# Use this explanatory data\nexplanatory_data = pd.DataFrame({\"qdrt_n_impressions\": np.arange(0, 3e6+1, 5e5) ** .25,\n                                 \"n_impressions\": np.arange(0, 3e6+1, 5e5)})\n\n# Complete prediction_data\nprediction_data = explanatory_data.assign(\n    qdrt_n_clicks = mdl_click_vs_impression.predict(explanatory_data)\n)\n\n# Print the result\nprint(prediction_data)\nprint(\"\\n Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your result\")\n\n\n   qdrt_n_impressions  n_impressions  qdrt_n_clicks\n0            0.000000            0.0       0.071748\n1           26.591479       500000.0       3.037576\n2           31.622777      1000000.0       3.598732\n3           34.996355      1500000.0       3.974998\n4           37.606031      2000000.0       4.266063\n5           39.763536      2500000.0       4.506696\n6           41.617915      3000000.0       4.713520\n\n Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your result\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\base\\model.py:127: ValueWarning: unknown kwargs ['ci']\n  warnings.warn(msg, ValueWarning)\n\n\n\n\n\nIn the previous section, we transformed the response variable, ran a regression, and made predictions. However, we are not yet finished! We will need to perform a back-transformation in order to interpret and visualize your predictions correctly.\n\n\nCode\n# Back transform qdrt_n_clicks\nprediction_data[\"n_clicks\"] = prediction_data['qdrt_n_clicks'] ** 4\nprint(prediction_data)\n\n\n   qdrt_n_impressions  n_impressions  qdrt_n_clicks    n_clicks\n0            0.000000            0.0       0.071748    0.000026\n1           26.591479       500000.0       3.037576   85.135121\n2           31.622777      1000000.0       3.598732  167.725102\n3           34.996355      1500000.0       3.974998  249.659131\n4           37.606031      2000000.0       4.266063  331.214159\n5           39.763536      2500000.0       4.506696  412.508546\n6           41.617915      3000000.0       4.713520  493.607180\n\n\n\n\nCode\n# Plot the transformed variables\nfig = plt.figure()\nsns.regplot(x=\"qdrt_n_impressions\", y=\"qdrt_n_clicks\", data=ad_conversion, ci=None)\n\n# Add a layer of your prediction points\nsns.scatterplot(data=prediction_data, x='qdrt_n_impressions', y='qdrt_n_clicks', color='red')\nplt.show()\nprint(\"\\n Notice that your back-transformed predictions nicely follow the trend line and allow you to make more accurate predictions\")\n\n\n\n\n\n\n Notice that your back-transformed predictions nicely follow the trend line and allow you to make more accurate predictions"
  },
  {
    "objectID": "posts/Quantifying model fit/Quantifying model fit.html",
    "href": "posts/Quantifying model fit/Quantifying model fit.html",
    "title": "Assessing model fit",
    "section": "",
    "text": "What questions to ask your model to determine its fit. We will discuss how to quantify how well a linear regression model fits, how to diagnose problems with the model using visualizations, and how each observation impacts the model.\nThis Assessing model fit is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n\n\n\nAnalyze and assess the accuracy of model predictions.\nCoefficient of determination: R-squared (1 is the best, 0 is as good as randomness).\nThe proportion of variance in the response variable that is predictable (explainable) by the explanatory variable. This information indicates whether the model at hand is effective in resuming our data or not. Data, context, and the way we transform variables heavily impact r-squared interpretation.\nAccessible inside .summary() or .rsquared\nResidual standard error (RSE)\nThe residual is the difference between the predicted and observed response values (the distance). It has the same unit as the response.\nMSE = RSE**2 RSE = np.sqrt(MSE)\nAccessible with .mse_resid()\nRSE is calculated manually by taking the square of each residual. The degrees of freedom are calculated (# of observations minus # of model coefficients). Then we take the square root of the sum divided by the deg_freedom.\nRoot mean square error\nUnlike MSE, we do not remove degrees of freedom (we divide only by the number of observations).\n\n\n\nA coefficient of determination measures how well the linear regression line fits the observed values. It is equal to the square root of the correlation between the explanatory and response variables in a simple linear regression.\n\n\nCode\n# fetch data for which model is created\nad_conversion=pd.read_csv('dataset/ad_conversion.csv')\nad_conversion.head()\n\n\n\n\n\n\n  \n    \n      \n      spent_usd\n      n_impressions\n      n_clicks\n    \n  \n  \n    \n      0\n      1.43\n      7350\n      1\n    \n    \n      1\n      1.82\n      17861\n      2\n    \n    \n      2\n      1.25\n      4259\n      1\n    \n    \n      3\n      1.29\n      4133\n      1\n    \n    \n      4\n      4.77\n      15615\n      3\n    \n  \n\n\n\n\n\n\nCode\n# click vs impression model\nmdl_click_vs_impression_orig = ols('n_clicks ~ n_impressions' , data = ad_conversion).fit()\n# Print a summary of mdl_click_vs_impression_orig\nprint(mdl_click_vs_impression_orig.summary())\n\n# Create qdrt_n_impressions and qdrt_n_clicks\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion['n_impressions'] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion['n_clicks'] ** 0.25\n\n# qdrnt click vs impression model\nmdl_click_vs_impression_trans = ols('qdrt_n_clicks ~ qdrt_n_impressions  ' , data = ad_conversion).fit()\n# Print a summary of mdl_click_vs_impression_trans\nprint(mdl_click_vs_impression_trans.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               n_clicks   R-squared:                       0.892\nModel:                            OLS   Adj. R-squared:                  0.891\nMethod:                 Least Squares   F-statistic:                     7683.\nDate:                Fri, 13 Jan 2023   Prob (F-statistic):               0.00\nTime:                        09:10:19   Log-Likelihood:                -4126.7\nNo. Observations:                 936   AIC:                             8257.\nDf Residuals:                     934   BIC:                             8267.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         1.6829      0.789      2.133      0.033       0.135       3.231\nn_impressions     0.0002   1.96e-06     87.654      0.000       0.000       0.000\n==============================================================================\nOmnibus:                      247.038   Durbin-Watson:                   0.870\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            13215.277\nSkew:                          -0.258   Prob(JB):                         0.00\nKurtosis:                      21.401   Cond. No.                     4.88e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.88e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          qdrt_n_clicks   R-squared:                       0.945\nModel:                            OLS   Adj. R-squared:                  0.944\nMethod:                 Least Squares   F-statistic:                 1.590e+04\nDate:                Fri, 13 Jan 2023   Prob (F-statistic):               0.00\nTime:                        09:10:19   Log-Likelihood:                 193.90\nNo. Observations:                 936   AIC:                            -383.8\nDf Residuals:                     934   BIC:                            -374.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.0717      0.017      4.171      0.000       0.038       0.106\nqdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n==============================================================================\nOmnibus:                       11.447   Durbin-Watson:                   0.568\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\nSkew:                          -0.216   Prob(JB):                      0.00490\nKurtosis:                       2.707   Cond. No.                         52.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# Print the coeff of determination for mdl_click_vs_impression_orig\nprint(mdl_click_vs_impression_orig.rsquared)\n\n# Print the coeff of determination for mdl_click_vs_impression_trans\nprint(mdl_click_vs_impression_trans.rsquared)\n\nprint(\"\\n The transformed model has a higher coefficient of determination than the original model, suggesting that it gives a better fit to the data.\")\n\n\n0.8916134973508041\n0.9445272817143905\n\n The transformed model has a higher coefficient of determination than the original model, suggesting that it gives a better fit to the data.\n\n\n\n\n\nThe residual standard error (RSE) measures the typical residual size. Predictions are measured by how wrong they can be. The data fits better with smaller numbers, with zero being perfect\n\n\nCode\n# Calculate mse_orig for mdl_click_vs_impression_orig\nmse_orig = mdl_click_vs_impression_orig.mse_resid\n\n# Calculate rse_orig for mdl_click_vs_impression_orig and print it\nrse_orig = np.sqrt(mse_orig)\nprint(\"RSE of original model: \", rse_orig)\n\n# Calculate mse_trans for mdl_click_vs_impression_trans\nmse_trans = mdl_click_vs_impression_trans.mse_resid\n\n# Calculate rse_trans for mdl_click_vs_impression_trans and print it\nrse_trans = np.sqrt(mse_trans)\nprint(\"RSE of transformed model: \", rse_trans)\n\n\nRSE of original model:  19.905838862478134\nRSE of transformed model:  0.19690640896875722\n\n\n\n\n\nIf the model is well fitted, the residuals should be normally distributed along the line/curve, and the mean should be zero. In addition, it indicates when the fitted residuals are positive or negative (above/below the straight line).\nResidual VS fitted values chart\nTrends can be visualized using this tool. The best accuracy is achieved by following the y=0 line. There is a problem if the curve goes all over the place.\nsns.residplot()\nQ-Q Plot\nThe best conditions are validated if the points track along a straight line and are normally distributed. Otherwise, they are not.\nqqplot() (from statsmodels.api import qqplot)\nSquare root of Standardized Residuals VS fitted values, Scale-location plot\nAs the fitted values change, the residuals change in size and whether they become smaller or larger. If it bounces all over or is irregular, it means residuals tend to vary randomly or in an inconsistent manner as fitted values change.\n\n\n\nLet’s draw diagnostic plots using the Taiwan real estate dataset and the model of house prices versus number of convenience stores.\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847\n    \n  \n\n\n\n\n\n\nCode\nmdl_price_vs_conv=ols(\"price_twd_msq ~ n_convenience\",data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Plot the residuals vs. fitted values\nsns.residplot(x='n_convenience', y='price_twd_msq', data=taiwan_real_estate, lowess=True)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Import qqplot\nfrom statsmodels.api import qqplot\n\n# Create the Q-Q plot of the residuals\nqqplot(data=mdl_price_vs_conv.resid, fit=True, line=\"45\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Preprocessing steps\nmodel_norm_residuals = mdl_price_vs_conv.get_influence().resid_studentized_internal\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# Create the scale-location plot\nsns.regplot(x=mdl_price_vs_conv.fittedvalues, y=model_norm_residuals_abs_sqrt, ci=None, lowess=True)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Sqrt of abs val of stdized residuals\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\nprint(\"Above three diagnostic plots are excellent for sanity-checking the quality of your models\")\n\n\nAbove three diagnostic plots are excellent for sanity-checking the quality of your models"
  },
  {
    "objectID": "posts/Quantifying model fit/Quantifying model fit.html#extracting-leverage-and-influence",
    "href": "posts/Quantifying model fit/Quantifying model fit.html#extracting-leverage-and-influence",
    "title": "Assessing model fit",
    "section": "Extracting leverage and influence",
    "text": "Extracting leverage and influence\nLets find leverage and influence for taiwan real estate data\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Create summary_info\nsummary_info = mdl_price_vs_dist.get_influence().summary_frame()\nprint(summary_info.head(n=10))\n\n\n   dfb_Intercept  dfb_sqrt_dist_to_mrt_m       cooks_d  standard_resid  \\\n0      -0.094893                0.073542  4.648246e-03       -1.266363   \n1      -0.013981                0.008690  1.216711e-04       -0.262996   \n2       0.025510               -0.009963  6.231096e-04        0.688143   \n3       0.055525               -0.021686  2.939394e-03        1.494602   \n4      -0.000932                0.000518  6.055123e-07       -0.019716   \n5      -0.012257                0.029560  7.976174e-04        0.544490   \n6       0.000592               -0.000187  3.896928e-07        0.017531   \n7       0.010115               -0.006428  6.232088e-05        0.185284   \n8      -0.087118                0.126666  9.060428e-03        0.915959   \n9       0.009041               -0.033610  1.378024e-03       -0.818660   \n\n   hat_diag  dffits_internal  student_resid    dffits  \n0  0.005764        -0.096418      -1.267294 -0.096489  \n1  0.003506        -0.015599      -0.262699 -0.015582  \n2  0.002625         0.035302       0.687703  0.035279  \n3  0.002625         0.076673       1.496850  0.076789  \n4  0.003106        -0.001100      -0.019692 -0.001099  \n5  0.005352         0.039940       0.544024  0.039906  \n6  0.002530         0.000883       0.017509  0.000882  \n7  0.003618         0.011164       0.185067  0.011151  \n8  0.021142         0.134614       0.915780  0.134587  \n9  0.004095        -0.052498      -0.818332 -0.052477  \n\n\n\n\nCode\n# Add the hat_diag column to taiwan_real_estate, name it leverage\ntaiwan_real_estate[\"leverage\"] = summary_info['hat_diag']\n\n# Sort taiwan_real_estate by leverage in descending order and print the head\nprint(taiwan_real_estate.sort_values(by='leverage', ascending=False).head())\n\n\n     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \\\n347       6488.021              1        15 to 30       3.388805   \n116       6396.283              1        30 to 45       3.691377   \n249       6306.153              1        15 to 30       4.538578   \n255       5512.038              1        30 to 45       5.264750   \n8         5512.038              1        30 to 45       5.688351   \n\n     sqrt_dist_to_mrt_m  leverage  \n347           80.548253  0.026665  \n116           79.976765  0.026135  \n249           79.411290  0.025617  \n255           74.243101  0.021142  \n8             74.243101  0.021142  \n\n\n\n\nCode\n# Add the cooks_d column to taiwan_real_estate, name it cooks_dist\ntaiwan_real_estate['cooks_dist'] = summary_info['cooks_d']\n\n# Sort taiwan_real_estate by cooks_dist in descending order and print the head.\nprint(taiwan_real_estate.sort_values(\"cooks_dist\", ascending=False).head())\n\n\n     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \\\n270       252.5822              1         0 to 15      35.552194   \n148      3780.5900              0        15 to 30      13.645991   \n228      3171.3290              0         0 to 15      14.099849   \n220       186.5101              9        30 to 45      23.691377   \n113       393.2606              6         0 to 15       2.299546   \n\n     sqrt_dist_to_mrt_m  leverage  cooks_dist  \n270           15.892835  0.003849    0.115549  \n148           61.486503  0.012147    0.052440  \n228           56.314554  0.009332    0.035384  \n220           13.656870  0.004401    0.025123  \n113           19.830799  0.003095    0.022813"
  },
  {
    "objectID": "posts/Random numbers and probability/Random numbers and probability.html",
    "href": "posts/Random numbers and probability/Random numbers and probability.html",
    "title": "Random Numbers and Probability",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nRandom Numbers and Probability\nThis Random Numbers and Probability is part of Datacamp course: Introduction to Statistic in Python\nHere we’ll explore how to generate random samples and measure chance using probability.We will work with real-world sales data to calculate the probability of a salesperson being successful. Finally, we will try to use the binomial distribution to model events with binary outcomes.\nThis is my learning experience of data science through DataCamp\n\nMeasuring chance\n\\[ P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total # of possible outcomes}} \\]\nSampling with replacement vs sampling without replacement\nsampling without replacement, in which a subset of the observations are selected randomly, and once an observation is selected it cannot be selected again. sampling with replacement, in which a subset of observations are selected randomly, and an observation may be selected more than once\n\n\nCalculating probabilities\nYou’re in charge of the sales team, and it’s time for performance reviews, starting with Amir. As part of the review, you want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals.\nRecall that the probability of an event can be calculated by\n\\[ P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total # of possible outcomes}} \\]\n\n\nCode\namir_deals = pd.read_csv('amir_deals.csv', index_col=0)\namir_deals.head()\n\n\n\n\n\n\n  \n    \n      \n      product\n      client\n      status\n      amount\n      num_users\n    \n  \n  \n    \n      1\n      Product F\n      Current\n      Won\n      7389.52\n      19\n    \n    \n      2\n      Product C\n      New\n      Won\n      4493.01\n      43\n    \n    \n      3\n      Product B\n      New\n      Won\n      5738.09\n      87\n    \n    \n      4\n      Product I\n      Current\n      Won\n      2591.24\n      83\n    \n    \n      5\n      Product E\n      Current\n      Won\n      6622.97\n      17\n    \n  \n\n\n\n\n\n\nCode\ncounts = amir_deals['product'].value_counts()\nprint(counts)\n\n# Calculate probability of picking a deal with each product\nprobs = counts / len(amir_deals['product'])\nprint(probs)\n\n\nProduct B    62\nProduct D    40\nProduct A    23\nProduct C    15\nProduct F    11\nProduct H     8\nProduct I     7\nProduct E     5\nProduct N     3\nProduct G     2\nProduct J     2\nName: product, dtype: int64\nProduct B    0.348315\nProduct D    0.224719\nProduct A    0.129213\nProduct C    0.084270\nProduct F    0.061798\nProduct H    0.044944\nProduct I    0.039326\nProduct E    0.028090\nProduct N    0.016854\nProduct G    0.011236\nProduct J    0.011236\nName: product, dtype: float64\n\n\n\n\nSampling deals\nIn the previous exercise, you counted the deals Amir worked on. Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement.\nAdditionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed before sampling from the deals.\n\n\nCode\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals without replacement\nsample_without_replacement = amir_deals.sample(5,replace=False)\nprint(sample_without_replacement)\n\n\n       product   client status   amount  num_users\n128  Product B  Current    Won  2070.25          7\n149  Product D  Current    Won  3485.48         52\n78   Product B  Current    Won  6252.30         27\n105  Product D  Current    Won  4110.98         39\n167  Product C      New   Lost  3779.86         11\n\n\n\n\nCode\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals with replacement\nsample_with_replacement = amir_deals.sample(5,replace=True)\nprint(sample_with_replacement)\n\n\n       product   client status   amount  num_users\n163  Product D  Current    Won  6755.66         59\n132  Product B  Current    Won  6872.29         25\n88   Product C  Current    Won  3579.63          3\n146  Product A  Current    Won  4682.94         63\n146  Product A  Current    Won  4682.94         63\n\n\n\n\nDiscrete distributions\n\nProbability distribution\n\nDescribe probability of each possible outcome in a scenario\nExpected value: mean of probability distribution\n\nLaw of large number (LLN): as size of sample increases, sample mean will approach expected value.\n\n\n\nCreating a probability distribution\nRestaurant management wants to optimize seating space based on the size of the groups that come most often to a new restaurant. One night, 10 groups of people are waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. This exercise examines the probability of picking groups of different sizes.\nRemember that expected value can be calculated by multiplying each possible outcome with its corresponding probability and taking the sum\n\n\nCode\nrestaurant_groups = pd.read_csv('restaurant_groups.csv')\nrestaurant_groups.head()\n\n\n\n\n\n\n  \n    \n      \n      group_id\n      group_size\n    \n  \n  \n    \n      0\n      A\n      2\n    \n    \n      1\n      B\n      4\n    \n    \n      2\n      C\n      6\n    \n    \n      3\n      D\n      2\n    \n    \n      4\n      E\n      2\n    \n  \n\n\n\n\n\n\nCode\n# Create a histogram of restaurant_groups and show plot\nrestaurant_groups['group_size'].hist(bins=[2, 3, 4, 5, 6])\n\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nCode\n# Create probability distribution\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\nprint(size_dist)\n\n# Expected value\nexpected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\nprint(expected_value)\n\n# Subset groups of size 4 or more\ngroups_4_or_more = size_dist[size_dist['group_size'] >=4]\n\n# Sum the probabilities of groups_4_or_more\nprob_4_or_more = groups_4_or_more['prob'].sum()\nprint(prob_4_or_more)\n\n\n   group_size  prob\n0           2   0.6\n1           4   0.2\n2           6   0.1\n3           3   0.1\n2.9000000000000004\n0.30000000000000004\n\n\n\n\nContinuous distributions\nData back-ups\nYour company’s sales software backs itself up automatically, but no one knows exactly when the back-ups take place. It is known, however, that back-ups occur every 30 minutes. Amir updates the client data after sales meetings at random times. When will his newly-entered data be backed up? Answer Amir’s questions using your new knowledge of continuous uniform distributions\n\n\nCode\nfrom scipy.stats import uniform\n\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 = uniform.cdf(5, min_time, max_time)\nprint(prob_less_than_5)\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\nprint(prob_greater_than_5)\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - \\\n                        uniform.cdf(10, min_time, max_time)\nprint(prob_between_10_and_20)\n\n\n0.16666666666666666\n0.8333333333333334\n0.3333333333333333\n\n\n\n\nSimulating wait times\nTo give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\n\n\nCode\nnp.random.seed(334)\n\n# Generates 1000 wait times between 0 and 30 mins\nwait_times = uniform.rvs(min_time, max_time, 1000)\nprint(wait_times[:10])\n\n# Create a histogram of simulated times and show plot\nplt.hist(wait_times);\nprint (\"Unless Amir figures out exactly what time each backup happens, he won't be able to time his data entry so it gets backed up sooner, but it looks like he'll wait about 15 minutes on average\\n\")\n\n\n[ 7.144097    0.97455866  3.72802787  5.11644319  8.70602482 24.69140099\n 23.98012075  3.19592668 25.1985306  17.89048629]\nUnless Amir figures out exactly what time each backup happens, he won't be able to time his data entry so it gets backed up sooner, but it looks like he'll wait about 15 minutes on average\n\n\n\n\n\n\n\n\nThe binomial distribution\n\nBinomial distribution\n\nProbability distribution of number of successes in a sequence of independent trials\nDescribed by n and p\n\nn: total number of trials\np: probability of success\n\nExpected value: n * p\nIndependence: The binomial distribution is a probability distribution of number of successes in a sequence of independent trials\n\n\n\n\nSimulating sales deals\nAssume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance.\n\n\nCode\n# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate a single deal\nprint(binom.rvs(1, 0.3, size=1))\n\n# Simulate 1 week of 3 deals\nprint(binom.rvs(3,0.3,size=1))\n\n# Simulate 52 weeks of 3 deals\ndeals = binom.rvs(3,0.3,size=52)\n\n# Print mean deals won per week\nprint(np.mean(deals))\n\nprint('\\nIn this simulated year, Amir won 0.83 deals on average each week')\n\n\n[1]\n[0]\n0.8461538461538461\n\nIn this simulated year, Amir won 0.83 deals on average each week\n\n\n\n\nCalculating binomial probabilities\nJust as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution.\n\n\nCode\n# Probability of closing 3 out of 3 deals\nprob_3 = binom.pmf(3,3,0.3)\nprint(prob_3)\n\n# Probability of closing <= 1 deal out of 3 deals\nprob_less_than_or_equal_1 = binom.cdf(1,3,0.3)\nprint(prob_less_than_or_equal_1)\n\n# Probability of closing > 1 deal out of 3 deals\nprob_greater_than_1 =1- binom.cdf(1,3,0.3)\nprint(prob_greater_than_1)\n\nprint(\"\\nAmir has about a 22% chance of closing more than one deal in a week.\")\n\n\n0.026999999999999996\n0.784\n0.21599999999999997\n\nAmir has about a 22% chance of closing more than one deal in a week.\n\n\n\n\nHow many sales will be won?\nNow Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by n*p\n\n\nCode\n# Expected number won with 30% win rate\nwon_30pct = 3 * 0.3\nprint(won_30pct)\n\n# Expected number won with 25% win rate\nwon_25pct = 3 * 0.25\nprint(won_25pct)\n\n# Expected number won with 35% win rate\nwon_35pct = 3 * 0.35\nprint(won_35pct)\n\nprint('\\nIf Amirs win rate goes up by 5%, he can expect to close more than 1 deal on average each week')\n\n\n0.8999999999999999\n0.75\n1.0499999999999998\n\nIf Amirs win rate goes up by 5%, he can expect to close more than 1 deal on average each week"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html",
    "title": "Simple Linear Regression Modeling",
    "section": "",
    "text": "We will learn the basics of this popular statistical model, what regression is, and how linear and logistic regressions differ. We’ll then learn how to fit simple linear regression models with numeric and categorical explanatory variables, and how to describe the relationship between the response and explanatory variables using model coefficients\nThis Simple Linear Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#fitting-a-linear-regression",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#fitting-a-linear-regression",
    "title": "Simple Linear Regression Modeling",
    "section": "Fitting a linear regression",
    "text": "Fitting a linear regression\nStraight lines are defined by two things:\n\nIntercept: The y value at the point when x is zero.\nSlope: The amount the y value increases if you increase x by one.\nEquation: y = intercept + slope ∗ x"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#linear-regression-with-ols",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#linear-regression-with-ols",
    "title": "Simple Linear Regression Modeling",
    "section": "Linear regression with ols()",
    "text": "Linear regression with ols()\nWhile sns.regplot() can display a linear regression trend line, it doesn’t give you access to the intercept and slope as variables, or allow you to work with the model results as variables. That means that sometimes you’ll need to run a linear regression yourself.\n\n\nCode\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n# Create the model object\nmdl_price_vs_conv = ols(\"price_twd_msq ~ n_convenience\", data=taiwan_real_estate)\n\n# Fit the model\nmdl_price_vs_conv = mdl_price_vs_conv.fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_conv.params)\n\n\nIntercept        8.224237\nn_convenience    0.798080\ndtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#categorical-explanatory-variables",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#categorical-explanatory-variables",
    "title": "Simple Linear Regression Modeling",
    "section": "Categorical explanatory variables",
    "text": "Categorical explanatory variables\nVariables that categorize observations are known as categorical variables. Known as levels, they have a limited number of values. Gender is a categorical variable that can take two levels: Male or Female.\nNumbers are required for regression analysis. It is therefore necessary to make the results interpretable when a categorical variable is included in a regression model.\nA set of binary variables is created by recoding categorical variables. The recoding process creates a contrast matrix table by “dummy coding”\nThere are two type of data variables: * Quantitative data: refers to amount * Data collected quantitatively represents actual amounts that can be added, subtracted, divided, etc. Quantitative variables can be: * discrete (integer variables): count of individual items in record e.g. No. of players * continuous (ratio variables): continuous / non-finite value measurements e.g. distance, age etc * Categorical: refers to grouping There are three types of categorical variables: * binary: yes / no e.g. head/tail of coin flip * nominal: group with no rank or order b/w them e.g. color, brand, species etc * ordinal: group that can be ranked in specific order e.g. rating scale in survey result"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#visualizing-numeric-vs.-categorical",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#visualizing-numeric-vs.-categorical",
    "title": "Simple Linear Regression Modeling",
    "section": "Visualizing numeric vs. categorical",
    "text": "Visualizing numeric vs. categorical\nIf the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn’t make sense. Instead, a good option is to draw a histogram for each category.\n\n\nCode\n# Histograms of price_twd_msq with 10 bins, split by the age of each house\nsns.displot(data=taiwan_real_estate,\n         x='price_twd_msq',\n         col='house_age_years',\n         bins=10)\n\n# Show the plot\nplt.show()\nprint(\"\\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest.\")\n\n\n\n\n\n\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest."
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#using-categories-to-calculate-means",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#using-categories-to-calculate-means",
    "title": "Simple Linear Regression Modeling",
    "section": "Using categories to calculate means",
    "text": "Using categories to calculate means\nUsing summary statistics for each category is a good way to explore categorical variables further. Using a categorical variable, you can calculate the mean and median of your response variable. Therefore, you can compare each category in more detail.\n\n\nCode\n# Calculate the mean of price_twd_msq, grouped by house age\nmean_price_by_age = taiwan_real_estate.groupby('house_age_years')['price_twd_msq'].mean()\n\n# Print the result\nprint(mean_price_by_age)\n\n\nhouse_age_years\n0 to 15     12.637471\n15 to 30     9.876743\n30 to 45    11.393264\nName: price_twd_msq, dtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#is-coefficient-of-linear-regression-model-is-mean-of-each-category",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#is-coefficient-of-linear-regression-model-is-mean-of-each-category",
    "title": "Simple Linear Regression Modeling",
    "section": "Is coefficient of linear regression model is mean of each category?",
    "text": "Is coefficient of linear regression model is mean of each category?\nWhile calculating linear regression with categorical explanatory variable, means of each category will also coefficient of linear regression but this hold true in case with only one categorical variable. Lets verify this\n\n\nCode\n# Create the model, fit it\nmdl_price_vs_age = ols(\"price_twd_msq ~ house_age_years\", data=taiwan_real_estate).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_age.params)\n\n\nIntercept                      12.637471\nhouse_age_years[T.15 to 30]    -2.760728\nhouse_age_years[T.30 to 45]    -1.244207\ndtype: float64\n\n\n\n\nCode\n# Update the model formula to remove the intercept\nmdl_price_vs_age0 = ols(\"price_twd_msq ~ house_age_years + 0\", data=taiwan_real_estate).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_age0.params)\nprint(\"\\n The coefficients of the model are just the means of each category you calculated previously. Fantastic job! \")\n\n\nhouse_age_years[0 to 15]     12.637471\nhouse_age_years[15 to 30]     9.876743\nhouse_age_years[30 to 45]    11.393264\ndtype: float64\n\n The coefficients of the model are just the means of each category you calculated previously. Fantastic job!"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html",
    "title": "Simple Logistic Regression Modeling",
    "section": "",
    "text": "Get a better understanding of logistic regression models. We will analyze real-world data to predict the likelihood of a customer closing their bank account in terms of probabilities of success and odds ratios, and quantify the performance of your model using confusion matrices.\nThis Simple Logistic Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression",
    "title": "Simple Logistic Regression Modeling",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nHow would the response variable be affected if it were binary or logical? It can be Yes/No, 1/0, Blue/Red, etc.\nFor categorical responses, a logistic regression model is another type of generalized linear model.\nAn S curve is drawn to represent the response. Probabilities can be considered to be the fitted values between 0 and 1."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#exploring-the-explanatory-variables",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#exploring-the-explanatory-variables",
    "title": "Simple Logistic Regression Modeling",
    "section": "Exploring the explanatory variables",
    "text": "Exploring the explanatory variables\nIn the case of a logical response variable, all points lie on the y=0 and y=1 lines, making it difficult to determine what is occurring. It was unclear how the explanatory variable was distributed on each line before you saw the trend line. A histogram of the explanatory variable, grouped by the response, can be used to resolve this problem.\nThese histograms will be used to gain an understanding of the financial services churn dataset\n\n\nCode\nchurn = pd.read_csv('dataset/churn.csv')\nprint(churn.head())\n\n\n   has_churned  time_since_first_purchase  time_since_last_purchase\n0            0                  -1.089221                 -0.721322\n1            0                   1.182983                  3.634435\n2            0                  -0.846156                 -0.427582\n3            0                   0.086942                 -0.535672\n4            0                  -1.166642                 -0.672640\n\n\n\n\nCode\n# Create the histograms of time_since_last_purchase split by has_churned\nsns.displot(data=churn,x='time_since_last_purchase',col='has_churned')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Redraw the plot with time_since_first_purchase\nsns.displot(data=churn,x='time_since_first_purchase', col='has_churned')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create the histograms of time_since_last_purchase split by has_churned\nsns.distplot(churn['time_since_last_purchase'])\n\n# Show the plot\nplt.show()\n\nprint(churn['has_churned'].unique())\n\nfor x in churn['has_churned'].unique():\n    values = churn.time_since_last_purchase[churn['has_churned'] == x]\n    sns.distplot(values, hist=False, rug=False)\n\ng = sns.FacetGrid(churn, col='has_churned', hue='has_churned')\np1 = g.map(sns.distplot, 'time_since_last_purchase', hist=True, rug=True).add_legend()\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(churn['time_since_last_purchase'])\n\n\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\n\n\n[0 1]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\nCode\n# Create the histograms of time_since_first_purchase split by has_churned\nsns.distplot(churn['time_since_first_purchase'])\n\n# Show the plot\nplt.show()\n\nprint(churn['has_churned'].unique())\n\nfor x in churn['has_churned'].unique():\n    values = churn.time_since_last_purchase[churn['has_churned'] == x]\n    sns.distplot(values, hist=False, rug=False)\n\ng = sns.FacetGrid(churn, col='has_churned', hue='has_churned')\np1 = g.map(sns.distplot, 'time_since_first_purchase', hist=True, rug=True).add_legend()\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(churn['time_since_first_purchase'])\n\n\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\n\n\n[0 1]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#visualizing-liner-and-logistic-model",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#visualizing-liner-and-logistic-model",
    "title": "Simple Logistic Regression Modeling",
    "section": "Visualizing liner and logistic model",
    "text": "Visualizing liner and logistic model\nA logistic regression model can be drawn using regplot() in the same manner as a linear regression without you having to concern yourself with the modeling code. Try drawing both trend lines side by side to see how linear and logistic regressions make different predictions. From the linear model, you should see a linear trend (straight line), whereas from the logistic model, you should see a logistic trend (S-shaped).\n\n\nCode\n# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(data=churn,x='time_since_first_purchase',y='has_churned'\n            ,line_kws={\"color\": \"red\"})\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(x=\"time_since_first_purchase\",\n            y=\"has_churned\",\n            data=churn,\n            ci=None,\n            line_kws={\"color\": \"red\"})\n\n# Draw a logistic regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(data=churn,x='time_since_first_purchase', y='has_churned',ci=None,logistic=True,line_kws={\"color\": \"blue\"})\n\nplt.show()\n\nprint(\"\\n The two models give similar predictions in some places, but notice the slight curve in the logistic model trend.\")\n\n\n\n\n\n\n The two models give similar predictions in some places, but notice the slight curve in the logistic model trend."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression-with-logit",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression-with-logit",
    "title": "Simple Logistic Regression Modeling",
    "section": "Logistic regression with logit()",
    "text": "Logistic regression with logit()\nLogistic regression requires another function from statsmodels.formula.api: logit(). It takes the same arguments as ols(): a formula and data argument. You then use .fit() to fit the model to the data.\n\n\nCode\n# Import logit\nfrom statsmodels.formula.api import logit\n\n# Fit a logistic regression of churn vs. length of relationship using the churn dataset\nmdl_churn_vs_relationship = logit('has_churned ~ time_since_first_purchase', data=churn).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_churn_vs_relationship.params)\n\n\nOptimization terminated successfully.\n         Current function value: 0.679663\n         Iterations 4\nIntercept                   -0.015185\ntime_since_first_purchase   -0.354795\ndtype: float64"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#predictions-and-odds-ratios",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#predictions-and-odds-ratios",
    "title": "Simple Logistic Regression Modeling",
    "section": "Predictions and odds ratios",
    "text": "Predictions and odds ratios\nOdds ratios\nTaking the probability that something will happen and dividing it by the probability that it will not happen. It is equal to (P/(1-P)). Probability in favor of / against. The data cannot be compared with the original data, but can instead be plotted using a special chart. This unit represents the probability of … occurring (3 times the probability of…). It is easy to interpret, the data cannot be altered easily, and it is precise.\nLog odds ratio\nIt is a nice property of odds ratios that they can be passed into a log() = linear regression. Data changes that are easy to interpret and precise.\nMost likely Outcome\nAccording to logistic regression, we discuss the rounded most likely outcome (response > 0.5 chance of churning, etc.) since response values can be interpreted as probabilities. This data is very easy to interpret, easy to change, and not precise (rounded).\nProbability\nOriginal data. Easy to interpret, not easy to change data on the fly, and precise.\nProbabilities\nWe will examine each of the four main ways of expressing a logistic regression model’s prediction in the following four exercises. Since the response variable is either “yes” or “no”, you can predict the probability of a “yes”. These probabilities will be calculated and visualized here.\n\n\nCode\nexplanatory_data = pd.DataFrame({'time_since_first_purchase': np.arange(-1.5, 4, .35)})\n\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n  has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n)\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned\n0                      -1.50     0.626448\n1                      -1.15     0.596964\n2                      -0.80     0.566762\n3                      -0.45     0.536056\n4                      -0.10     0.505074\n\n\n\n\nCode\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n    has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n)\n\nfig = plt.figure()\n\n# Create a scatter plot with logistic trend line\nsns.regplot(x='time_since_first_purchase', y='has_churned', data=churn, ci=None, logistic=True)\n\n# Overlay with prediction_data, colored red\nsns.scatterplot(x='time_since_first_purchase',y='has_churned',\ndata=prediction_data, color='red')\n\nplt.show()"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#most-likely-outcome",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#most-likely-outcome",
    "title": "Simple Logistic Regression Modeling",
    "section": "Most likely outcome",
    "text": "Most likely outcome\nA non-technical audience may appreciate you not discussing probabilities and simply explaining the most likely outcome. Thus, instead of stating that there is a 60% chance of a customer leaving, you state that churn is the most likely outcome. There is a trade-off here between easier interpretation and nuance.\n\n\nCode\n# Update prediction data by adding most_likely_outcome\nprediction_data[\"most_likely_outcome\"] = np.round(prediction_data['has_churned'])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome\n0                      -1.50     0.626448                  1.0\n1                      -1.15     0.596964                  1.0\n2                      -0.80     0.566762                  1.0\n3                      -0.45     0.536056                  1.0\n4                      -0.10     0.505074                  1.0\n\n\n\n\nCode\n# Update prediction data by adding most_likely_outcome\nprediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n\nfig = plt.figure()\n\n# Create a scatter plot with logistic trend line (from previous exercise)\nsns.regplot(x=\"time_since_first_purchase\",\n            y=\"has_churned\",\n            data=churn,\n            ci=None,\n            logistic=True)\n\n# Overlay with prediction_data, colored red\nsns.scatterplot(x='time_since_first_purchase', y='most_likely_outcome', data=prediction_data, color='red')\n\nplt.show()\nprint(\"\\nThe most likely outcome is that you will master logistic regression! Providing the most likely response is a great way to share the model results with a non-technical audience.\")\n\n\n\n\n\n\nThe most likely outcome is that you will master logistic regression! Providing the most likely response is a great way to share the model results with a non-technical audience."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#odds-ratio",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#odds-ratio",
    "title": "Simple Logistic Regression Modeling",
    "section": "Odds ratio",
    "text": "Odds ratio\nAn odds ratio is a measure of the probability of something occurring compared to the probability that it will not occur. Often, this is easier to understand than probabilities, particularly when making decisions regarding choices. If, for example, a customer has a 20% chance of churning, it may be more intuitive to state “the chances of them not churning are four times higher than the chances of them churning.”.\n\n\nCode\n# Update prediction data with odds_ratio\nprediction_data[\"odds_ratio\"] =  prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio\n0                      -1.50     0.626448                  1.0    1.677003\n1                      -1.15     0.596964                  1.0    1.481166\n2                      -0.80     0.566762                  1.0    1.308199\n3                      -0.45     0.536056                  1.0    1.155431\n4                      -0.10     0.505074                  1.0    1.020502\n\n\n\n\nCode\n# Update prediction data with odds_ratio\nprediction_data[\"odds_ratio\"] = prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n\nfig = plt.figure()\n\n# Create a line plot of odds_ratio vs time_since_first_purchase\nsns.lineplot(x='time_since_first_purchase', y='odds_ratio',data=prediction_data)\n\n# Add a dotted horizontal line at odds_ratio = 1\nplt.axhline(y=1, linestyle=\"dotted\")\n\nplt.show()\n\nprint(\"\\nOdds ratios provide an alternative to probabilities that make it easier to compare positive and negative responses\")\n\n\n\n\n\n\nOdds ratios provide an alternative to probabilities that make it easier to compare positive and negative responses"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#log-odds-ratio",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#log-odds-ratio",
    "title": "Simple Logistic Regression Modeling",
    "section": "Log odds ratio",
    "text": "Log odds ratio\nThe disadvantage of probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. It is therefore difficult to understand what happens to the prediction when the explanatory variable is changed. The logarithm of the odds ratio (the “log odds ratio” or “logit”) does exhibit a linear relationship between predicted response and explanatory variable. As the explanatory variable changes, the response metric does not change significantly - only linearly.\nFor visualization purposes, it is usually better to plot the odds ratio and apply a log transformation to the y-axis scale since the actual values of log odds ratio are less intuitive than (linear) odds ratio.\n\n\nCode\n# Update prediction data with log_odds_ratio\nprediction_data[\"log_odds_ratio\"] = np.log(prediction_data['odds_ratio'])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio  \\\n0                      -1.50     0.626448                  1.0    1.677003   \n1                      -1.15     0.596964                  1.0    1.481166   \n2                      -0.80     0.566762                  1.0    1.308199   \n3                      -0.45     0.536056                  1.0    1.155431   \n4                      -0.10     0.505074                  1.0    1.020502   \n\n   log_odds_ratio  \n0        0.517008  \n1        0.392830  \n2        0.268651  \n3        0.144473  \n4        0.020295  \n\n\n\n\nCode\n# Update prediction data with log_odds_ratio\nprediction_data[\"log_odds_ratio\"] = np.log(prediction_data[\"odds_ratio\"])\n\nfig = plt.figure()\n\n# Update the line plot: log_odds_ratio vs. time_since_first_purchase\nsns.lineplot(x=\"time_since_first_purchase\",\n             y=\"log_odds_ratio\",\n             data=prediction_data)\n\n# Add a dotted horizontal line at log_odds_ratio = 0\nplt.axhline(y=0, linestyle=\"dotted\")\n\nplt.show()\nprint(\"\\nThe linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about.\")\n\n\n\n\n\n\nThe linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#section",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#section",
    "title": "Simple Logistic Regression Modeling",
    "section": "",
    "text": "Quantifying logistic regression fit\nResid plot, QQplot & Scale location plot are less useful in the case of logistic regression. Instead, we can use confusion matrices to analyze the fit performance. With True/False positive & negative outcomes. We can also compute metrics based on various ratios.\nAccuracy : proportion of correct predictions. Higher better.\nTN+TP / (TN+FN+FP+TP)\nSensitivity : proportions of observations where the actual response was true and where the model also predicted it was true. Higher better.\nTP / (FN + TP)\nSpecificity : proportions of observations where the actual was false and where the model also predicted it was false. Higher better.\nTN / (TN + FP) Calculating the confusion matrix\nA confusion matrix (occasionally called a confusion table) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.\nTrue positive: The customer churned and the model predicted they would.\nFalse positive: The customer didn't churn, but the model predicted they would.\nTrue negative: The customer didn't churn and the model predicted they wouldn't.\nFalse negative: The customer churned, but the model predicted they wouldn't.\n\n\nCode\n# Get the actual responses\nactual_response = churn[\"has_churned\"]\n\n# Get the predicted responses\npredicted_response = np.round(mdl_churn_vs_relationship.predict())\n\n# Create outcomes as a DataFrame of both Series\noutcomes = pd.DataFrame({\"actual_response\": actual_response,\n                         \"predicted_response\": predicted_response})\n\n# Print the outcomes\nprint(outcomes.value_counts(sort = False))\n\n\nactual_response  predicted_response\n0                0.0                   112\n                 1.0                    88\n1                0.0                    76\n                 1.0                   124\ndtype: int64\n\n\n\n\nCode\nconf_matrix = pd.crosstab(outcomes['actual_response'], outcomes['predicted_response'], rownames=['Actual'], colnames=['Predicted'])\nprint(conf_matrix)\n\n\nPredicted  0.0  1.0\nActual             \n0          112   88\n1           76  124"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#drawing-a-mosaic-plot-of-the-confusion-matrix",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#drawing-a-mosaic-plot-of-the-confusion-matrix",
    "title": "Simple Logistic Regression Modeling",
    "section": "Drawing a mosaic plot of the confusion matrix",
    "text": "Drawing a mosaic plot of the confusion matrix\nWhile calculating the performance matrix might be fun, it would become tedious if you needed multiple confusion matrices of different models. Luckily, the .pred_table() method can calculate the confusion matrix for you.\nAdditionally, you can use the output from the .pred_table() method to visualize the confusion matrix, using the mosaic() function.\n\n\nCode\n# Import mosaic from statsmodels.graphics.mosaicplot\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# Calculate the confusion matrix conf_matrix\nconf_matrix = mdl_churn_vs_relationship.pred_table()\n\n# Print it\nprint(conf_matrix)\n\n# Draw a mosaic plot of conf_matrix\nmosaic(conf_matrix)\nplt.show()\n\n\n[[112.  88.]\n [ 76. 124.]]"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#measuring-logistic-model-performance",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#measuring-logistic-model-performance",
    "title": "Simple Logistic Regression Modeling",
    "section": "Measuring logistic model performance",
    "text": "Measuring logistic model performance\nAs you know by now, several metrics exist for measuring the performance of a logistic regression model. In this last exercise, you’ll manually calculate accuracy, sensitivity, and specificity.\nAccuracy is the proportion of predictions that are correct.\naccuracy = (TN + TP) / TN + FN + FP + TP\nSensitivity is the proportion of true observations that are correctly predicted by the model as being true\nsensitivity = TP / (TP + FN)\nspecificity is the proportion of false observations that are correctly predicted by the model as being false.\nspecificity = TN / (TN + FP)\n\n\nCode\n# Extract TN, TP, FN and FP from conf_matrix\nTN = conf_matrix[0,0]\nTP = conf_matrix[1,1]\nFN = conf_matrix[1,0]\nFP = conf_matrix[0,1]\n\n# Calculate and print the accuracy\naccuracy = (TN + TP) / (TN + FN + FP + TP)\nprint(\"accuracy: \", accuracy)\n\n# Calculate and print the sensitivity\nsensitivity = TP / (TP + FN)\nprint(\"sensitivity: \", sensitivity)\n\n# Calculate and print the specificity\nspecificity = TN / (TN + FP)\nprint(\"specificity: \", specificity)\n\nprint(\"\\n Using these metrics, it becomes much easier to interpret and compare logistic regression models.\")\n\n\naccuracy:  0.59\nsensitivity:  0.62\nspecificity:  0.56\n\n Using these metrics, it becomes much easier to interpret and compare logistic regression models."
  },
  {
    "objectID": "posts/Summary of statistics/Summary Of Statistics.html",
    "href": "posts/Summary of statistics/Summary Of Statistics.html",
    "title": "Summary Of Statistics",
    "section": "",
    "text": "Datacamp course: Introduction to Statistic in Python\nThe study of statistics involves collecting, analyzing, and interpreting data. You can use it to bring the future into focus and infer answers to tons of questions. How many calls will your support team receive, and how many jeans sizes should you manufacture to fit 95% of the population? Statistical skills are developed in this course, which teaches you how to calculate averages, plot relationships between numeric values, and calculate correlations. In addition, you’ll learn how to conduct a well-designed study using Python to draw your own conclusions.\nCourse Takeaways:\n\nSummary Statistics\nRandom Numbers & Probability\nMore Distributions and the Central Limit Theorem\nCorrelation and Experimental Design\n\n\n\nStatistics - what is it?\n\nStatistics is the practice and study of collecting and analyzing data\nA summary statistic is a fact about or a summary of some data\n\nHow can statistics be used?\n\nDoes a product have a high likelihood of being purchased? People are more likely to purchase the product if they are familiar with it\nIs there an alternative payment system available?\nCan you tell me how many occupants your hotel will have? In what ways can you optimize occupancy?\nTo meet the needs of 95% of the population, how many sizes of jeans should be manufactured?\nCan the same number of each size be produced?\nA/B tests: Which advertisement is more effective in motivating the purchase of a product?\n\n\n\n\nDescriptive: To describe & summarize data e.g. 25% ride bike, 35% take bus ride & 50% drive to work\nInferential : Use sample data to make inferences about a larger population e.g. what percent of people drive to work?\n\n\n\n\n\nNumeric (quantitative)\nContinuous (measured)\n\nairplance speed\ntime spent waiting\n\nDiscrete (counted)\n\nnumber of devices\nnumber of people\n\nCategorical (qualitative)\nNominal (unordered)\n\nsingle / married\ncountry of residence\n\nOrdinal (ordered) agree, disagree, strongly diagree\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\nCode\nfood_consumption=pd.read_csv('food_consumption.csv')\n\n\n\n\nCode\n# Filter for Belgium\nbe_consumption = food_consumption[food_consumption['country']=='Belgium']\n\n# Filter for USA\nusa_consumption = food_consumption[food_consumption['country']=='USA']\n\n# Calculate mean and median consumption in Belgium\nprint(np.mean(be_consumption['consumption']))\nprint(np.median(be_consumption['consumption']))\n\n# Calculate mean and median consumption in USA\nprint(np.mean(usa_consumption['consumption']))\nprint(np.median(usa_consumption['consumption']))\n\n\n42.13272727272727\n12.59\n44.650000000000006\n14.58\n\n\n\n\nCode\n# Subset for Belgium and USA only\nbe_and_usa = food_consumption[(food_consumption['country']=='Belgium') | (food_consumption['country']=='USA')]\n\n# Group by country, select consumption column, and compute mean and median\nprint(be_and_usa.groupby('country')['consumption'].agg([np.mean,np.median]))\n\n\n              mean  median\ncountry                   \nBelgium  42.132727   12.59\nUSA      44.650000   14.58\n\n\nMean vs Median\n\n\nCode\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Subset for food_category equals rice\nrice_consumption = food_consumption[food_consumption['food_category']=='rice']\n\n# Histogram of co2_emission for rice and show plot\nplt.hist(rice_consumption['co2_emission'])\nplt.show()\n\n\n\n\n\n\n\nCode\n# Calculate mean and median of co2_emission with .agg()\nprint(rice_consumption['co2_emission'].agg([np.mean,np.median]))\n\n\nmean      37.591615\nmedian    15.200000\nName: co2_emission, dtype: float64\n\n\n\n\n\n\nVariance: Average distance from each data point to the data’s mean\nStandard Deviation\n\n\n\nCode\n# Calculate the quartiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,5)))\n\n# Calculate the quintiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,6)))\n\n# Calculate the deciles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,11)))\n\n\n[   0.        5.21     16.53     62.5975 1712.    ]\n[   0.       3.54    11.026   25.59    99.978 1712.   ]\n[0.00000e+00 6.68000e-01 3.54000e+00 7.04000e+00 1.10260e+01 1.65300e+01\n 2.55900e+01 4.42710e+01 9.99780e+01 2.03629e+02 1.71200e+03]\n\n\n\n\n\nA variable’s variance and standard deviation are two of the most common ways to measure its spread, and you will practice calculating them in this exercise. Spread informs expectations. In other words, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10, they might sell 40 products one day, and one or two the next. Predictions require information like this.\n\n\nCode\n# Print variance and sd of co2_emission for each food_category\nprint(food_consumption.groupby('food_category')['co2_emission'].agg([np.var,np.std]))\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Create histogram of co2_emission for food_category 'beef'\nplt.hist(food_consumption[food_consumption['food_category']=='beef']['co2_emission'])\n# Show plot\nplt.show()\n\n# Create histogram of co2_emission for food_category 'eggs'\nplt.hist(food_consumption[food_consumption['food_category']=='eggs']['co2_emission'])\n# Show plot\nplt.show()\n\n\n                        var         std\nfood_category                          \nbeef           88748.408132  297.906710\ndairy          17671.891985  132.935669\neggs              21.371819    4.622966\nfish             921.637349   30.358481\nlamb_goat      16475.518363  128.356996\nnuts              35.639652    5.969895\npork            3094.963537   55.632396\npoultry          245.026801   15.653332\nrice            2281.376243   47.763754\nsoybeans           0.879882    0.938020\nwheat             71.023937    8.427570\n\n\n\n\n\n\n\n\nFinding outliers using IQR\nOutliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that’s less influenced by outliers. IQR is also often used to find outliers. If a value is less than Q1−1.5×IQRQ1−1.5×IQR or greater than Q3+1.5×IQRQ3+1.5×IQR, it’s considered an outlier.\n\n\nCode\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\nprint(emissions_by_country)\n\n\ncountry\nAlbania      1777.85\nAlgeria       707.88\nAngola        412.99\nArgentina    2172.40\nArmenia      1109.93\n              ...   \nUruguay      1634.91\nVenezuela    1104.10\nVietnam       641.51\nZambia        225.30\nZimbabwe      350.33\nName: co2_emission, Length: 130, dtype: float64\n\n\n\n\nCode\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quantiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n\n# Subset emissions_by_country to find outliers\noutliers = emissions_by_country[(emissions_by_country > upper) | (emissions_by_country < lower)]\nprint(outliers)\n\n\ncountry\nArgentina    2172.4\nName: co2_emission, dtype: float64\n\n\n\n\n\n\nIn this chapter, you’ll learn how to generate random samples and measure chance using probability. You’ll work with real-world sales data to calculate the probability of a salesperson being successful. Finally, you’ll use the binomial distribution to model events with binary outcomes"
  },
  {
    "objectID": "posts/Intoducting to sampling/Introduction to Sampling.html",
    "href": "posts/Intoducting to sampling/Introduction to Sampling.html",
    "title": "Introduction to sampling",
    "section": "",
    "text": "Get a better understanding of what sampling is and why it is so powerful. Additionally, We will learn about the problems associated with convenience sampling and what the difference between true randomness and pseudo-randomness is.\nThis Introduction to sampling is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\n\nPopulation: It is complete dataset\nSample: It is subset of data you calculate on\n\nPopulation parameter: It is a calculation on population dataset Points vs. flavor: population pts_vs_flavor_pop = coffee_ratings[[“total_cup_points”, “flavor”]] np.mean(pts_vs_flavor_pop[‘total_cup_points’])\nPoint estimate: Or sample statistic is a calculation made on sample dataset Points vs. flavor: 10 row sample pts_vs_flavor_samp = pts_vs_flavor_pop.sample(n=10) cup_points_samp = coffee_ratings[‘total_cup_points’].sample(n=10) np.mean(cup_points_samp)\n\n\n\nThe purpose of this exercise is to explore Spotify song data. There are over 40,000 rows in this population dataset, each representing a song. These columns include the title of the song, the artists who performed it, the release year, and attributes of the song, such as its duration, tempo, and danceability. To begin, you should examine the durations.\nThe Spotify dataset will be sampled and the mean duration of the sample will be compared with the mean duration of the population.\n\n\nCode\nspotify_population=pd.read_feather(\"dataset/spotify_2000_2020.feather\")\nspotify_population.head()\n\n\n\n\n\n\n  \n    \n      \n      acousticness\n      artists\n      danceability\n      duration_ms\n      duration_minutes\n      energy\n      explicit\n      id\n      instrumentalness\n      key\n      liveness\n      loudness\n      mode\n      name\n      popularity\n      release_date\n      speechiness\n      tempo\n      valence\n      year\n    \n  \n  \n    \n      0\n      0.97200\n      ['David Bauer']\n      0.567\n      313293.0\n      5.221550\n      0.227\n      0.0\n      0w0D8H1ubRerCXHWYJkinO\n      0.601000\n      10.0\n      0.110\n      -13.441\n      1.0\n      Shout to the Lord\n      47.0\n      2000\n      0.0290\n      136.123\n      0.0396\n      2000.0\n    \n    \n      1\n      0.32100\n      ['Etta James']\n      0.821\n      360240.0\n      6.004000\n      0.418\n      0.0\n      4JVeqfE2tpi7Pv63LJZtPh\n      0.000372\n      9.0\n      0.222\n      -9.841\n      0.0\n      Miss You\n      51.0\n      2000-12-12\n      0.0407\n      117.382\n      0.8030\n      2000.0\n    \n    \n      2\n      0.00659\n      ['Quasimoto']\n      0.706\n      202507.0\n      3.375117\n      0.602\n      1.0\n      5pxtdhLAi0RTh1gNqhGMNA\n      0.000138\n      11.0\n      0.400\n      -8.306\n      0.0\n      Real Eyes\n      44.0\n      2000-06-13\n      0.3420\n      89.692\n      0.4790\n      2000.0\n    \n    \n      3\n      0.00390\n      ['Millencolin']\n      0.368\n      173360.0\n      2.889333\n      0.977\n      0.0\n      3jRsoe4Vkxa4BMYqGHX8L0\n      0.000000\n      11.0\n      0.350\n      -2.757\n      0.0\n      Penguins & Polarbears\n      52.0\n      2000-02-22\n      0.1270\n      165.889\n      0.5480\n      2000.0\n    \n    \n      4\n      0.12200\n      ['Steve Chou']\n      0.501\n      344200.0\n      5.736667\n      0.511\n      0.0\n      4mronxcllhfyhBRqyZi8kU\n      0.000000\n      7.0\n      0.279\n      -9.836\n      0.0\n      黃昏\n      53.0\n      2000-12-25\n      0.0291\n      78.045\n      0.1130\n      2000.0\n    \n  \n\n\n\n\n\n\nCode\n# Sample 1000 rows from spotify_population\nspotify_sample = spotify_population.sample(n=1000)\n\n# Print the sample\nprint(spotify_sample)\n\n\n       acousticness                        artists  danceability  duration_ms  \\\n23850      0.001130                  ['Lil Wayne']         0.769     199427.0   \n41092      0.121000  ['Justin Bieber', 'Big Sean']         0.644     286853.0   \n25592      0.000120                     ['O.A.R.']         0.652     207493.0   \n15256      0.974000          [\"Dustin O'Halloran\"]         0.429     188400.0   \n40880      0.280000       ['Two Door Cinema Club']         0.731     286093.0   \n...             ...                            ...           ...          ...   \n5239       0.082500               ['Matt and Kim']         0.549     212560.0   \n37135      0.354000                         ['NF']         0.501     221573.0   \n36982      0.244000               ['Lil Uzi Vert']         0.626     248175.0   \n38148      0.562000    ['Tego Calderon', 'Yandel']         0.848     184480.0   \n40085      0.000078                  ['Rammstein']         0.496     235627.0   \n\n       duration_minutes  energy  explicit                      id  \\\n23850          3.323783  0.6600       1.0  58PWl2rA7lU90UBvDufzWm   \n41092          4.780883  0.5950       0.0  23UV8pJEvRnAZd6ZAXBC0H   \n25592          3.458217  0.7070       0.0  7n9GxM3guijiK2isLfUgJt   \n15256          3.140000  0.0245       0.0  6CngZA32wwYDbfY0f3pVj2   \n40880          4.768217  0.7300       0.0  7GDDxCiTyiQuC1kig8AqFH   \n...                 ...     ...       ...                     ...   \n5239           3.542667  0.8820       0.0  7vLeZPIeblZLz04hLRRAES   \n37135          3.692883  0.3540       0.0  0wf9rSX8ds3yEI2gf2QdPs   \n36982          4.136250  0.7700       1.0  2p2DLGuXtnLLClzpmKkyCx   \n38148          3.074667  0.8010       1.0  4UxD6L5uah5Dvzs5kQwThG   \n40085          3.927117  0.9340       0.0  4hLS1e3mrL0gbm8BqX1VvN   \n\n       instrumentalness   key  liveness  loudness  mode  \\\n23850          0.000000   9.0    0.0780    -7.375   1.0   \n41092          0.000000   1.0    0.2790    -6.877   1.0   \n25592          0.000003   4.0    0.4850    -4.552   1.0   \n15256          0.917000   5.0    0.1100   -31.444   1.0   \n40880          0.000000   7.0    0.0880    -6.728   1.0   \n...                 ...   ...       ...       ...   ...   \n5239           0.000000   7.0    0.3720    -4.911   1.0   \n37135          0.000003  11.0    0.1110   -13.031   0.0   \n36982          0.000000   2.0    0.2290    -6.368   1.0   \n38148          0.000011  10.0    0.0624    -6.821   1.0   \n40085          0.344000   5.0    0.1150    -5.305   1.0   \n\n                         name  popularity release_date  speechiness    tempo  \\\n23850                  3 Peat        58.0   2008-06-10       0.3310  153.092   \n41092             No Pressure        63.0   2015-11-13       0.2150  148.889   \n25592       Love and Memories        55.0   2005-10-04       0.0297  121.998   \n15256                 Opus 23        44.0   2006-10-10       0.0364   71.187   \n40880   Next Year - RAC Remix        50.0   2013-01-01       0.0489  123.051   \n...                       ...         ...          ...          ...      ...   \n5239                  Cameras        46.0   2010-11-02       0.0505  169.994   \n37135                  Dreams        65.0   2017-10-06       0.1770   86.289   \n36982                 Enemies        57.0   2015-12-18       0.2500  155.738   \n38148  Cuando Baila Reggaeton        55.0   2006-08-29       0.1020   95.027   \n40085         TE QUIERO PUTA!        53.0   2005-10-27       0.1450  159.931   \n\n       valence    year  \n23850    0.471  2008.0  \n41092    0.554  2015.0  \n25592    0.655  2005.0  \n15256    0.197  2006.0  \n40880    0.397  2013.0  \n...        ...     ...  \n5239     0.645  2010.0  \n37135    0.301  2017.0  \n36982    0.478  2015.0  \n38148    0.908  2006.0  \n40085    0.268  2005.0  \n\n[1000 rows x 20 columns]\n\n\n\n\nCode\n# Calculate the mean duration in mins from spotify_population\nmean_dur_pop = spotify_population['duration_minutes'].mean()\n\n# Calculate the mean duration in mins from spotify_sample\nmean_dur_samp = spotify_sample['duration_minutes'].mean()\n\n# Print the means\nprint(mean_dur_pop)\nprint(mean_dur_samp)\nprint(\"\\n Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.\")\n\n\n3.8521519140900073\n3.8350666166666665\n\n Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.\n\n\n\n\n\n\n\nCode\n# Subset the loudness column of spotify_population\nloudness_pop = spotify_population['loudness']\n\n# Sample 100 values of loudness_pop\nloudness_samp = loudness_pop.sample(n=100)\n\n# Print the sample\nprint(loudness_samp)\n\n\n9068     -9.554\n16607   -26.071\n6391    -10.661\n29709    -4.182\n32548    -3.494\n          ...  \n11177    -5.791\n18327    -6.229\n35183    -5.448\n3999    -11.212\n38095    -6.221\nName: loudness, Length: 100, dtype: float64\n\n\n\n\nCode\n# Calculate the mean of loudness_pop\nmean_loudness_pop = np.mean(loudness_pop)\n\n# Calculate the mean of loudness_samp\nmean_loudness_samp = np.mean(loudness_samp)\n\n# Print the means\nprint(mean_loudness_pop)\nprint(mean_loudness_samp)\nprint(\"\\n Again, notice that the calculated value (the mean) is close but not identical in each case\")\n\n\n-7.366856851353947\n-7.382759999999999\n\n Again, notice that the calculated value (the mean) is close but not identical in each case\n\n\n\n\n\nCollecting data by easiest method is convenience sampling\nSample bias: sample not true representation of population Selection bias\n\n\n\nIn your previous example, you saw that convenience sampling, which is the collection of data using the simplest method, can lead to samples that are not representative of the population. In other words, the findings of the sample cannot be generalized to the entire population. It is possible to determine whether or not a sample is representative of the population by examining the distributions of the population and the sample\n\n\nCode\n# Visualize the distribution of acousticness with a histogram\nwidth = 0.01\nspotify_population['acousticness'].hist(bins=np.arange(0,1.01,width))\nplt.show()\n\n\n\n\n\n\n\nCode\nspotify_mysterious_sample=spotify_population.sample(n=1107)\n# Update the histogram to use spotify_mysterious_sample\nspotify_mysterious_sample['acousticness'].hist(bins=np.arange(0, 1.01, 0.01))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Visualize the distribution of duration_minutes as a histogram\nspotify_population['duration_minutes'].hist(bins=np.arange(0,15.5,0.5))\nplt.show()\n\n\n\n\n\n\n\nCode\nspotify_mysterious_sample2=spotify_population.sample(n=50)\n# Update the histogram to use spotify_mysterious_sample2\nspotify_mysterious_sample2['duration_minutes'].hist(bins=np.arange(0, 15.5, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate random numbers from a Uniform(-3, 3)\nuniforms = np.random.uniform(low=-3, high=3, size=5000)\n\n# Print uniforms\nprint(uniforms)\n\n# Plot a histogram of uniform values, binwidth 0.25\nplt.hist(uniforms, bins=np.arange(-3,3.25,0.25))\nplt.show()\n\n\n[-2.12436214  0.26133352  2.10798367 ... -2.25488851 -1.16266127\n -1.16448391]\n\n\n\n\n\n\n\nCode\n# Generate random numbers from a Normal(5, 2)\nnormals = np.random.normal(loc=5,scale=2,size=5000)\n\n# Print normals\nprint(normals)\n\n# Plot a histogram of normal values, binwidth 0.5\nplt.hist(normals,np.arange(-2,13.5,0.5))\nplt.show()\n\n\n[2.06050457 2.77955126 1.88655315 ... 6.44834334 6.68448638 7.13942115]\n\n\n\n\n\n\n\nCode"
  },
  {
    "objectID": "posts/Sampling Methods/Sampling Methods.html",
    "href": "posts/Sampling Methods/Sampling Methods.html",
    "title": "Sampling Methods",
    "section": "",
    "text": "It’s time to get hands-on and perform the four random sampling methods in Python: simple, systematic, stratified, and cluster.\nThis Sampling Methods is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nAlthough there are several sampling methods such as: * Simple random sampling * Systematic random sampling * Stratified & weight random sampling * Cluster sampling\n\nSimple random sampling: Work like raffle or lottery & consider simplest method of sampling a population. involves picking rows at random, one at a time, where each row has the same chance of being picked as any other\nSystematic random sampling: This samples the population at regular intervals and this method avoid randomness\n\n\n\n\n\nCode\nattrition_pop=pd.read_feather('dataset/attrition.feather')\nattrition_pop.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      Attrition\n      BusinessTravel\n      DailyRate\n      Department\n      DistanceFromHome\n      Education\n      EducationField\n      EnvironmentSatisfaction\n      Gender\n      ...\n      PerformanceRating\n      RelationshipSatisfaction\n      StockOptionLevel\n      TotalWorkingYears\n      TrainingTimesLastYear\n      WorkLifeBalance\n      YearsAtCompany\n      YearsInCurrentRole\n      YearsSinceLastPromotion\n      YearsWithCurrManager\n    \n  \n  \n    \n      0\n      21\n      0.0\n      Travel_Rarely\n      391\n      Research_Development\n      15\n      College\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      6\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      1\n      19\n      1.0\n      Travel_Rarely\n      528\n      Sales\n      22\n      Below_College\n      Marketing\n      Very_High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      2\n      Good\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18\n      1.0\n      Travel_Rarely\n      230\n      Research_Development\n      3\n      Bachelor\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      High\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18\n      0.0\n      Travel_Rarely\n      812\n      Sales\n      10\n      Bachelor\n      Medical\n      Very_High\n      Female\n      ...\n      Excellent\n      Low\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18\n      1.0\n      Travel_Frequently\n      1306\n      Sales\n      5\n      Bachelor\n      Marketing\n      Medium\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      3\n      Better\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 31 columns\n\n\n\n\n\nCode\n# Sample 70 rows using simple random sampling and set the seed\nattrition_samp = attrition_pop.sample(n=70, random_state=18900217)\n\n# Print the sample\nprint(attrition_samp)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1134   35        0.0      Travel_Rarely        583  Research_Development   \n1150   52        0.0         Non-Travel        585                 Sales   \n531    33        0.0      Travel_Rarely        931  Research_Development   \n395    31        0.0      Travel_Rarely       1332  Research_Development   \n392    29        0.0      Travel_Rarely        942  Research_Development   \n...   ...        ...                ...        ...                   ...   \n361    27        0.0  Travel_Frequently       1410                 Sales   \n1180   36        0.0      Travel_Rarely        530                 Sales   \n230    26        0.0      Travel_Rarely       1443                 Sales   \n211    29        0.0  Travel_Frequently        410  Research_Development   \n890    30        0.0  Travel_Frequently       1312  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1134                25         Master           Medical   \n1150                29         Master     Life_Sciences   \n531                 14       Bachelor           Medical   \n395                 11        College           Medical   \n392                 15  Below_College     Life_Sciences   \n...                ...            ...               ...   \n361                  3  Below_College           Medical   \n1180                 2         Master     Life_Sciences   \n230                 23       Bachelor         Marketing   \n211                  2  Below_College     Life_Sciences   \n890                  2         Master  Technical_Degree   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1134                    High  Female  ...          Excellent   \n1150                     Low    Male  ...          Excellent   \n531                Very_High  Female  ...          Excellent   \n395                     High    Male  ...          Excellent   \n392                   Medium  Female  ...          Excellent   \n...                      ...     ...  ...                ...   \n361                Very_High  Female  ...        Outstanding   \n1180                    High  Female  ...          Excellent   \n230                     High  Female  ...          Excellent   \n211                Very_High  Female  ...          Excellent   \n890                Very_High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1134                     High                 1                16   \n1150                   Medium                 2                16   \n531                 Very_High                 1                 8   \n395                 Very_High                 0                 6   \n392                       Low                 1                 6   \n...                       ...               ...               ...   \n361                    Medium                 2                 6   \n1180                     High                 0                17   \n230                      High                 1                 5   \n211                      High                 3                 4   \n890                 Very_High                 0                10   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1134                     3            Good              16   \n1150                     3            Good               9   \n531                      5          Better               8   \n395                      2            Good               6   \n392                      2            Good               5   \n...                    ...             ...             ...   \n361                      3          Better               6   \n1180                     2            Good              13   \n230                      2            Good               2   \n211                      3          Better               3   \n890                      2          Better               9   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1134                  10                       10                    1  \n1150                   8                        0                    0  \n531                    7                        1                    6  \n395                    5                        0                    1  \n392                    4                        1                    3  \n...                  ...                      ...                  ...  \n361                    5                        0                    4  \n1180                   7                        6                    7  \n230                    2                        0                    0  \n211                    2                        0                    2  \n890                    7                        0                    7  \n\n[70 rows x 31 columns]\n\n\n\n\n\n\n\nCode\n# Set the sample size to 70\nsample_size = 70\n\n# Calculate the population size from attrition_pop\npop_size = len(attrition_pop)\n\n# Calculate the interval\ninterval = pop_size // sample_size\n\n# Systematically sample 70 rows\nattrition_sys_samp = attrition_pop.iloc[::interval]\n\n# Print the sample\nprint(attrition_sys_samp)\n\n\n      Age  Attrition BusinessTravel  DailyRate            Department  \\\n0      21        0.0  Travel_Rarely        391  Research_Development   \n21     19        0.0  Travel_Rarely       1181  Research_Development   \n42     45        0.0  Travel_Rarely        252  Research_Development   \n63     23        0.0  Travel_Rarely        373  Research_Development   \n84     30        1.0  Travel_Rarely        945                 Sales   \n...   ...        ...            ...        ...                   ...   \n1365   48        0.0  Travel_Rarely        715  Research_Development   \n1386   48        0.0  Travel_Rarely       1355  Research_Development   \n1407   50        0.0  Travel_Rarely        989  Research_Development   \n1428   50        0.0     Non-Travel        881  Research_Development   \n1449   52        0.0  Travel_Rarely        699  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n0                   15        College  Life_Sciences                    High   \n21                   3  Below_College        Medical                  Medium   \n42                   2       Bachelor  Life_Sciences                  Medium   \n63                   1        College  Life_Sciences               Very_High   \n84                   9       Bachelor        Medical                  Medium   \n...                ...            ...            ...                     ...   \n1365                 1       Bachelor  Life_Sciences               Very_High   \n1386                 4         Master  Life_Sciences                    High   \n1407                 7        College        Medical                  Medium   \n1428                 2         Master  Life_Sciences                     Low   \n1449                 1         Master  Life_Sciences                    High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n0       Male  ...          Excellent                Very_High   \n21    Female  ...          Excellent                Very_High   \n42    Female  ...          Excellent                Very_High   \n63      Male  ...        Outstanding                Very_High   \n84      Male  ...          Excellent                     High   \n...      ...  ...                ...                      ...   \n1365    Male  ...          Excellent                     High   \n1386    Male  ...          Excellent                   Medium   \n1407  Female  ...          Excellent                Very_High   \n1428    Male  ...          Excellent                Very_High   \n1449    Male  ...          Excellent                      Low   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n0                    0                 0                     6   \n21                   0                 1                     3   \n42                   0                 1                     3   \n63                   1                 1                     2   \n84                   0                 1                     3   \n...                ...               ...                   ...   \n1365                 0                25                     3   \n1386                 0                27                     3   \n1407                 1                29                     2   \n1428                 1                31                     3   \n1449                 1                34                     5   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n0             Better               0                   0   \n21            Better               1                   0   \n42            Better               1                   0   \n63            Better               1                   0   \n84              Good               1                   0   \n...              ...             ...                 ...   \n1365            Best               1                   0   \n1386          Better              15                  11   \n1407            Good              27                   3   \n1428          Better              31                   6   \n1449          Better              33                  18   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n0                           0                    0  \n21                          0                    0  \n42                          0                    0  \n63                          0                    1  \n84                          0                    0  \n...                       ...                  ...  \n1365                        0                    0  \n1386                        4                    8  \n1407                       13                    8  \n1428                       14                    7  \n1449                       11                    9  \n\n[70 rows x 31 columns]\n\n\n\n\n\nIn the case of systematic sampling, there is a problem: if the data has been sorted or there is a pattern or meaning behind the row order, then the resulting sample may not be representative of the entire population. If the rows are shuffled, the problem can be solved, but then systematic sampling becomes equivalent to simple random sampling.\n\n\nCode\n# Add an index column to attrition_pop\nattrition_pop_id = attrition_pop.reset_index()\n\n# Plot YearsAtCompany vs. index for attrition_pop_id\nattrition_pop_id.plot(x='index',y='YearsAtCompany',kind='scatter')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Shuffle the rows of attrition_pop\nattrition_shuffled = attrition_pop.sample(frac=1)\n\n# Reset the row indexes and create an index column\nattrition_shuffled = attrition_shuffled.reset_index(drop=True).reset_index()\n\n# Plot YearsAtCompany vs. index for attrition_shuffled\nattrition_shuffled.plot(x='index',y='YearsAtCompany',kind='scatter')\nplt.show()\n\n\n\n\n\n\n\n\nStratified sampling is a technique that allows us to sample a population that contains subgroups\n\nWeighted random sampling\n\nA close relative of stratified sampling that provides even more flexibility is weighted random sampling. In this variant, we create a column of weights that adjust the relative probability of sampling each row.\n\n\n\nYou may need to carefully control the counts of each subgroup within the population if you are interested in subgroups within the population. As a result of proportional stratified sampling, the subgroup sizes within the sample are representative of the subgroup sizes within the population as a whole.\n\n\nCode\n# Proportion of employees by Education level\neducation_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n\n# Print education_counts_pop\nprint(education_counts_pop)\n\n\nBachelor         0.389116\nMaster           0.270748\nCollege          0.191837\nBelow_College    0.115646\nDoctor           0.032653\nName: Education, dtype: float64\n\n\n\n\nCode\n# Proportional stratified sampling for 40% of each Education group\nattrition_strat = attrition_pop.groupby('Education').sample(frac=0.4, random_state=2022)\n\n# Print the sample\nprint(attrition_strat)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1191   53        0.0      Travel_Rarely        238                 Sales   \n407    29        0.0  Travel_Frequently        995  Research_Development   \n1233   59        0.0  Travel_Frequently       1225                 Sales   \n366    37        0.0      Travel_Rarely        571  Research_Development   \n702    31        0.0  Travel_Frequently        163  Research_Development   \n...   ...        ...                ...        ...                   ...   \n733    38        0.0  Travel_Frequently        653  Research_Development   \n1061   44        0.0  Travel_Frequently        602       Human_Resources   \n1307   41        0.0      Travel_Rarely       1276                 Sales   \n1060   33        0.0      Travel_Rarely        516  Research_Development   \n177    29        0.0      Travel_Rarely        738  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1191                 1  Below_College           Medical   \n407                  2  Below_College     Life_Sciences   \n1233                 1  Below_College     Life_Sciences   \n366                 10  Below_College     Life_Sciences   \n702                 24  Below_College  Technical_Degree   \n...                ...            ...               ...   \n733                 29         Doctor     Life_Sciences   \n1061                 1         Doctor   Human_Resources   \n1307                 2         Doctor     Life_Sciences   \n1060                 8         Doctor     Life_Sciences   \n177                  9         Doctor             Other   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1191               Very_High  Female  ...        Outstanding   \n407                      Low    Male  ...          Excellent   \n1233                     Low  Female  ...          Excellent   \n366                Very_High  Female  ...          Excellent   \n702                Very_High  Female  ...        Outstanding   \n...                      ...     ...  ...                ...   \n733                Very_High  Female  ...          Excellent   \n1061                     Low    Male  ...          Excellent   \n1307                  Medium  Female  ...          Excellent   \n1060               Very_High    Male  ...          Excellent   \n177                   Medium    Male  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1191                Very_High                 0                18   \n407                 Very_High                 1                 6   \n1233                Very_High                 0                20   \n366                    Medium                 2                 6   \n702                 Very_High                 0                 9   \n...                       ...               ...               ...   \n733                 Very_High                 0                10   \n1061                     High                 0                14   \n1307                   Medium                 1                22   \n1060                      Low                 0                14   \n177                      High                 0                 4   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1191                     2            Best              14   \n407                      0            Best               6   \n1233                     2            Good               4   \n366                      3            Good               5   \n702                      3            Good               5   \n...                    ...             ...             ...   \n733                      2          Better              10   \n1061                     3          Better              10   \n1307                     2          Better              18   \n1060                     6          Better               0   \n177                      2          Better               3   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1191                   7                        8                   10  \n407                    4                        1                    3  \n1233                   3                        1                    3  \n366                    3                        4                    3  \n702                    4                        1                    4  \n...                  ...                      ...                  ...  \n733                    3                        9                    9  \n1061                   7                        0                    2  \n1307                  16                       11                    8  \n1060                   0                        0                    0  \n177                    2                        2                    2  \n\n[588 rows x 31 columns]\n\n\n\n\nCode\n# Calculate the Education level proportions from attrition_strat\neducation_counts_strat = attrition_strat['Education'].value_counts(normalize=True)\n\n# Print education_counts_strat\nprint(education_counts_strat)\nprint('\\nBy grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population.')\n\n\nBachelor         0.389456\nMaster           0.270408\nCollege          0.192177\nBelow_College    0.115646\nDoctor           0.032313\nName: Education, dtype: float64\n\nBy grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population.\n\n\n\n\n\nWhen one subgroup is larger than another in the population, but you do not want to factor this difference into your analysis, you can use equal counts stratified sampling to generate samples in which each subgroup has the same amount of data.\n\n\nCode\n# Get 30 employees from each Education group\nattrition_eq = attrition_pop.groupby('Education').sample(n=30, random_state=2022)\n\n# Print the sample\nprint(attrition_eq)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1191   53        0.0      Travel_Rarely        238                 Sales   \n407    29        0.0  Travel_Frequently        995  Research_Development   \n1233   59        0.0  Travel_Frequently       1225                 Sales   \n366    37        0.0      Travel_Rarely        571  Research_Development   \n702    31        0.0  Travel_Frequently        163  Research_Development   \n...   ...        ...                ...        ...                   ...   \n774    33        0.0      Travel_Rarely        922  Research_Development   \n869    45        0.0      Travel_Rarely       1015  Research_Development   \n530    32        0.0      Travel_Rarely        120  Research_Development   \n1049   48        0.0      Travel_Rarely        163                 Sales   \n350    29        1.0      Travel_Rarely        408  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1191                 1  Below_College           Medical   \n407                  2  Below_College     Life_Sciences   \n1233                 1  Below_College     Life_Sciences   \n366                 10  Below_College     Life_Sciences   \n702                 24  Below_College  Technical_Degree   \n...                ...            ...               ...   \n774                  1         Doctor           Medical   \n869                  5         Doctor           Medical   \n530                  6         Doctor     Life_Sciences   \n1049                 2         Doctor         Marketing   \n350                 25         Doctor  Technical_Degree   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1191               Very_High  Female  ...        Outstanding   \n407                      Low    Male  ...          Excellent   \n1233                     Low  Female  ...          Excellent   \n366                Very_High  Female  ...          Excellent   \n702                Very_High  Female  ...        Outstanding   \n...                      ...     ...  ...                ...   \n774                      Low  Female  ...          Excellent   \n869                     High  Female  ...          Excellent   \n530                     High    Male  ...        Outstanding   \n1049                  Medium  Female  ...          Excellent   \n350                     High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1191                Very_High                 0                18   \n407                 Very_High                 1                 6   \n1233                Very_High                 0                20   \n366                    Medium                 2                 6   \n702                 Very_High                 0                 9   \n...                       ...               ...               ...   \n774                      High                 1                10   \n869                       Low                 0                10   \n530                       Low                 0                 8   \n1049                      Low                 1                14   \n350                    Medium                 0                 6   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1191                     2            Best              14   \n407                      0            Best               6   \n1233                     2            Good               4   \n366                      3            Good               5   \n702                      3            Good               5   \n...                    ...             ...             ...   \n774                      2          Better               6   \n869                      3          Better              10   \n530                      2          Better               5   \n1049                     2          Better               9   \n350                      2            Best               2   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1191                   7                        8                   10  \n407                    4                        1                    3  \n1233                   3                        1                    3  \n366                    3                        4                    3  \n702                    4                        1                    4  \n...                  ...                      ...                  ...  \n774                    1                        0                    5  \n869                    7                        1                    4  \n530                    4                        1                    4  \n1049                   7                        6                    7  \n350                    2                        1                    1  \n\n[150 rows x 31 columns]\n\n\n\n\nCode\n# Get the proportions from attrition_eq\neducation_counts_eq = attrition_eq['Education'].value_counts(normalize=True)\n\n# Print the results\nprint(education_counts_eq)\n\n\nBelow_College    0.2\nCollege          0.2\nBachelor         0.2\nMaster           0.2\nDoctor           0.2\nName: Education, dtype: float64\n\n\n\n\n\nThe stratified sampling method determines the probability of picking rows from your dataset based on the subgroups within your dataset. A generalization of this is weighted sampling, which allows you to specify rules regarding the probability of selecting rows at the row level. A row’s probability of being selected is proportional to its weight value.\n\n\nCode\n# Plot YearsAtCompany from attrition_pop as a histogram\nattrition_pop['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Sample 400 employees weighted by YearsAtCompany\nattrition_weight = attrition_pop.sample(n=400, weights='YearsAtCompany')\n\n# Print the sample\nprint(attrition_weight)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n853    36        0.0      Travel_Rarely        172  Research_Development   \n481    34        0.0  Travel_Frequently        618  Research_Development   \n1148   38        0.0      Travel_Rarely       1321                 Sales   \n1430   51        0.0  Travel_Frequently        237                 Sales   \n517    39        0.0      Travel_Rarely        835  Research_Development   \n...   ...        ...                ...        ...                   ...   \n1351   45        0.0      Travel_Rarely       1038  Research_Development   \n1412   54        0.0      Travel_Rarely        971  Research_Development   \n1248   47        0.0  Travel_Frequently       1379  Research_Development   \n1210   36        0.0  Travel_Frequently        688  Research_Development   \n1328   55        0.0  Travel_Frequently       1091  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n853                  4         Master  Life_Sciences                     Low   \n481                  3  Below_College  Life_Sciences                     Low   \n1148                 1         Master  Life_Sciences               Very_High   \n1430                 9       Bachelor  Life_Sciences               Very_High   \n517                 19         Master          Other               Very_High   \n...                ...            ...            ...                     ...   \n1351                20       Bachelor        Medical                  Medium   \n1412                 1       Bachelor        Medical               Very_High   \n1248                16         Master        Medical                    High   \n1210                 4        College  Life_Sciences               Very_High   \n1328                 2  Below_College  Life_Sciences               Very_High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n853     Male  ...          Excellent                     High   \n481     Male  ...          Excellent                     High   \n1148    Male  ...          Excellent                      Low   \n1430    Male  ...        Outstanding                      Low   \n517     Male  ...          Excellent                   Medium   \n...      ...  ...                ...                      ...   \n1351    Male  ...          Excellent                   Medium   \n1412  Female  ...          Excellent                Very_High   \n1248    Male  ...          Excellent                     High   \n1210  Female  ...          Excellent                   Medium   \n1328    Male  ...          Excellent                   Medium   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n853                  0                10                     2   \n481                  0                 7                     1   \n1148                 2                16                     3   \n1430                 1                31                     5   \n517                  3                 7                     2   \n...                ...               ...                   ...   \n1351                 1                24                     2   \n1412                 0                29                     3   \n1248                 0                20                     3   \n1210                 3                18                     3   \n1328                 1                23                     4   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n853             Good              10                   4   \n481             Good               6                   2   \n1148          Better              15                  13   \n1430            Good              29                  10   \n517           Better               2                   2   \n...              ...             ...                 ...   \n1351          Better               7                   7   \n1412            Good              20                   7   \n1248            Best              19                  10   \n1210          Better               4                   2   \n1328          Better               3                   2   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n853                         1                    8  \n481                         0                    4  \n1148                        5                    8  \n1430                       11                   10  \n517                         2                    2  \n...                       ...                  ...  \n1351                        0                    7  \n1412                       12                    7  \n1248                        2                    7  \n1210                        0                    2  \n1328                        1                    2  \n\n[400 rows x 31 columns]\n\n\n\n\nCode\n# Plot YearsAtCompany from attrition_weight as a histogram\nattrition_weight['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Plot YearsAtCompany from attrition_pop as a histogram\nattrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\nplt.show()\n\n# Sample 400 employees weighted by YearsAtCompany\nattrition_weight = attrition_pop.sample(n=400, weights=\"YearsAtCompany\")\n\n# Plot YearsAtCompany from attrition_weight as a histogram\nattrition_weight['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStratified sampling vs. cluster sampling * Stratified sampling: * Split the population into subgroups * Use simple random sampling on every subgroup * Cluster sampling * Use simple random sampling to pick some subgroups * Use simple random sampling on only those subgroups\n\n\nCode\nimport random\n# Create a list of unique JobRole values\njob_roles_pop = list(attrition_pop['JobRole'].unique())\n\n# Randomly sample four JobRole values\njob_roles_samp = random.sample(job_roles_pop,k=4)\n\n# Print the result\nprint(job_roles_samp)\n\n\n['Research_Director', 'Sales_Executive', 'Sales_Representative', 'Laboratory_Technician']\n\n\n\n\nCode\n# Filter for rows where JobRole is in job_roles_samp\njobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\nattrition_filtered = attrition_pop[jobrole_condition]\n\n# Print the result\nprint(attrition_filtered)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1      19        1.0      Travel_Rarely        528                 Sales   \n2      18        1.0      Travel_Rarely        230  Research_Development   \n3      18        0.0      Travel_Rarely        812                 Sales   \n4      18        1.0  Travel_Frequently       1306                 Sales   \n7      18        1.0         Non-Travel        247  Research_Development   \n...   ...        ...                ...        ...                   ...   \n1457   55        0.0      Travel_Rarely        692  Research_Development   \n1458   56        0.0  Travel_Frequently        906                 Sales   \n1459   54        0.0      Travel_Rarely        685  Research_Development   \n1467   58        0.0      Travel_Rarely        682                 Sales   \n1469   58        1.0      Travel_Rarely        286  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n1                   22  Below_College      Marketing               Very_High   \n2                    3       Bachelor  Life_Sciences                    High   \n3                   10       Bachelor        Medical               Very_High   \n4                    5       Bachelor      Marketing                  Medium   \n7                    8  Below_College        Medical                    High   \n...                ...            ...            ...                     ...   \n1457                14         Master        Medical                    High   \n1458                 6       Bachelor  Life_Sciences                    High   \n1459                 3       Bachelor  Life_Sciences               Very_High   \n1467                10         Master        Medical               Very_High   \n1469                 2         Master  Life_Sciences               Very_High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n1       Male  ...          Excellent                Very_High   \n2       Male  ...          Excellent                     High   \n3     Female  ...          Excellent                      Low   \n4       Male  ...          Excellent                Very_High   \n7       Male  ...          Excellent                Very_High   \n...      ...  ...                ...                      ...   \n1457    Male  ...          Excellent                Very_High   \n1458  Female  ...          Excellent                Very_High   \n1459    Male  ...          Excellent                      Low   \n1467    Male  ...          Excellent                     High   \n1469    Male  ...          Excellent                Very_High   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n1                    0                 0                     2   \n2                    0                 0                     2   \n3                    0                 0                     2   \n4                    0                 0                     3   \n7                    0                 0                     0   \n...                ...               ...                   ...   \n1457                 0                36                     3   \n1458                 3                36                     0   \n1459                 0                36                     2   \n1467                 0                38                     1   \n1469                 0                40                     2   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n1               Good               0                   0   \n2             Better               0                   0   \n3             Better               0                   0   \n4             Better               0                   0   \n7             Better               0                   0   \n...              ...             ...                 ...   \n1457          Better              24                  15   \n1458            Good               7                   7   \n1459          Better              10                   9   \n1467            Good              37                  10   \n1469          Better              31                  15   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n1                           0                    0  \n2                           0                    0  \n3                           0                    0  \n4                           0                    0  \n7                           0                    0  \n...                       ...                  ...  \n1457                        2                   15  \n1458                        7                    7  \n1459                        0                    9  \n1467                        1                    8  \n1469                       13                    8  \n\n[748 rows x 31 columns]\n\n\n\n\nCode\n# Remove categories with no rows\nattrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n\n# Randomly sample 10 employees from each sampled job role\nattrition_clust = attrition_filtered.groupby('JobRole').sample(n=10,random_state=2022)\n\n# Print the sample\nprint(attrition_clust)\n\nprint(\"\\n The two-stage sampling technique gives you control over sampling both between subgroups and within subgroups.\")\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1124   36        0.0      Travel_Rarely       1396  Research_Development   \n576    45        0.0      Travel_Rarely        974  Research_Development   \n995    42        0.0  Travel_Frequently        748  Research_Development   \n1243   50        0.0      Travel_Rarely       1207  Research_Development   \n869    45        0.0      Travel_Rarely       1015  Research_Development   \n599    33        0.0      Travel_Rarely       1099  Research_Development   \n117    24        0.0      Travel_Rarely        350  Research_Development   \n472    30        0.0      Travel_Rarely        921  Research_Development   \n149    27        0.0         Non-Travel       1277  Research_Development   \n49     20        1.0      Travel_Rarely        129  Research_Development   \n1302   40        0.0      Travel_Rarely       1416  Research_Development   \n1126   42        0.0      Travel_Rarely        810  Research_Development   \n1216   38        0.0      Travel_Rarely        849  Research_Development   \n1362   43        0.0      Travel_Rarely        982  Research_Development   \n1327   46        0.0      Travel_Rarely        430  Research_Development   \n664    27        0.0      Travel_Rarely        269  Research_Development   \n1284   40        0.0      Travel_Rarely       1308  Research_Development   \n1440   50        0.0  Travel_Frequently        333  Research_Development   \n790    36        0.0         Non-Travel        427  Research_Development   \n1432   55        0.0      Travel_Rarely       1136  Research_Development   \n941    36        0.0      Travel_Rarely        329                 Sales   \n454    27        0.0  Travel_Frequently       1242                 Sales   \n460    37        0.0      Travel_Rarely        228                 Sales   \n636    45        0.0      Travel_Rarely       1268                 Sales   \n293    33        0.0  Travel_Frequently        430                 Sales   \n976    39        1.0      Travel_Rarely       1162                 Sales   \n813    30        0.0      Travel_Rarely        231                 Sales   \n288    35        1.0  Travel_Frequently        662                 Sales   \n1111   53        1.0      Travel_Rarely       1168                 Sales   \n1075   40        0.0      Travel_Rarely        630                 Sales   \n133    34        1.0  Travel_Frequently        296                 Sales   \n725    36        1.0      Travel_Rarely       1218                 Sales   \n4      18        1.0  Travel_Frequently       1306                 Sales   \n169    41        1.0      Travel_Rarely       1356                 Sales   \n1      19        1.0      Travel_Rarely        528                 Sales   \n48     19        1.0      Travel_Rarely        419                 Sales   \n150    31        0.0  Travel_Frequently        793                 Sales   \n130    21        0.0         Non-Travel        895                 Sales   \n3      18        0.0      Travel_Rarely        812                 Sales   \n99     31        0.0  Travel_Frequently        444                 Sales   \n\n      DistanceFromHome      Education    EducationField  \\\n1124                 5        College     Life_Sciences   \n576                  1         Master           Medical   \n995                  9        College           Medical   \n1243                28  Below_College           Medical   \n869                  5         Doctor           Medical   \n599                  4         Master           Medical   \n117                 21        College  Technical_Degree   \n472                  1       Bachelor     Life_Sciences   \n149                  8         Doctor     Life_Sciences   \n49                   4       Bachelor  Technical_Degree   \n1302                 2        College           Medical   \n1126                23         Doctor     Life_Sciences   \n1216                25        College     Life_Sciences   \n1362                12       Bachelor     Life_Sciences   \n1327                 1         Master           Medical   \n664                  5  Below_College  Technical_Degree   \n1284                14       Bachelor           Medical   \n1440                22         Doctor           Medical   \n790                  8       Bachelor     Life_Sciences   \n1432                 1         Master           Medical   \n941                 16         Master         Marketing   \n454                 20       Bachelor     Life_Sciences   \n460                  6         Master           Medical   \n636                  4        College     Life_Sciences   \n293                  7       Bachelor           Medical   \n976                  3        College           Medical   \n813                  8        College             Other   \n288                 18         Master         Marketing   \n1111                24         Master     Life_Sciences   \n1075                 4         Master         Marketing   \n133                  6        College         Marketing   \n725                  9         Master     Life_Sciences   \n4                    5       Bachelor         Marketing   \n169                 20        College         Marketing   \n1                   22  Below_College         Marketing   \n48                  21       Bachelor             Other   \n150                 20       Bachelor     Life_Sciences   \n130                  9        College           Medical   \n3                   10       Bachelor           Medical   \n99                   5       Bachelor         Marketing   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1124               Very_High    Male  ...          Excellent   \n576                Very_High  Female  ...          Excellent   \n995                      Low  Female  ...          Excellent   \n1243               Very_High    Male  ...          Excellent   \n869                     High  Female  ...          Excellent   \n599                      Low  Female  ...          Excellent   \n117                     High    Male  ...          Excellent   \n472                Very_High    Male  ...        Outstanding   \n149                      Low    Male  ...          Excellent   \n49                       Low    Male  ...          Excellent   \n1302                     Low    Male  ...          Excellent   \n1126                     Low  Female  ...          Excellent   \n1216                     Low  Female  ...          Excellent   \n1362                     Low    Male  ...          Excellent   \n1327               Very_High    Male  ...          Excellent   \n664                     High    Male  ...          Excellent   \n1284                    High    Male  ...          Excellent   \n1440                    High    Male  ...          Excellent   \n790                      Low  Female  ...          Excellent   \n1432                  Medium    Male  ...          Excellent   \n941                     High  Female  ...          Excellent   \n454                Very_High  Female  ...          Excellent   \n460                     High    Male  ...          Excellent   \n636                     High  Female  ...          Excellent   \n293                Very_High    Male  ...          Excellent   \n976                Very_High  Female  ...          Excellent   \n813                     High    Male  ...          Excellent   \n288                Very_High  Female  ...          Excellent   \n1111                     Low    Male  ...          Excellent   \n1075                    High    Male  ...          Excellent   \n133                Very_High  Female  ...          Excellent   \n725                     High    Male  ...        Outstanding   \n4                     Medium    Male  ...          Excellent   \n169                   Medium  Female  ...        Outstanding   \n1                  Very_High    Male  ...          Excellent   \n48                 Very_High    Male  ...          Excellent   \n150                     High    Male  ...          Excellent   \n130                      Low    Male  ...        Outstanding   \n3                  Very_High  Female  ...          Excellent   \n99                 Very_High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1124                Very_High                 0                16   \n576                 Very_High                 2                 8   \n995                      High                 0                12   \n1243                     High                 3                20   \n869                       Low                 0                10   \n599                 Very_High                 0                 8   \n117                    Medium                 3                 2   \n472                      High                 2                 7   \n149                 Very_High                 3                 3   \n49                     Medium                 0                 1   \n1302                Very_High                 1                22   \n1126                   Medium                 0                16   \n1216                     High                 1                19   \n1362                     High                 1                25   \n1327                Very_High                 2                23   \n664                    Medium                 1                 9   \n1284                      Low                 0                21   \n1440                Very_High                 0                32   \n790                       Low                 1                10   \n1432                Very_High                 2                31   \n941                       Low                 2                11   \n454                 Very_High                 0                 7   \n460                    Medium                 1                 7   \n636                       Low                 1                 9   \n293                       Low                 2                 5   \n976                       Low                 0                12   \n813                       Low                 1                10   \n288                      High                 1                 5   \n1111                   Medium                 0                15   \n1075                      Low                 1                15   \n133                 Very_High                 1                 3   \n725                    Medium                 0                10   \n4                   Very_High                 0                 0   \n169                 Very_High                 0                 4   \n1                   Very_High                 0                 0   \n48                     Medium                 0                 1   \n150                       Low                 1                 3   \n130                      High                 0                 3   \n3                         Low                 0                 0   \n99                       High                 1                 2   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1124                     3            Best              13   \n576                      2          Better               5   \n995                      3          Better              12   \n1243                     3          Better              20   \n869                      3          Better              10   \n599                      5          Better               5   \n117                      3          Better               1   \n472                      2          Better               2   \n149                      4          Better               3   \n49                       2          Better               1   \n1302                     5          Better              21   \n1126                     2          Better               1   \n1216                     2          Better              10   \n1362                     3          Better              25   \n1327                     0          Better               2   \n664                      3          Better               9   \n1284                     2            Best              20   \n1440                     2          Better              32   \n790                      2          Better               8   \n1432                     4            Best               7   \n941                      3            Good               3   \n454                      2          Better               7   \n460                      5            Best               5   \n636                      3            Best               5   \n293                      2          Better               4   \n976                      3            Good               1   \n813                      2            Best               8   \n288                      0            Good               4   \n1111                     2            Good               2   \n1075                     2            Good              12   \n133                      3            Good               2   \n725                      4          Better               5   \n4                        3          Better               0   \n169                      5            Good               4   \n1                        2            Good               0   \n48                       3            Best               1   \n150                      4          Better               2   \n130                      3            Good               3   \n3                        2          Better               0   \n99                       5            Good               2   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1124                  11                        3                    7  \n576                    3                        0                    2  \n995                    9                        5                    8  \n1243                   8                        3                    8  \n869                    7                        1                    4  \n599                    4                        0                    2  \n117                    1                        0                    0  \n472                    2                        0                    2  \n149                    2                        1                    2  \n49                     0                        0                    0  \n1302                   7                        3                    9  \n1126                   0                        0                    0  \n1216                   8                        0                    1  \n1362                  10                        3                    9  \n1327                   2                        2                    2  \n664                    8                        0                    8  \n1284                   7                        4                    9  \n1440                   6                       13                    9  \n790                    7                        0                    5  \n1432                   7                        0                    0  \n941                    2                        0                    2  \n454                    7                        0                    7  \n460                    4                        0                    1  \n636                    4                        0                    3  \n293                    3                        0                    3  \n976                    0                        0                    0  \n813                    4                        7                    7  \n288                    2                        3                    2  \n1111                   2                        2                    2  \n1075                  11                        2                   11  \n133                    2                        1                    0  \n725                    3                        0                    3  \n4                      0                        0                    0  \n169                    3                        0                    2  \n1                      0                        0                    0  \n48                     0                        0                    0  \n150                    2                        2                    2  \n130                    2                        2                    2  \n3                      0                        0                    0  \n99                     2                        2                    2  \n\n[40 rows x 31 columns]\n\n The two-stage sampling technique gives you control over sampling both between subgroups and within subgroups.\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_28748\\2564666783.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n\n\n\n\n\nYou’re going to compare the performance of point estimates using simple, stratified, and cluster sampling. Before doing that, you’ll have to set up the samples\n\n\nCode\n# Perform simple random sampling to get 0.25 of the population\nattrition_srs = attrition_pop.sample(frac=1/4, random_state=2022)\nattrition_srs.shape\n\n\n(368, 31)\n\n\n\n\nCode\n# Perform stratified sampling to get 0.25 of each relationship group\nattrition_strat = attrition_pop.groupby('RelationshipSatisfaction').sample(frac=1/4, random_state=2022)\nattrition_strat.shape\n\n\n(368, 31)\n\n\n\n\nCode\n# Create a list of unique RelationshipSatisfaction values\nsatisfaction_unique = list(attrition_pop['RelationshipSatisfaction'].unique())\n\n# Randomly sample 2 unique satisfaction values\nsatisfaction_samp = random.sample(satisfaction_unique, k=2)\n\n# Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction\nsatis_condition = attrition_pop['RelationshipSatisfaction'].isin(satisfaction_samp)\nattrition_clust_prep = attrition_pop[satis_condition]\nattrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n\n# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop\nattrition_clust = attrition_clust_prep.groupby(\"RelationshipSatisfaction\")\\\n    .sample(n=len(attrition_pop) // 4, random_state=2022)\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_28748\\1225069142.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  attrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n\n\nValueError: Cannot take a larger sample than population when 'replace=False'\n\n\n\n\n\n\n\nCode\n# Mean Attrition by RelationshipSatisfaction group\nmean_attrition_pop = attrition_pop.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_pop)\n\n\nRelationshipSatisfaction\nLow          0.206522\nMedium       0.148515\nHigh         0.154684\nVery_High    0.148148\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the simple random sample\nmean_attrition_srs = attrition_srs.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_srs)\n\n\nRelationshipSatisfaction\nLow          0.134328\nMedium       0.164179\nHigh         0.160000\nVery_High    0.155963\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the stratified sample\nmean_attrition_strat = attrition_strat.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_strat)\n\n\nRelationshipSatisfaction\nLow          0.144928\nMedium       0.078947\nHigh         0.165217\nVery_High    0.129630\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the cluster sample\nmean_attrition_clust = attrition_clust.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_clust)\n\n\nRelationshipSatisfaction\nLow          0.090909\nMedium       0.500000\nHigh         0.125000\nVery_High    0.307692\nName: Attrition, dtype: float64"
  },
  {
    "objectID": "posts/Sampling Distribution/Sampling Distributions.html",
    "href": "posts/Sampling Distribution/Sampling Distributions.html",
    "title": "Sampling Distribution",
    "section": "",
    "text": "We will discover how to quantify the accuracy of sample statistics using relative errors, and measure variation in your estimates by generating sampling distributions.\nThis Sampling Distribution is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nThe size of the sample has a significant impact on the accuracy of the point estimates.\nRelative error vs. sample size\n\nReally noise, particularly for small samples\nAmplitude is initially steep, then flattens\nRelative error decreases to zero (when the sample size = population)\n\n\n\nIt is important for a sample mean to be close to the population mean when it is calculated. There is, however, a possibility that this may not be the case if your sample size is too small.\nRelative error is the most common metric for assessing accuracy. This is the difference between the population parameter and the point estimate, divided by the population parameter. Sometimes it is expressed as a percentage.\n\n\nCode\nattrition_pop=pd.read_feather('dataset/attrition.feather')\nattrition_pop.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      Attrition\n      BusinessTravel\n      DailyRate\n      Department\n      DistanceFromHome\n      Education\n      EducationField\n      EnvironmentSatisfaction\n      Gender\n      ...\n      PerformanceRating\n      RelationshipSatisfaction\n      StockOptionLevel\n      TotalWorkingYears\n      TrainingTimesLastYear\n      WorkLifeBalance\n      YearsAtCompany\n      YearsInCurrentRole\n      YearsSinceLastPromotion\n      YearsWithCurrManager\n    \n  \n  \n    \n      0\n      21\n      0.0\n      Travel_Rarely\n      391\n      Research_Development\n      15\n      College\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      6\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      1\n      19\n      1.0\n      Travel_Rarely\n      528\n      Sales\n      22\n      Below_College\n      Marketing\n      Very_High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      2\n      Good\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18\n      1.0\n      Travel_Rarely\n      230\n      Research_Development\n      3\n      Bachelor\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      High\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18\n      0.0\n      Travel_Rarely\n      812\n      Sales\n      10\n      Bachelor\n      Medical\n      Very_High\n      Female\n      ...\n      Excellent\n      Low\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18\n      1.0\n      Travel_Frequently\n      1306\n      Sales\n      5\n      Bachelor\n      Marketing\n      Medium\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      3\n      Better\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 31 columns\n\n\n\n\n\nCode\n# Generate a simple random sample of 50 rows, with seed 2022\nattrition_srs50 = attrition_pop.sample(n=50, random_state=2022)\n\n# Calculate the mean employee attrition in the sample\nmean_attrition_srs50 = attrition_srs50['Attrition'].mean()\nmean_attrition_pop= attrition_pop['Attrition'].mean()\n\n# Calculate the relative error percentage\nrel_error_pct50 = 100 * abs(mean_attrition_pop - mean_attrition_srs50) / mean_attrition_pop\n\n# Print rel_error_pct50\nprint(rel_error_pct50)\n\n\n62.78481012658227\n\n\n\n\nCode\n# Generate a simple random sample of 100 rows, with seed 2022\nattrition_srs100 = attrition_pop.sample(n=100, random_state=2022)\n\n# Calculate the mean employee attrition in the sample\nmean_attrition_srs100 = attrition_srs100['Attrition'].mean()\n\n# Calculate the relative error percentage\nrel_error_pct100 = 100 * abs(mean_attrition_pop - mean_attrition_srs100) / mean_attrition_pop\n\n# Print rel_error_pct100\nprint(rel_error_pct100)\nprint(\"\\n As you increase the sample size, the sample mean generally gets closer to the population mean, and the relative error decreases\")\n\n\n6.962025316455695\n\n As you increase the sample size, the sample mean generally gets closer to the population mean, and the relative error decreases\n\n\n\n\n\nReplicating samples\nWhenever you calculate a point estimate, such as a sample mean, the value you calculate depends on the rows included in the sample. As a result, there is some randomness in the answer. A sample mean (or another statistic) can be calculated for each sample in order to quantify the variation caused by this randomness.\n\n\nCode\n# Create an empty list\nmean_attritions=[]\n# Loop 500 times to create 500 sample means\nfor i in range(500):\n    mean_attritions.append(\n        attrition_pop.sample(n=60)['Attrition'].mean()\n    )\n\n# Print out the first few entries of the list\nprint(mean_attritions[0:5])\n\n\n[0.13333333333333333, 0.11666666666666667, 0.13333333333333333, 0.18333333333333332, 0.2]\n\n\n\n\nCode\n# Create a histogram of the 500 sample means\nplt.hist(mean_attritions,bins=16)\nplt.show()\n\n\n\n\n\n\n\n\nEarlier we saw that while increasing the number of replicates didn’t affect the relative error of the sample means; it did result in a more consistent shape to the distribution.\n\n\nCode\nimport itertools\ndef expand_grid(data_dict):\n    rows = itertools.product(*data_dict.values())\n    return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n\n\n\n\nCode\n# Expand a grid representing 5 8-sided dice\ndice = expand_grid(\n  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n  })\n\n# Add a column of mean rolls and convert to a categorical\ndice['mean_roll'] = (dice['die1'] + dice['die2'] +\n                    dice['die3'] + dice['die4'] +\n                    dice['die5'] ) / 5\n\n\ndice['mean_roll'] = dice['mean_roll'].astype('category')\n\n# Print result\nprint(dice)\n\n\n       die1  die2  die3  die4  die5 mean_roll\n0         1     1     1     1     1       1.0\n1         1     1     1     1     2       1.2\n2         1     1     1     1     3       1.4\n3         1     1     1     1     4       1.6\n4         1     1     1     1     5       1.8\n...     ...   ...   ...   ...   ...       ...\n32763     8     8     8     8     4       7.2\n32764     8     8     8     8     5       7.4\n32765     8     8     8     8     6       7.6\n32766     8     8     8     8     7       7.8\n32767     8     8     8     8     8       8.0\n\n[32768 rows x 6 columns]\n\n\n\n\nCode\n# Draw a bar plot of mean_roll\ndice['mean_roll'].value_counts(sort=False).plot(kind='bar')\nplt.show()\n\n\n\n\n\n\n\n\nIt is only possible to calculate the exact sampling distribution in very simple situations. If just five eight-sided dice are used, the number of possible rolls is 8**5, which is over thirty thousand. The number of possible outcomes becomes too difficult to calculate when the dataset is more complicated, such as when a variable has hundreds or thousands of categories.\nYou can calculate an approximate sampling distribution by simulating the exact sampling distribution. You can repeat a procedure repeatedly to simulate both the sampling process and the sample statistic calculation process.\n\n\nCode\n# Sample one to eight, five times, with replacement\nfive_rolls = np.random.choice(list(range(1,9)), size=5, replace=True)\n\n# Print the mean of five_rolls\nprint(five_rolls.mean())\n\n\n6.2\n\n\n\n\nCode\n# Replicate the sampling code 1000 times\nsample_means_1000 = []\nfor i in range(1000):\n    sample_means_1000.append(\n        np.random.choice(list(range(1, 9)), size=5, replace=True).mean())\n\n\n# Print the first 10 entries of the result\nprint(sample_means_1000[0:10])\n\n\n[4.0, 4.8, 5.4, 2.8, 3.4, 3.4, 4.0, 4.8, 5.0, 5.4]\n\n\n\n\nCode\n# Draw a histogram of sample_means_1000 with 20 bins\nplt.hist(sample_means_1000, bins=20)\nplt.show()\n\n\n\n\n\n\n\n\nIn statistics, the Gaussian distribution (also known as the normal distribution) is very important. It has a distinctive bell-shaped curve\nthe central limit theorem: Averages of independent samples have approximately normal distributions As the sample size increases, * The distribution of the averages gets closer to being normally distributed * The width of the sampling distribution gets narrower\nStandard error: * Standard deviation of the sampling distribution * Important tool in understanding sampling variability"
  },
  {
    "objectID": "posts/Introduction to hypothesis testing/Introduction to Hypothesis Testing.html",
    "href": "posts/Introduction to hypothesis testing/Introduction to Hypothesis Testing.html",
    "title": "Introduction to Hypothesis Testing",
    "section": "",
    "text": "We will walk you through the steps of creating a one sample proportional test so that you will be able to better understand how hypothesis tests work and what problems they can solve. In doing so, we will also introduce important concepts such as z-scores, parabolae, and false negative and false positive errors.\nThis Introduction to Hypothesis Testing is part of Datacamp course: Hypothesis Testing in Python\nThis is my learning experience of data science through DataCamp\n\n\n\n\nA/B testing: also known as split testing, refers to random experiment to test variable / outcome on treatment & control group Hypothesis: a theory or assumption yet to be proved Point estimation: sample statistics or sample mean of population mean_samp = population[‘column’].mean() Standard error: standard deviation of sample statistics or sample mean in bootstrap distribution estimates standard error std_error = np.std(so_boot_distn, ddof=1)\n\n\n\nSince variables have different units & ranges, we need to standardize their value before testing our hypothesis standardized value = (value - mean) / standard deviation\nz = (sample statistics - hypothesis param value) / standard error\nStandard normal distribution: normal distribution with mean = 0 + standard deviation =1\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\n\n\n\n\nCode\nlate_shipments= pd.read_feather('dataset/late_shipments.feather')\nlate_shipments.head()\n\n\n\n\n\n\n  \n    \n      \n      id\n      country\n      managed_by\n      fulfill_via\n      vendor_inco_term\n      shipment_mode\n      late_delivery\n      late\n      product_group\n      sub_classification\n      ...\n      line_item_quantity\n      line_item_value\n      pack_price\n      unit_price\n      manufacturing_site\n      first_line_designation\n      weight_kilograms\n      freight_cost_usd\n      freight_cost_groups\n      line_item_insurance_usd\n    \n  \n  \n    \n      0\n      36203.0\n      Nigeria\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      1.0\n      Yes\n      HRDT\n      HIV test\n      ...\n      2996.0\n      266644.00\n      89.00\n      0.89\n      Alere Medical Co., Ltd.\n      Yes\n      1426.0\n      33279.83\n      expensive\n      373.83\n    \n    \n      1\n      30998.0\n      Botswana\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      HRDT\n      HIV test\n      ...\n      25.0\n      800.00\n      32.00\n      1.60\n      Trinity Biotech, Plc\n      Yes\n      10.0\n      559.89\n      reasonable\n      1.72\n    \n    \n      2\n      69871.0\n      Vietnam\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      ARV\n      Adult\n      ...\n      22925.0\n      110040.00\n      4.80\n      0.08\n      Hetero Unit III Hyderabad IN\n      Yes\n      3723.0\n      19056.13\n      expensive\n      181.57\n    \n    \n      3\n      17648.0\n      South Africa\n      PMO - US\n      Direct Drop\n      DDP\n      Ocean\n      0.0\n      No\n      ARV\n      Adult\n      ...\n      152535.0\n      361507.95\n      2.37\n      0.04\n      Aurobindo Unit III, India\n      Yes\n      7698.0\n      11372.23\n      expensive\n      779.41\n    \n    \n      4\n      5647.0\n      Uganda\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      HRDT\n      HIV test - Ancillary\n      ...\n      850.0\n      8.50\n      0.01\n      0.00\n      Inverness Japan\n      Yes\n      56.0\n      360.00\n      reasonable\n      0.01\n    \n  \n\n5 rows × 27 columns\n\n\n\n\n\n\nWe’ll begin our analysis by calculating a point estimate (or sample statistic), namely the proportion of late shipments.\n\n\nCode\n# Print the late_shipments dataset\nprint(late_shipments)\n\n# Calculate the proportion of late shipments\nlate_prop_samp = (late_shipments['late']=='Yes').mean()\n\n# Print the results\nprint(late_prop_samp)\nprint(\"\\nThe proportion of late shipments in the sample is 0.061, or 6.1%\")\n\n\n          id       country managed_by  fulfill_via vendor_inco_term  \\\n0    36203.0       Nigeria   PMO - US  Direct Drop              EXW   \n1    30998.0      Botswana   PMO - US  Direct Drop              EXW   \n2    69871.0       Vietnam   PMO - US  Direct Drop              EXW   \n3    17648.0  South Africa   PMO - US  Direct Drop              DDP   \n4     5647.0        Uganda   PMO - US  Direct Drop              EXW   \n..       ...           ...        ...          ...              ...   \n995  13608.0        Uganda   PMO - US  Direct Drop              DDP   \n996  80394.0    Congo, DRC   PMO - US  Direct Drop              EXW   \n997  61675.0        Zambia   PMO - US  Direct Drop              EXW   \n998  39182.0  South Africa   PMO - US  Direct Drop              DDP   \n999   5645.0      Botswana   PMO - US  Direct Drop              EXW   \n\n    shipment_mode  late_delivery late product_group    sub_classification  \\\n0             Air            1.0  Yes          HRDT              HIV test   \n1             Air            0.0   No          HRDT              HIV test   \n2             Air            0.0   No           ARV                 Adult   \n3           Ocean            0.0   No           ARV                 Adult   \n4             Air            0.0   No          HRDT  HIV test - Ancillary   \n..            ...            ...  ...           ...                   ...   \n995           Air            0.0   No           ARV                 Adult   \n996           Air            0.0   No          HRDT              HIV test   \n997           Air            1.0  Yes          HRDT              HIV test   \n998         Ocean            0.0   No           ARV                 Adult   \n999           Air            0.0   No          HRDT              HIV test   \n\n     ... line_item_quantity line_item_value pack_price unit_price  \\\n0    ...             2996.0       266644.00      89.00       0.89   \n1    ...               25.0          800.00      32.00       1.60   \n2    ...            22925.0       110040.00       4.80       0.08   \n3    ...           152535.0       361507.95       2.37       0.04   \n4    ...              850.0            8.50       0.01       0.00   \n..   ...                ...             ...        ...        ...   \n995  ...              121.0         9075.00      75.00       0.62   \n996  ...              292.0         9344.00      32.00       1.60   \n997  ...             2127.0       170160.00      80.00       0.80   \n998  ...           191011.0       861459.61       4.51       0.15   \n999  ...              200.0        14398.00      71.99       0.72   \n\n               manufacturing_site first_line_designation  weight_kilograms  \\\n0         Alere Medical Co., Ltd.                    Yes            1426.0   \n1            Trinity Biotech, Plc                    Yes              10.0   \n2    Hetero Unit III Hyderabad IN                    Yes            3723.0   \n3       Aurobindo Unit III, India                    Yes            7698.0   \n4                 Inverness Japan                    Yes              56.0   \n..                            ...                    ...               ...   \n995     Janssen-Cilag, Latina, IT                    Yes              43.0   \n996          Trinity Biotech, Plc                    Yes              99.0   \n997       Alere Medical Co., Ltd.                    Yes             881.0   \n998     Aurobindo Unit III, India                    Yes           16234.0   \n999               Inverness Japan                    Yes              46.0   \n\n     freight_cost_usd  freight_cost_groups  line_item_insurance_usd  \n0            33279.83            expensive                   373.83  \n1              559.89           reasonable                     1.72  \n2            19056.13            expensive                   181.57  \n3            11372.23            expensive                   779.41  \n4              360.00           reasonable                     0.01  \n..                ...                  ...                      ...  \n995            199.00           reasonable                    12.72  \n996           2162.55           reasonable                    13.10  \n997          14019.38            expensive                   210.49  \n998          14439.17            expensive                  1421.41  \n999           1028.18           reasonable                    23.04  \n\n[1000 rows x 27 columns]\n0.061\n\nThe proportion of late shipments in the sample is 0.061, or 6.1%\n\n\n\n\nCode\nlate_prop_Yes=late_shipments[late_shipments['late']=='Yes']\nlate_prop_Yes.head(10)\nlate_prop_Yes.shape\n\n\n(61, 27)\n\n\n\n\n\n\n\nCode\nlate_shipments_boot_distn=[0.064,\n 0.049,\n 0.06,\n 0.066,\n 0.052,\n 0.066,\n 0.071,\n 0.061,\n 0.051,\n 0.06,\n 0.053,\n 0.066,\n 0.069,\n 0.068,\n 0.063,\n 0.061,\n 0.052,\n 0.045,\n 0.054,\n 0.054,\n 0.064,\n 0.064,\n 0.058,\n 0.062,\n 0.05,\n 0.053,\n 0.064,\n 0.058,\n 0.071,\n 0.064,\n 0.052,\n 0.063,\n 0.056,\n 0.05,\n 0.058,\n 0.06,\n 0.068,\n 0.065,\n 0.056,\n 0.052,\n 0.061,\n 0.059,\n 0.054,\n 0.071,\n 0.067,\n 0.079,\n 0.069,\n 0.069,\n 0.05,\n 0.059,\n 0.062,\n 0.046,\n 0.068,\n 0.057,\n 0.067,\n 0.042,\n 0.074,\n 0.063,\n 0.056,\n 0.063,\n 0.068,\n 0.06,\n 0.068,\n 0.064,\n 0.052,\n 0.045,\n 0.058,\n 0.072,\n 0.078,\n 0.055,\n 0.069,\n 0.048,\n 0.047,\n 0.061,\n 0.066,\n 0.062,\n 0.059,\n 0.062,\n 0.054,\n 0.063,\n 0.061,\n 0.059,\n 0.057,\n 0.059,\n 0.058,\n 0.068,\n 0.067,\n 0.059,\n 0.054,\n 0.064,\n 0.047,\n 0.054,\n 0.065,\n 0.063,\n 0.057,\n 0.062,\n 0.058,\n 0.046,\n 0.052,\n 0.065,\n 0.053,\n 0.069,\n 0.068,\n 0.065,\n 0.052,\n 0.061,\n 0.058,\n 0.042,\n 0.064,\n 0.063,\n 0.068,\n 0.067,\n 0.061,\n 0.056,\n 0.061,\n 0.044,\n 0.058,\n 0.051,\n 0.075,\n 0.064,\n 0.073,\n 0.058,\n 0.056,\n 0.055,\n 0.063,\n 0.056,\n 0.067,\n 0.075,\n 0.061,\n 0.063,\n 0.051,\n 0.065,\n 0.069,\n 0.066,\n 0.05,\n 0.066,\n 0.057,\n 0.064,\n 0.065,\n 0.062,\n 0.071,\n 0.062,\n 0.065,\n 0.062,\n 0.066,\n 0.071,\n 0.058,\n 0.053,\n 0.062,\n 0.051,\n 0.056,\n 0.061,\n 0.074,\n 0.054,\n 0.059,\n 0.069,\n 0.073,\n 0.066,\n 0.052,\n 0.065,\n 0.072,\n 0.071,\n 0.059,\n 0.065,\n 0.06,\n 0.055,\n 0.053,\n 0.059,\n 0.066,\n 0.061,\n 0.053,\n 0.053,\n 0.06,\n 0.058,\n 0.074,\n 0.05,\n 0.059,\n 0.067,\n 0.06,\n 0.064,\n 0.061,\n 0.072,\n 0.06,\n 0.048,\n 0.066,\n 0.059,\n 0.08,\n 0.062,\n 0.066,\n 0.065,\n 0.06,\n 0.048,\n 0.064,\n 0.07,\n 0.053,\n 0.035,\n 0.071,\n 0.061,\n 0.051,\n 0.052,\n 0.051,\n 0.069,\n 0.052,\n 0.052,\n 0.065,\n 0.053,\n 0.055,\n 0.063,\n 0.066,\n 0.062,\n 0.067,\n 0.079,\n 0.062,\n 0.056,\n 0.058,\n 0.068,\n 0.062,\n 0.045,\n 0.063,\n 0.069,\n 0.054,\n 0.065,\n 0.061,\n 0.057,\n 0.05,\n 0.048,\n 0.069,\n 0.058,\n 0.052,\n 0.056,\n 0.057,\n 0.071,\n 0.059,\n 0.062,\n 0.064,\n 0.053,\n 0.065,\n 0.056,\n 0.06,\n 0.062,\n 0.042,\n 0.054,\n 0.051,\n 0.061,\n 0.049,\n 0.071,\n 0.072,\n 0.059,\n 0.063,\n 0.049,\n 0.074,\n 0.063,\n 0.052,\n 0.055,\n 0.072,\n 0.054,\n 0.067,\n 0.067,\n 0.067,\n 0.055,\n 0.073,\n 0.064,\n 0.069,\n 0.06,\n 0.053,\n 0.057,\n 0.056,\n 0.058,\n 0.067,\n 0.065,\n 0.064,\n 0.053,\n 0.055,\n 0.069,\n 0.058,\n 0.07,\n 0.068,\n 0.062,\n 0.062,\n 0.05,\n 0.069,\n 0.061,\n 0.057,\n 0.066,\n 0.056,\n 0.053,\n 0.055,\n 0.062,\n 0.064,\n 0.055,\n 0.056,\n 0.061,\n 0.058,\n 0.068,\n 0.079,\n 0.057,\n 0.049,\n 0.052,\n 0.063,\n 0.064,\n 0.059,\n 0.071,\n 0.064,\n 0.052,\n 0.066,\n 0.063,\n 0.069,\n 0.056,\n 0.057,\n 0.062,\n 0.057,\n 0.055,\n 0.062,\n 0.06,\n 0.064,\n 0.057,\n 0.062,\n 0.069,\n 0.067,\n 0.052,\n 0.061,\n 0.056,\n 0.055,\n 0.056,\n 0.055,\n 0.064,\n 0.068,\n 0.051,\n 0.054,\n 0.057,\n 0.054,\n 0.07,\n 0.049,\n 0.058,\n 0.063,\n 0.07,\n 0.046,\n 0.059,\n 0.064,\n 0.059,\n 0.061,\n 0.066,\n 0.06,\n 0.073,\n 0.08,\n 0.069,\n 0.061,\n 0.071,\n 0.068,\n 0.065,\n 0.063,\n 0.054,\n 0.07,\n 0.061,\n 0.053,\n 0.059,\n 0.047,\n 0.064,\n 0.071,\n 0.068,\n 0.049,\n 0.063,\n 0.057,\n 0.057,\n 0.059,\n 0.061,\n 0.048,\n 0.084,\n 0.07,\n 0.077,\n 0.043,\n 0.065,\n 0.057,\n 0.057,\n 0.054,\n 0.064,\n 0.062,\n 0.067,\n 0.068,\n 0.06,\n 0.054,\n 0.066,\n 0.048,\n 0.048,\n 0.06,\n 0.054,\n 0.067,\n 0.064,\n 0.064,\n 0.067,\n 0.058,\n 0.066,\n 0.06,\n 0.048,\n 0.058,\n 0.054,\n 0.056,\n 0.055,\n 0.068,\n 0.077,\n 0.06,\n 0.061,\n 0.055,\n 0.065,\n 0.064,\n 0.058,\n 0.058,\n 0.058,\n 0.055,\n 0.067,\n 0.061,\n 0.063,\n 0.065,\n 0.071,\n 0.051,\n 0.066,\n 0.066,\n 0.066,\n 0.07,\n 0.068,\n 0.061,\n 0.062,\n 0.054,\n 0.058,\n 0.066,\n 0.059,\n 0.061,\n 0.058,\n 0.057,\n 0.065,\n 0.053,\n 0.053,\n 0.06,\n 0.068,\n 0.067,\n 0.068,\n 0.061,\n 0.067,\n 0.059,\n 0.057,\n 0.055,\n 0.067,\n 0.058,\n 0.055,\n 0.055,\n 0.054,\n 0.061,\n 0.074,\n 0.071,\n 0.057,\n 0.056,\n 0.047,\n 0.07,\n 0.054,\n 0.052,\n 0.072,\n 0.054,\n 0.064,\n 0.063,\n 0.075,\n 0.064,\n 0.051,\n 0.061,\n 0.064,\n 0.047,\n 0.067,\n 0.061,\n 0.06,\n 0.057,\n 0.059,\n 0.058,\n 0.07,\n 0.06,\n 0.056,\n 0.064,\n 0.056,\n 0.066,\n 0.051,\n 0.064,\n 0.054,\n 0.058,\n 0.064,\n 0.041,\n 0.057,\n 0.055,\n 0.06,\n 0.06,\n 0.051,\n 0.054,\n 0.07,\n 0.053,\n 0.063,\n 0.058,\n 0.066,\n 0.059,\n 0.051,\n 0.067,\n 0.078,\n 0.056,\n 0.068,\n 0.057,\n 0.059,\n 0.062,\n 0.053,\n 0.064,\n 0.067,\n 0.068,\n 0.071,\n 0.066,\n 0.057,\n 0.063,\n 0.067,\n 0.059,\n 0.057,\n 0.064,\n 0.049,\n 0.066,\n 0.055,\n 0.071,\n 0.061,\n 0.078,\n 0.062,\n 0.052,\n 0.058,\n 0.066,\n 0.06,\n 0.054,\n 0.058,\n 0.054,\n 0.062,\n 0.072,\n 0.068,\n 0.057,\n 0.059,\n 0.066,\n 0.066,\n 0.065,\n 0.067,\n 0.071,\n 0.064,\n 0.072,\n 0.067,\n 0.064,\n 0.064,\n 0.051,\n 0.061,\n 0.047,\n 0.07,\n 0.073,\n 0.06,\n 0.066,\n 0.058,\n 0.056,\n 0.064,\n 0.059,\n 0.062,\n 0.046,\n 0.07,\n 0.07,\n 0.071,\n 0.056,\n 0.061,\n 0.066,\n 0.058,\n 0.055,\n 0.073,\n 0.068,\n 0.073,\n 0.055,\n 0.074,\n 0.063,\n 0.049,\n 0.063,\n 0.063,\n 0.056,\n 0.061,\n 0.065,\n 0.066,\n 0.06,\n 0.057,\n 0.07,\n 0.06,\n 0.053,\n 0.055,\n 0.066,\n 0.07,\n 0.069,\n 0.051,\n 0.067,\n 0.055,\n 0.06,\n 0.074,\n 0.06,\n 0.057,\n 0.06,\n 0.054,\n 0.054,\n 0.058,\n 0.06,\n 0.057,\n 0.059,\n 0.065,\n 0.061,\n 0.073,\n 0.067,\n 0.063,\n 0.079,\n 0.063,\n 0.063,\n 0.051,\n 0.074,\n 0.06,\n 0.07,\n 0.063,\n 0.072,\n 0.066,\n 0.058,\n 0.046,\n 0.059,\n 0.064,\n 0.058,\n 0.071,\n 0.055,\n 0.062,\n 0.05,\n 0.055,\n 0.061,\n 0.052,\n 0.059,\n 0.063,\n 0.058,\n 0.044,\n 0.052,\n 0.069,\n 0.056,\n 0.057,\n 0.064,\n 0.067,\n 0.058,\n 0.07,\n 0.065,\n 0.068,\n 0.061,\n 0.055,\n 0.06,\n 0.053,\n 0.066,\n 0.052,\n 0.064,\n 0.051,\n 0.076,\n 0.069,\n 0.056,\n 0.057,\n 0.068,\n 0.07,\n 0.065,\n 0.062,\n 0.066,\n 0.063,\n 0.066,\n 0.054,\n 0.061,\n 0.061,\n 0.055,\n 0.053,\n 0.054,\n 0.065,\n 0.073,\n 0.064,\n 0.054,\n 0.065,\n 0.06,\n 0.059,\n 0.056,\n 0.064,\n 0.057,\n 0.06,\n 0.07,\n 0.063,\n 0.064,\n 0.067,\n 0.061,\n 0.053,\n 0.06,\n 0.064,\n 0.064,\n 0.057,\n 0.046,\n 0.057,\n 0.065,\n 0.074,\n 0.062,\n 0.063,\n 0.054,\n 0.074,\n 0.064,\n 0.077,\n 0.068,\n 0.06,\n 0.063,\n 0.059,\n 0.06,\n 0.068,\n 0.052,\n 0.064,\n 0.057,\n 0.059,\n 0.069,\n 0.061,\n 0.064,\n 0.047,\n 0.062,\n 0.069,\n 0.054,\n 0.069,\n 0.063,\n 0.077,\n 0.06,\n 0.061,\n 0.055,\n 0.069,\n 0.061,\n 0.06,\n 0.061,\n 0.067,\n 0.05,\n 0.061,\n 0.062,\n 0.081,\n 0.071,\n 0.057,\n 0.055,\n 0.054,\n 0.07,\n 0.068,\n 0.063,\n 0.056,\n 0.081,\n 0.049,\n 0.07,\n 0.048,\n 0.046,\n 0.069,\n 0.056,\n 0.066,\n 0.058,\n 0.058,\n 0.062,\n 0.052,\n 0.065,\n 0.043,\n 0.062,\n 0.063,\n 0.053,\n 0.073,\n 0.058,\n 0.064,\n 0.071,\n 0.073,\n 0.059,\n 0.08,\n 0.052,\n 0.053,\n 0.053,\n 0.053,\n 0.057,\n 0.061,\n 0.069,\n 0.046,\n 0.063,\n 0.078,\n 0.06,\n 0.06,\n 0.064,\n 0.063,\n 0.065,\n 0.069,\n 0.059,\n 0.068,\n 0.061,\n 0.066,\n 0.064,\n 0.064,\n 0.058,\n 0.046,\n 0.073,\n 0.06,\n 0.056,\n 0.073,\n 0.07,\n 0.058,\n 0.056,\n 0.064,\n 0.069,\n 0.065,\n 0.063,\n 0.063,\n 0.054,\n 0.081,\n 0.044,\n 0.048,\n 0.059,\n 0.058,\n 0.046,\n 0.063,\n 0.072,\n 0.063,\n 0.059,\n 0.063,\n 0.047,\n 0.063,\n 0.065,\n 0.071,\n 0.061,\n 0.05,\n 0.063,\n 0.065,\n 0.054,\n 0.053,\n 0.061,\n 0.054,\n 0.063,\n 0.056,\n 0.071,\n 0.057,\n 0.058,\n 0.049,\n 0.074,\n 0.057,\n 0.058,\n 0.07,\n 0.063,\n 0.057,\n 0.052,\n 0.064,\n 0.074,\n 0.047,\n 0.071,\n 0.051,\n 0.059,\n 0.05,\n 0.059,\n 0.05,\n 0.05,\n 0.057,\n 0.075,\n 0.053,\n 0.07,\n 0.062,\n 0.062,\n 0.075,\n 0.058,\n 0.057,\n 0.05,\n 0.062,\n 0.061,\n 0.067,\n 0.062,\n 0.059,\n 0.059,\n 0.049,\n 0.052,\n 0.062,\n 0.069,\n 0.062,\n 0.054,\n 0.05,\n 0.063,\n 0.052,\n 0.063,\n 0.069,\n 0.057,\n 0.067,\n 0.064,\n 0.057,\n 0.057,\n 0.057,\n 0.05,\n 0.062,\n 0.069,\n 0.075,\n 0.075,\n 0.05,\n 0.06,\n 0.065,\n 0.051,\n 0.063,\n 0.075,\n 0.06,\n 0.058,\n 0.063,\n 0.069,\n 0.055,\n 0.062,\n 0.06,\n 0.057,\n 0.079,\n 0.046,\n 0.059,\n 0.07,\n 0.055,\n 0.08,\n 0.048,\n 0.061,\n 0.042,\n 0.068,\n 0.082,\n 0.044,\n 0.054,\n 0.063,\n 0.054,\n 0.071,\n 0.053,\n 0.061,\n 0.06,\n 0.065,\n 0.072,\n 0.063,\n 0.062,\n 0.053,\n 0.072,\n 0.067,\n 0.058,\n 0.075,\n 0.07,\n 0.052,\n 0.056,\n 0.056,\n 0.082,\n 0.055,\n 0.056,\n 0.057,\n 0.056,\n 0.054,\n 0.073,\n 0.081,\n 0.063,\n 0.063,\n 0.054,\n 0.058,\n 0.062,\n 0.065,\n 0.063,\n 0.062,\n 0.056,\n 0.063,\n 0.06,\n 0.061,\n 0.068,\n 0.067,\n 0.07,\n 0.059,\n 0.06,\n 0.063,\n 0.057,\n 0.052,\n 0.062,\n 0.064,\n 0.065,\n 0.07,\n 0.063,\n 0.062,\n 0.052,\n 0.055,\n 0.055,\n 0.053,\n 0.057,\n 0.058,\n 0.062,\n 0.06,\n 0.056,\n 0.064,\n 0.074,\n 0.071,\n 0.059,\n 0.056,\n 0.063,\n 0.059,\n 0.058,\n 0.054,\n 0.058,\n 0.069,\n 0.06,\n 0.063,\n 0.054,\n 0.047,\n 0.061,\n 0.057,\n 0.059,\n 0.057,\n 0.063,\n 0.06,\n 0.071,\n 0.062,\n 0.06,\n 0.071,\n 0.059,\n 0.049,\n 0.077]\n\n\n\n\nCode\nlate_shipments['late'].unique()\n\n\narray(['Yes', 'No'], dtype=object)\n\n\n\n\nCode\n# Hypothesize that the proportion is 6%\nlate_prop_hyp = 0.06\n\n#for i in range(5000):\n    #np.mean(late_shipments_boot_distn.append(late_shipments.sample(frac=1, replace=True)['late']))\n\n#print(late_shipments_boot_distn)\n# Calculate the standard error\nstd_error = np.std(late_shipments_boot_distn,ddof=1)\n\n# Find z-score of late_prop_samp\nz_score = (late_prop_samp - late_prop_hyp) / std_error\n\n# Print z_score\nprint(z_score)\nprint(\"\\nThe z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic\")\n\n\n0.13387997080083944\n\nThe z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic\n\n\n\n\n\nHypothesis tests check if the sample statistics lie in the tails of the null distribution\nalternative different from null : Two Tail test alternative greater than null : right tail test alternative lower than null: left tail test\n\n\n\np-values measure the strength of support for the null hypothesis, or in other words, they measure the probability of obtaining a result, assuming the null hypothesis is true. Large p-values mean our statistic is producing a result that is likely not in a tail of our null distribution, and chance could be a good explanation for the result. Small p-values mean our statistic is producing a result likely in the tail of our null distribution. Because p-values are probabilities, they are always between zero and one\nCalculating p-value Left tail test: norm.cdf() right tail test: 1-norm.cdf()\np-values quantify evidence for the null hypothesis large p-value => fail to reject null hypothesis small p-value => reject null hypothesis\n\n\nCode\n# Calculate the z-score of late_prop_samp\nz_score = (late_prop_samp - late_prop_hyp) / std_error\n\n# Calculate the p-value\np_value = 1-norm.cdf(z_score, loc=0, scale=1)\n\n# Print the p-value\nprint(p_value)\n\n\n0.44674874433656875\n\n\n\n\n\nType I and type II errors\nFor hypothesis tests and for criminal trials, there are two states of truth and two possible outcomes. Two combinations are correct test outcomes, and there are two ways it can go wrong.\nThe errors are known as false positives (or “type I errors”), and false negatives (or “type II errors”)."
  },
  {
    "objectID": "posts/Two Sample Anova test/Two-Sample and ANOVA Tests.html",
    "href": "posts/Two Sample Anova test/Two-Sample and ANOVA Tests.html",
    "title": "Two-Sample and ANOVA Tests",
    "section": "",
    "text": "Two-Sample and ANOVA Tests\nWe will learn how to test for differences in means between two groups using t-tests and extend this to more than two groups using ANOVA and pairwise t-tests\nThis Two-Sample and ANOVA Tests is part of Datacamp course: Hypothesis Testing in Python\nThis is my learning experience of data science through DataCamp\n\nHypotheses\nHo : The mean compensation (in USD) is the same for those that coded first as a child and those that coded first as an adult. Ho : μ (child) = μ (adult) H : μ(child) − μ(adult) = 0 H : The mean compensation (in USD) is greater for those that coded first as a child compared to those that coded first as an adult. Ha : μ(child) > μ(adult) Ha : μ(child) − μ(adult) > 0\n\n\nTest statistics\n\nSample mean estimates the population mean \n\n\n\n\n\n\nPaired t-tests\n\n\n\nCode\nimport pandas as pd\nimport pingouin as pingouin\nimport sns as sns\n\nsample_dem_data = pd.read_feather('dataset/dem_votes_potus_12_16.feather')\nsample_dem_data.head()\n\n\n\n\n\n\n  \n    \n      \n      state\n      county\n      dem_percent_12\n      dem_percent_16\n    \n  \n  \n    \n      0\n      Alabama\n      Bullock\n      76.305900\n      74.946921\n    \n    \n      1\n      Alabama\n      Chilton\n      19.453671\n      15.847352\n    \n    \n      2\n      Alabama\n      Clay\n      26.673672\n      18.674517\n    \n    \n      3\n      Alabama\n      Cullman\n      14.661752\n      10.028252\n    \n    \n      4\n      Alabama\n      Escambia\n      36.915731\n      31.020546\n    \n  \n\n\n\n\n\n\nCode\n# Calculate the differences from 2012 to 2016\nsample_dem_data['diff'] = sample_dem_data['dem_percent_12'] - sample_dem_data['dem_percent_16']\n\n# Print sample_dem_data\nprint(sample_dem_data)\n\n\n       state       county  dem_percent_12  dem_percent_16       diff\n0    Alabama      Bullock       76.305900       74.946921   1.358979\n1    Alabama      Chilton       19.453671       15.847352   3.606319\n2    Alabama         Clay       26.673672       18.674517   7.999155\n3    Alabama      Cullman       14.661752       10.028252   4.633500\n4    Alabama     Escambia       36.915731       31.020546   5.895185\n..       ...          ...             ...             ...        ...\n495  Wyoming        Uinta       19.065464       14.191263   4.874201\n496  Wyoming     Washakie       20.131846       13.948610   6.183235\n497   Alaska   District 3       33.514582       16.301064  17.213518\n498   Alaska  District 18       61.284271       52.810051   8.474220\n499   Alaska  District 24       42.913980       39.405286   3.508694\n\n[500 rows x 5 columns]\n\n\n\n\nCode\nfrom matplotlib import pyplot as plt\n\n# Calculate the differences from 2012 to 2016\nsample_dem_data['diff'] = sample_dem_data['dem_percent_12'] - sample_dem_data['dem_percent_16']\n\n# Find the mean of the diff column\nxbar_diff = sample_dem_data['diff'].mean()\n\n# Find the standard deviation of the diff column\ns_diff = sample_dem_data['diff'].std()\n\n# Plot a histogram of diff with 20 bins\nsample_dem_data['diff'].hist(bins=20)\nplt.show()\n\n\n\n\n\n\n\nUsing ttest\n\n\nCode\nimport pingouin\n# Conduct a t-test on diff\ntest_results = pingouin.ttest(x=sample_dem_data['diff'],y=0,alternative='two-sided')\n\n# Print the test results\nprint(test_results)\nprint(\"\\n Testing differences between two means using ttest()\")\n\n\n                T  dof alternative          p-val         CI95%   cohen-d  \\\nT-test  30.298384  499   two-sided  3.600634e-115  [6.39, 7.27]  1.354985   \n\n              BF10  power  \nT-test  2.246e+111    1.0  \n\n Testing differences between two means using ttest()\n\n\n\n\nCode\n# Conduct a paired t-test on dem_percent_12 and dem_percent_16\npaired_test_results = pingouin.ttest(x=sample_dem_data['dem_percent_12'],y=sample_dem_data['dem_percent_16'],paired=True,\nalternative=\"two-sided\")\n\n# Print the paired test results\nprint(paired_test_results)\n\n\n                T  dof alternative          p-val         CI95%   cohen-d  \\\nT-test  30.298384  499   two-sided  3.600634e-115  [6.39, 7.27]  0.454202   \n\n              BF10  power  \nT-test  2.246e+111    1.0  \n\n\n\n\nAnova (Analysis of Variance) test\nEDA of late shipment mode\n\n\nCode\nimport seaborn as sns\nlate_shipments=pd.read_feather('dataset/late_shipments.feather')\n# Calculate the mean pack_price for each shipment_mode\nxbar_pack_by_mode = late_shipments.groupby(\"shipment_mode\")['pack_price'].mean()\nprint(xbar_pack_by_mode)\n\n# Calculate the standard deviation of the pack_price for each shipment_mode\ns_pack_by_mode = late_shipments.groupby(\"shipment_mode\")['pack_price'].std()\nprint(s_pack_by_mode)\n\n# Boxplot of shipment_mode vs. pack_price\nsns.boxplot(x='pack_price',y='shipment_mode',data=late_shipments)\nplt.show()\n\n\nshipment_mode\nAir            39.712395\nAir Charter     4.226667\nOcean           6.432273\nName: pack_price, dtype: float64\nshipment_mode\nAir            48.932861\nAir Charter     0.992969\nOcean           5.303047\nName: pack_price, dtype: float64\n\n\n\n\n\n\n\nConducting an ANOVA test\nhe box plots made it look like the distribution of pack price was different for each of the three shipment modes. However, it didn’t tell us whether the mean pack price was different in each category. To determine that, we can use an ANOVA test. The null and alternative hypotheses can be written as follows.\nHo: Pack prices for every category of shipment mode are the same.\nHa: Pack prices for some categories of shipment mode are different. Use a significance level of 0.1.\n\n\nCode\n# Run an ANOVA for pack_price across shipment_mode\nanova_results = pingouin.anova(data=late_shipments,dv='pack_price', between='shipment_mode')\n# Print anova_results\nprint(anova_results)\nprint(\"\\n Since p-value is less or equal to significance level, so null hypothesis should be rejected. There is a significant difference in pack prices between the shipment modes. However, we don't know which shipment modes this applies to.\")\n\n\n          Source  ddof1  ddof2        F         p-unc       np2\n0  shipment_mode      2    997  21.8646  5.089479e-10  0.042018\n\n Since p-value is less or equal to significance level, so null hypothesis should be rejected. There is a significant difference in pack prices between the shipment modes. However, we don't know which shipment modes this applies to.\n\n\n\n\nPairwise t-tests\nThe ANOVA test didn’t tell you which categories of shipment mode had significant differences in pack prices. To pinpoint which categories had differences, you could instead use pairwise t-tests\n\n\nCode\n# Perform a pairwise t-test on pack price, grouped by shipment mode\npairwise_results = pingouin.pairwise_tests(data=late_shipments,dv='pack_price', between='shipment_mode',padjust='none')\n\n# Print pairwise_results\nprint(pairwise_results)\n\n\n        Contrast            A            B  Paired  Parametric          T  \\\n0  shipment_mode          Air  Air Charter   False        True  21.179625   \n1  shipment_mode          Air        Ocean   False        True  19.335760   \n2  shipment_mode  Air Charter        Ocean   False        True  -3.170654   \n\n          dof alternative         p-unc       BF10    hedges  \n0  600.685682   two-sided  8.748346e-75  5.809e+76  0.726592  \n1  986.979785   two-sided  6.934555e-71  1.129e+67  0.711119  \n2   35.615026   two-sided  3.123012e-03     15.277 -0.423775  \n\n\n\n\nCode\n# Modify the pairwise t-tests to use Bonferroni p-value adjustment\npairwise_results = pingouin.pairwise_tests(data=late_shipments,\n                                           dv=\"pack_price\",\n                                           between=\"shipment_mode\",\n                                           padjust=\"bonf\")\n\n# Print pairwise_results\nprint(pairwise_results)\n\n\n        Contrast            A            B  Paired  Parametric          T  \\\n0  shipment_mode          Air  Air Charter   False        True  21.179625   \n1  shipment_mode          Air        Ocean   False        True  19.335760   \n2  shipment_mode  Air Charter        Ocean   False        True  -3.170654   \n\n          dof alternative         p-unc        p-corr p-adjust       BF10  \\\n0  600.685682   two-sided  8.748346e-75  2.624504e-74     bonf  5.809e+76   \n1  986.979785   two-sided  6.934555e-71  2.080367e-70     bonf  1.129e+67   \n2   35.615026   two-sided  3.123012e-03  9.369037e-03     bonf     15.277   \n\n     hedges  \n0  0.726592  \n1  0.711119  \n2 -0.423775"
  }
]