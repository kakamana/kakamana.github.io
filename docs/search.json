[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Muhammad Asad Kamran is passionate\nMuhammad Asad Kamran has more than 15 years of experience in Software Engineering profession. Having strong hands on for architect, development, consultation & team building/managing experience of enterprise application & their integration with business critical applications, which includes SharePoint/Project Server, Asp.Net MVC & Biztalk complex applications.\nBeing a certified PMP & Prince 2 practitioner, Asad has been managing & mentoring mission critical team which delivered successful & award wining projects worth millions for clients in Middle East, Europe including Telco Operators, Oil & Gas clients, ministries, Insurance giant & multinational Attorney giants. Having numerous success stories of working with key stakeholders to develop architectural framework that aligns strategy, processes, and IT assets with business goals.\nExcellent communication, presentation, and organizational skills. Involved in successful Digital Transformation & Integration projects which provides G2G, G2B, B2B, B2C & G2C e-commerce & Digital services. His core competencies are Collaboration, Messaging, Enterprise solution architecture, Digitization transformation, project management, Agile methodologies & Data analytics."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity Of Michigan | Michigan, USA Masters in applied data science (MADS) | May 2022 - April 2024\nComsats Institute Of Information Technology | Lahore, Pakistan Bachelors Of Computer Science (Software Engineering) | 2002 - 2005"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kakamana’s Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Hypothesis Testing\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nhypothesis\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSampling Distribution\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAssessing model fit\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nSampling Methods\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSimple Logistic Regression Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to sampling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPredictions and model objects in linear regression\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSimple Linear Regression Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation in a nutshell\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nclassification\n\n\nmachine learning\n\n\nsupervised\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation and Experimental Design\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nDistribution (pdf, cdf) of iris data\n\n\n\n\n\n\n\npython\n\n\nstatistics\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nFine Tunning Model\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nsupervised\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nPreprocessing and Pipelines\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nsupervised\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nRandom Numbers and Probability\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSummary Of Statistics\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/Classification.html",
    "href": "posts/Classification/Classification.html",
    "title": "Classification",
    "section": "",
    "text": "We will explore how to solve classification problems using supervised learning techniques, which include splitting data into training and test sets, fitting a model, predicting outcomes, and evaluating accuracy. You’ll learn the relationship between model complexity and performance, applying what you learn to a churn dataset, in which you’ll classify customers’ churn status.\nThis Classification is part of Datacamp course: Supervised Learning with scikit-learn\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import the course packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport scipy.stats\nimport seaborn as sns\n\n# Import the course datasets as DataFrames\nauto = pd.read_csv(\"auto.csv\")\nboston = pd.read_csv(\"boston.csv\")\ndiabetes = pd.read_csv(\"diabetes.csv\")\ngapminder = pd.read_csv(\"gm_2008_region.csv\")\nvotes = pd.read_csv(\"votes.csv\")\nwhitewine = pd.read_csv(\"white-wine.csv\")\nredwine = pd.read_csv(\"winequality-red.csv\")\n\n# Preview the first DataFrame\nauto\n\n\n\n\n\n\n  \n    \n      \n      mpg\n      displ\n      hp\n      weight\n      accel\n      origin\n      size\n    \n  \n  \n    \n      0\n      18.0\n      250.0\n      88\n      3139\n      14.5\n      US\n      15.0\n    \n    \n      1\n      9.0\n      304.0\n      193\n      4732\n      18.5\n      US\n      20.0\n    \n    \n      2\n      36.1\n      91.0\n      60\n      1800\n      16.4\n      Asia\n      10.0\n    \n    \n      3\n      18.5\n      250.0\n      98\n      3525\n      19.0\n      US\n      15.0\n    \n    \n      4\n      34.3\n      97.0\n      78\n      2188\n      15.8\n      Europe\n      10.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      387\n      18.0\n      250.0\n      88\n      3021\n      16.5\n      US\n      15.0\n    \n    \n      388\n      27.0\n      151.0\n      90\n      2950\n      17.3\n      US\n      10.0\n    \n    \n      389\n      29.5\n      98.0\n      68\n      2135\n      16.6\n      Asia\n      10.0\n    \n    \n      390\n      17.5\n      250.0\n      110\n      3520\n      16.4\n      US\n      15.0\n    \n    \n      391\n      25.1\n      140.0\n      88\n      2720\n      15.4\n      US\n      10.0\n    \n  \n\n392 rows × 7 columns\n\n\n\n\nSupervised learning: Uses labeled data\nUnsupervised learning: uses unlabled data\n\n\n\n\nUncovering hidden patterns from unlabeled data\nExample:\n\ngrouping customers into distinct categories (clustering)\n\n\n\n\n\n\nPredictor variables / features and a target variable\nAim: predict target variable, given predictor variables\n\nClassification: Target variable consist of categories\nRegression: Target variable is continuous\n\n\n\n\n\n\nSoftware agents interact with an environment\n\nLearn how to optimize their behavior\nGiven a system of rewards and punishments\nDraws inspiration from behavioral psychology\n\nApplications\n\nEconomics\nGenetics\nGame playing\n\n\nEDA of IRIS dataset\n\n\nCode\nfrom sklearn import datasets\nplt.style.use('ggplot')\n\n\n\n\nCode\niris=datasets.load_iris()\n\n\n\n\nCode\nprint(type(iris))\nprint(iris.keys())\nprint(type(iris.data))\nprint(type(iris.keys))\nprint(iris.data.shape)\nprint(iris.target_names)\n\n\n<class 'sklearn.utils._bunch.Bunch'>\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n<class 'numpy.ndarray'>\n<class 'builtin_function_or_method'>\n(150, 4)\n['setosa' 'versicolor' 'virginica']\n\n\n\n\nCode\nx=iris.data\ny=iris.target\ndf=pd.DataFrame(x,columns=iris.feature_names)\nprint(df.head())\n\n\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2\n\n\n\n\nCode\n_ = pd.plotting.scatter_matrix(df, c = y, figsize = [8, 8], s=150, marker = 'D')\n\n\n\n\n\nBasic EDA of votes dataset\n\n\nCode\nvotes.head()\n\n\n\n\n\n\n  \n    \n      \n      party\n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      0\n      republican\n      n\n      y\n      n\n      y\n      y\n      y\n      n\n      n\n      n\n      y\n      ?\n      y\n      y\n      y\n      n\n      y\n    \n    \n      1\n      republican\n      n\n      y\n      n\n      y\n      y\n      y\n      n\n      n\n      n\n      n\n      n\n      y\n      y\n      y\n      n\n      ?\n    \n    \n      2\n      democrat\n      ?\n      y\n      y\n      ?\n      y\n      y\n      n\n      n\n      n\n      n\n      y\n      n\n      y\n      y\n      n\n      n\n    \n    \n      3\n      democrat\n      n\n      y\n      y\n      n\n      ?\n      y\n      n\n      n\n      n\n      n\n      y\n      n\n      y\n      n\n      n\n      y\n    \n    \n      4\n      democrat\n      y\n      y\n      y\n      n\n      y\n      y\n      n\n      n\n      n\n      n\n      y\n      ?\n      y\n      y\n      y\n      y\n    \n  \n\n\n\n\n\n\nCode\nvotes.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 435 entries, 0 to 434\nData columns (total 17 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   party              435 non-null    object\n 1   infants            435 non-null    object\n 2   water              435 non-null    object\n 3   budget             435 non-null    object\n 4   physician          435 non-null    object\n 5   salvador           435 non-null    object\n 6   religious          435 non-null    object\n 7   satellite          435 non-null    object\n 8   aid                435 non-null    object\n 9   missile            435 non-null    object\n 10  immigration        435 non-null    object\n 11  synfuels           435 non-null    object\n 12  education          435 non-null    object\n 13  superfund          435 non-null    object\n 14  crime              435 non-null    object\n 15  duty_free_exports  435 non-null    object\n 16  eaa_rsa            435 non-null    object\ndtypes: object(17)\nmemory usage: 57.9+ KB\n\n\n\n\nCode\nvotes.describe()\n\n\n\n\n\n\n  \n    \n      \n      party\n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      count\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n    \n    \n      unique\n      2\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n    \n    \n      top\n      democrat\n      n\n      y\n      y\n      n\n      y\n      y\n      y\n      y\n      y\n      y\n      n\n      n\n      y\n      y\n      n\n      y\n    \n    \n      freq\n      267\n      236\n      195\n      253\n      247\n      212\n      272\n      239\n      242\n      207\n      216\n      264\n      233\n      209\n      248\n      233\n      269\n    \n  \n\n\n\n\n\n\nCode\nvotes.shape\n\n\n(435, 17)\n\n\n\n\nCode\nplt.figure()\nsns.countplot(x='education', hue='party', data=votes, palette='RdBu')\nplt.xticks([0,1],['No','Yes'])\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.figure()\nsns.countplot(x='satellite', hue='party', data=votes, palette='RdBu')\nplt.xticks([0,1], ['No', 'Yes'])\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.figure()\nsns.countplot(x='missile', hue='party', data=votes, palette='RdBu')\nplt.xticks([0,1], ['No', 'Yes'])\nplt.show()\n\n\n\n\n\nUsing scikit-learn to fit classifier for IRIS data to fit a classifier\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=6)\nknn.fit(iris['data'],iris['target'])\n\n\nKNeighborsClassifier(n_neighbors=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=6)\n\n\n\n\nCode\nprint(iris['data'].shape)\nprint(iris['target'].shape)\n\n\n(150, 4)\n(150,)\n\n\nPredicting on unlabeled data\n\n\nCode\nX_new = np.array([[5.6, 2.8, 3.9, 1.1],\n[5.7, 2.6, 3.8, 1.3],\n[4.7, 3.2, 1.3, 0.2]])\n\nprediction = knn.predict(X_new)\nX_new.shape\n\n\n(3, 4)\n\n\n\n\nCode\nprint('Prediction: {}'.format(prediction))\n\n\nPrediction: [1 1 0]\n\n\nHere above it predicts one which relates to ‘versicolor’ for first two observation & 0 which relates to ‘setosa’ for the third observation as show below\n\n\nCode\nprint(iris.target_names)\n\n\n['setosa' 'versicolor' 'virginica']\n\n\nk-Nearest Neighbors: Fit\nNow that you have explored the Congressional voting records dataset, it is time to build your first classifier. You will fit a k-Nearest Neighbors classifier to the voting dataset, which has once again been loaded into a DataFrame.\nHugo discussed the importance of making sure your data conforms to the scikit-learn API format. It is necessary to place the features in an array where each column represents a feature and each row represents an observation or data point - in this case, a Congressman’s voting record. The target column must have the same number of observations as the feature data. This exercise has done this for you. Notice we named the feature array X and response variable y: This is in accordance with the common scikit-learn practice.\nBy specifying the n_neighbors parameter, you need to create a k-NN classifier with 6 neighbors and then fit it to the data\n\n\nCode\n#Here I need to load tranformed / cleaned data for votes which replace 'n' with 0 and 'y' with 1\ndf_votes = pd.read_csv(\"votes-ch1.csv\")\n# Create arrays for the features and the response variable\ny = df_votes['party'].values\nX = df_votes.drop('party', axis=1).values\n\n# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X,y)\n\n\nKNeighborsClassifier(n_neighbors=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=6)\n\n\nNow that as above shows our k-NN classifier with 6 neighbors has been fit to the data, it can be used to predict the labels of new data points.\nPredict k-nearest neighbors\nOnce we have fitted a k-NN classifier, we can use it to predict a new data point’s label. Due to the fact that all the data was used to fit the model, there are no unlabeled data available! In some cases, we can still use the .predict() method on the X that was used to fit the model, but it is not a good indicator of the model’s ability to generalize to new data sets.\nHugo will discuss a solution to this problem in the next video. For now, an unlabeled random data point has been generated and is available to you as X_new. As well as using the training data X, we will use your classifier to predict the label for this new data point. If you use .predict() on X_new, we will generate one prediction, but if you we it on X, we will generate 435 predictions: one for each sample.\nThe DataFrame has been pre-loaded as df. You will create the feature array X and target variable array Y yourself this time. DataFrame df has been pre-loaded with the data.\n\n\nCode\nX_new = pd.DataFrame((np.random.rand(1,16)))\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X)\n\n# Predict and print the label for the new data point X_new\nnew_prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(new_prediction))\n\n\nPrediction: ['democrat']\n\n\nOur model predict ‘democrat’ or ‘republican’? How sure can you be of its predictions? In other words, how can you measure its performance? This is what you will learn in below\nThe digits recognition dataset\nUp until now, you have been performing binary classification, since the target variable had two possible outcomes. Hugo, however, got to perform multi-class classification in the videos, where the target variable could take on three possible outcomes. Why does he get to have all the fun?! In the following exercises, you’ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn’s included datasets, and that is the one we will use in this exercise.\nEach sample in this scikit-learn dataset is an 8x8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. Recall that scikit-learn’s built-in datasets are of type Bunch, which are dictionary-like objects. Helpfully for the MNIST dataset, scikit-learn provides an ‘images’ key in addition to the ‘data’ and ‘target’ keys that you have seen with the Iris data. Because it is a 2D array of the images corresponding to each sample, this ‘images’ key is useful for visualizing the images, as you’ll see in this exercise (for more on plotting 2D arrays, see Chapter 2 of DataCamp’s course on Data Visualization with Python). On the other hand, the ‘data’ key contains the feature array - that is, the images as a flattened array of 64 pixels.\nNotice that you can access the keys of these Bunch objects in two different ways: By using the . notation, as in digits.images, or the [] notation, as in digits[‘images’].\nFor more on the MNIST data, check out this exercise in Part 1 of DataCamp’s Importing Data in Python course. There, the full version of the MNIST dataset is used, in which the images are 28x28. It is a famous dataset in machine learning and computer vision, and frequently used as a benchmark to evaluate the performance of a new model.\n\n\nCode\n# Import necessary modules\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\n# Load the digits dataset: digits\ndigits = datasets.load_digits()\n\n# Print the keys and DESCR of the dataset\nprint(digits.keys())\nprint(digits['DESCR'])\n\n# Print the shape of the images and data keys\nprint(digits.images.shape)\nprint(digits.data.shape)\n\n# Display digit 1010\nplt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()\n\n\ndict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n.. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 1797\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n\n(1797, 8, 8)\n(1797, 64)\n\n\nAbove It looks like the image in question corresponds to the digit ‘5’. Now, can we build a classifier that can make this prediction not only for this image, but for all the other ones in the dataset? we’ll do so in the next exercise!\nTrain/Test Split + Fit/Predict/Accuracy\nNow that we know why it’s important to separate your data into training and test sets, let’s practice on the digits dataset! Arrays for the features and target variable will be divided into training and test sets, and a k-NN classifier will be fitted to the training data and its accuracy will be calculated using .score().\n\n\nCode\n# Import necessary modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Create feature and target arrays\nX = digits.data\ny = digits.target\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n\n# Create a k-NN classifier with 7 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn.fit(X_train,y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))\n\n\nIncredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be\n\n\nCode\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n\n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train,y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n\n\nIt looks like the test accuracy is highest when using 3 and 5 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data"
  },
  {
    "objectID": "posts/Correlation and experimental design/Correlation and experimental design.html",
    "href": "posts/Correlation and experimental design/Correlation and experimental design.html",
    "title": "Correlation and Experimental Design",
    "section": "",
    "text": "We will explore how to quantify the strength of a linear relationship between two variables, and explore how confounding variables can affect the relationship between two other variables. we’ll also see how a study’s design can influence its results, change how the data should be analyzed, and potentially affect the reliability of your conclusions\nThis Correlation and Experimental Design is part of Datacamp course: Introduction to Statistic in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport numpy as np\n\n\n\n\n* Correlation coefficient\n    * Quantifies the linear relationship between two variables\n    * Number between -1 and 1\n    * Magnitude corresponds to strength of relationship\n    * Sign (+ or -) corresponds to direction of relationship\n\n* Pearson product-moment correlation(rr)\n\n\n\nHere we’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\nIn this exercise, you’ll examine the relationship between a country’s life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively.\n\n\nCode\nworld_happiness = pd.read_csv('world_happiness.csv', index_col=0)\nworld_happiness.head()\n\n\n\n\n\n\n  \n    \n      \n      country\n      social_support\n      freedom\n      corruption\n      generosity\n      gdp_per_cap\n      life_exp\n      happiness_score\n    \n  \n  \n    \n      1\n      Finland\n      2.0\n      5.0\n      4.0\n      47.0\n      42400\n      81.8\n      155\n    \n    \n      2\n      Denmark\n      4.0\n      6.0\n      3.0\n      22.0\n      48300\n      81.0\n      154\n    \n    \n      3\n      Norway\n      3.0\n      3.0\n      8.0\n      11.0\n      66300\n      82.6\n      153\n    \n    \n      4\n      Iceland\n      1.0\n      7.0\n      45.0\n      3.0\n      47900\n      83.0\n      152\n    \n    \n      5\n      Netherlands\n      15.0\n      19.0\n      12.0\n      7.0\n      50500\n      81.8\n      151\n    \n  \n\n\n\n\n\n\nCode\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x='life_exp', y='happiness_score',data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create scatterplot of happiness_score vs life_exp with trendline\nsns.lmplot(x='life_exp', y='happiness_score',data=world_happiness, ci=None)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Correlation between life_exp and happiness_score\ncor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n\nprint(cor)\n\n\n0.7802249053272062\n\n\n\n\n\n* Correlation only accounts for linear relationships\n* Transformation\n   *  Certain statistical methods rely on variables having a linear relationship\n        * Correlation coefficient\n        * Linear regression\n* Correlation does not imply causation\n    * x is correlated with yy does not mean xx causes y\n\n\n\nWhile the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it’s far from perfect. Here we’ll explore one of the caveats of the correlation coefficient by examining the relationship between a country’s GDP per capita (gdp_per_cap) and happiness score.\n\n\nCode\n# Scatterplot of gdp_per_cap and life_exp\nsns.scatterplot(x='gdp_per_cap',y='life_exp', data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Correlation between gdp_per_cap and life_exp\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n\nprint(cor)\n\n\n0.7019547642148015\n\n\n\n\n\nWhen variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. Here we’ll perform a transformation yourself\n\n\nCode\n# Scatterplot of happiness_score vs. gdp_per_cap\nsns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n\n\n\n0.7279733012222975\n\n\n\n\nCode\n# Create log_gdp_per_cap column\nworld_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n\n# Scatterplot of log_gdp_per_cap and happiness_score\nsns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness);\nplt.show()\n\n# Calculate correlation\ncor =  world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\nprint(\"\\n The relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP\")\n\n\n\n\n\n0.8043146004918288\n\n The relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP\n\n\n\n\n\nA new column has been added to world_happiness called grams_sugar_per_day, which contains the average amount of sugar eaten per person per day in each country. We’ll examine the effect of a country’s average sugar consumption on its happiness score.\n\n\nCode\nworld_happiness = pd.read_csv('world_happiness_add_sugar.csv', index_col=0)\nworld_happiness\n\n\n\n\n\n\n  \n    \n      \n      country\n      social_support\n      freedom\n      corruption\n      generosity\n      gdp_per_cap\n      life_exp\n      happiness_score\n      grams_sugar_per_day\n    \n    \n      Unnamed: 0\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      Finland\n      2\n      5\n      4.0\n      47\n      42400\n      81.8\n      155\n      86.8\n    \n    \n      2\n      Denmark\n      4\n      6\n      3.0\n      22\n      48300\n      81.0\n      154\n      152.0\n    \n    \n      3\n      Norway\n      3\n      3\n      8.0\n      11\n      66300\n      82.6\n      153\n      120.0\n    \n    \n      4\n      Iceland\n      1\n      7\n      45.0\n      3\n      47900\n      83.0\n      152\n      132.0\n    \n    \n      5\n      Netherlands\n      15\n      19\n      12.0\n      7\n      50500\n      81.8\n      151\n      122.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129\n      Yemen\n      100\n      147\n      83.0\n      155\n      2340\n      68.1\n      5\n      77.9\n    \n    \n      130\n      Rwanda\n      144\n      21\n      2.0\n      90\n      2110\n      69.1\n      4\n      14.1\n    \n    \n      131\n      Tanzania\n      131\n      78\n      34.0\n      49\n      2980\n      67.7\n      3\n      28.0\n    \n    \n      132\n      Afghanistan\n      151\n      155\n      136.0\n      137\n      1760\n      64.1\n      2\n      24.5\n    \n    \n      133\n      Central African Republic\n      155\n      133\n      122.0\n      113\n      794\n      52.9\n      1\n      22.4\n    \n  \n\n133 rows × 9 columns\n\n\n\n\n\nCode\n# Scatterplot of grams_sugar_per_day and happiness_score\nsns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor =  world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n\n\n\n0.6939100021829634\n\n\n\n\n\n\n* Vocabulary\n    * Experiment aims to answer: What is the effect of the treatment on the response?\n        * Treatment: explanatory / independent variable\n        * Response: response / dependent variable\n    * E.g.: What is the effect of an advertisement on the number of products purchased?\n        * Treatment: advertisement\n        * Response: number of products purchased\n            * Controlled experiments\n            * Participants are assigned by researchers to either treatment group or control group\n            * Treatment group sees advertisement\n            * Control group does not\n            * Group should be comparable so that causation can be inferred\n            * If groups are not comparable, this could lead to confounding (bias)\n* Gold standard of experiment\n    * Randomized controlled trial\n        * Participants are assigned to treatment/control randomly, not based on any other characteristics\n        C* hoosing randomly helps ensure that groups are comparable\n    * Placebo\n        * Resembles treatement, but has no effect\n        * Participants will not know which group they're in\n    * Double-blind trial\n        * Person administering the treatment/running the study doesn't know whether the treatment is real or a placebo\n        * Prevents bias in the response and/or analysis of results\n    * Fewopportunities for bias = more reliable conclusion about causation\n* Observational studies\n    * Participants are not assigned randomly to groups\n        * Participants assign themselves, usually based on pre-existing characteristics\n    * Many research questions are not conductive to a controlled experiment\n        * Cannot force someone to smoke or have a disease\n    * Establish association, not causation\n        * Effects can be confounded by factors that got certain people into the control or treatment group\n        * There are ways to control for confounders to get more reliable conclusions about association\n            * Longitudinal vs. cross-sectional studies\n    * Longitudinal study\n        * Participants are followed over a period of time to examine effect of treatment on response\n        * Effect of age on height is not confounded by generation\n        * More expensive, results take longer\n    * Cross-sectional study\n        * Data on participants is collected from a single snapshot in time\n        * Effect of age on height is confounded by generation\n        * Cheaper, fater, more convenient"
  },
  {
    "objectID": "posts/Correlation in a nutshell/Correlation in a nutshell.html",
    "href": "posts/Correlation in a nutshell/Correlation in a nutshell.html",
    "title": "Correlation in a nutshell",
    "section": "",
    "text": "In this article we will explore basically a linear relationship between two variables, its possible quantification (magnitude & direction). We will also touch high level of confounding & caveats of correlation. This article use exploration of study for mammals sleeping habits & world happiness\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nCode\ndf = pd.read_csv('mammals.csv')\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      species\n      body_wt\n      brain_wt\n      non_dreaming\n      dreaming\n      total_sleep\n      life_span\n      gestation\n      predation\n      exposure\n      danger\n    \n  \n  \n    \n      0\n      Africanelephant\n      6654.000\n      5712.0\n      NaN\n      NaN\n      3.3\n      38.6\n      645.0\n      3\n      5\n      3\n    \n    \n      1\n      Africangiantpouchedrat\n      1.000\n      6.6\n      6.3\n      2.0\n      8.3\n      4.5\n      42.0\n      3\n      1\n      3\n    \n    \n      2\n      ArcticFox\n      3.385\n      44.5\n      NaN\n      NaN\n      12.5\n      14.0\n      60.0\n      1\n      1\n      1\n    \n    \n      3\n      Arcticgroundsquirrel\n      0.920\n      5.7\n      NaN\n      NaN\n      16.5\n      NaN\n      25.0\n      5\n      2\n      3\n    \n    \n      4\n      Asianelephant\n      2547.000\n      4603.0\n      2.1\n      1.8\n      3.9\n      69.0\n      624.0\n      3\n      5\n      4\n    \n  \n\n\n\n\n\n\nCode\ndf.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 62 entries, 0 to 61\nData columns (total 11 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   species       62 non-null     object \n 1   body_wt       62 non-null     float64\n 2   brain_wt      62 non-null     float64\n 3   non_dreaming  48 non-null     float64\n 4   dreaming      50 non-null     float64\n 5   total_sleep   58 non-null     float64\n 6   life_span     58 non-null     float64\n 7   gestation     58 non-null     float64\n 8   predation     62 non-null     int64  \n 9   exposure      62 non-null     int64  \n 10  danger        62 non-null     int64  \ndtypes: float64(7), int64(3), object(1)\nmemory usage: 5.5+ KB\n\n\n\n\n\nThe sleep time of 39 species of mammals distributed over 13 orders is analyzed in regards to their distribution over the 13 orders. There are 62 observations across 11 variables.\nspecies : Mammal species\nbody_wt : Mammal’s total body weight (kg)\nbrain_wt : Mammal’s brain weight (kg)\nnon_dreaming : Sleep hours without dreaming\ndreaming : Sleep hours spent dreaming\ntotal_sleep : Total number of hours of sleep\nlife_span : Life span (in years)\ngestation : Days during gestation / pregnancy\nThe likelihood that a mammal will be preyed upon. 1 = least likely to be preyed on. 5 = most likely to be preyed upon.\nexposure : How exposed a mammal is during sleep. 1 = least exposed (e.g., sleeps in a well-protected den). 5 = most exposed.\nA measure of how much danger the mammal faces. This index is based upon Predation and Exposure. 1 = least danger from other animals. 5 = most danger from other animals.\n\n\nCode\ndf.isnull().sum()\n\n\nspecies          0\nbody_wt          0\nbrain_wt         0\nnon_dreaming    14\ndreaming        12\ntotal_sleep      4\nlife_span        4\ngestation        4\npredation        0\nexposure         0\ndanger           0\ndtype: int64\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Body Weight Distribution')\nsns.histplot(df['body_wt'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Body Weight Distribution'}, xlabel='body_wt', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Body Wight')\nsns.barplot(x='body_wt', y='species', data=df.sort_values('body_wt',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Body Wight'}, xlabel='body_wt', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"body_wt\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c10a9430>\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Brain Weight Distribution')\nsns.histplot(df['brain_wt'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Brain Weight Distribution'}, xlabel='brain_wt', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Brain Wight')\nsns.barplot(x='brain_wt', y='species', data=df.sort_values('brain_wt',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Brain Wight'}, xlabel='brain_wt', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"brain_wt\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c5b40bb0>\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Life Span Distribution')\nsns.histplot(df['life_span'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Life Span Distribution'}, xlabel='life_span', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Life Span')\nsns.barplot(x='life_span', y='species', data=df.sort_values('life_span',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Life Span'}, xlabel='life_span', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"life_span\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c6460d60>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Predation Total Sleep Visualization')\nsns.countplot(x='predation',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"predation\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"predation\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"predation\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3907231976.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"predation\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='predation', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Exposure Total Sleep Visualization')\nsns.countplot(x='exposure',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"exposure\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"exposure\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"exposure\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3542283944.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"exposure\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='exposure', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Danger Total Sleep Visualization')\nsns.countplot(x='danger',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"danger\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"danger\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"danger\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3554697531.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"danger\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='danger', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\nx = explanatory / independent variables y = response / dependent variable\n\n\nCode\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x='total_sleep', y='dreaming',data=df)\n\n# Show plot\nplt.title('Sleeping habits')\nplt.ylabel(\"rem sleep per day(hour\")\nplt.xlabel(\"total sleep per day(hour\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.lmplot(x=\"total_sleep\", y=\"dreaming\", data=df, ci=None)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\ndf['total_sleep'].corr(df['dreaming'])\n\n\n0.7270869571641637\n\n\n\n\nCode\ndf['dreaming'].corr(df['total_sleep'])\n\n\n0.7270869571641637\n\n\n\n\n\n\n\nCode\n# Create a scatterplot of gestation vs. total_sleep and show\nsns.scatterplot(x='gestation', y='total_sleep',data=df)\n\n# Show plot\nplt.title('High negative correlation')\nplt.ylabel(\"gestation\")\nplt.xlabel(\"total sleep per day(hour\")\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.scatterplot(x='total_sleep', y='dreaming',data=df)\nplt.title(\"low positive correlation\")\nplt.show()"
  },
  {
    "objectID": "posts/Distribution (pdf, cdf) of iris dataset/Distribution (pdf, cdf) of iris data set.html",
    "href": "posts/Distribution (pdf, cdf) of iris dataset/Distribution (pdf, cdf) of iris data set.html",
    "title": "Distribution (pdf, cdf) of iris data",
    "section": "",
    "text": "Lets explore distribution functions pdf and cdf using Iris data set\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings;\nwarnings.filterwarnings('ignore');\n\n\n\n\nCode\niris=pd.read_csv('iris.csv')\niris.head()\n\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      type\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n    \n  \n\n\n\n\n\n\nCode\niris.shape\n\n\n(150, 5)\n\n\n\n\nCode\niris.columns\n\n\nIndex(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'type'], dtype='object')\n\n\n\n\nCode\niris['type'].value_counts()\n\n\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\nName: type, dtype: int64\n\n\n\n\nCode\nsetosa=iris[iris['type']==\"Iris-setosa\"]\nsetosa['petal_length'].value_counts()\n\n\n1.5    14\n1.4    12\n1.3     7\n1.6     7\n1.7     4\n1.2     2\n1.9     2\n1.1     1\n1.0     1\nName: petal_length, dtype: int64\n\n\n\n\n\n\nCode\niris.plot(kind='scatter',x='sepal_length',y='sepal_width');\nplt.show()\n\n\n\n\n\n\n\nCode\n#here we plot the scatter diagram with colour coding\nsns.set_style('whitegrid')\nsns.FacetGrid(iris,hue=\"type\",aspect = 2).map(plt.scatter,\"sepal_length\",\"sepal_width\").add_legend()\nplt.show()\n\n\n\n\n\n\n\n\nFor cross-referencing\n\n\nCode\nplt.close()\nsns.set_style(\"whitegrid\")\nsns.pairplot(iris,hue=\"type\",size=3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsetosa=iris.loc[iris[\"type\"]==\"Iris-setosa\"]\nversicolor=iris.loc[iris[\"type\"]==\"Iris-versicolor\"]\nvirginica=iris.loc[iris[\"type\"]==\"Iris-virginica\"]\n\n\n\n\nCode\nplt.plot(setosa[\"petal_length\"],np.zeros_like(setosa['petal_length']), 'o')\nplt.plot(versicolor[\"petal_length\"],np.zeros_like(versicolor['petal_length']), 'o')\nplt.plot(virginica[\"petal_length\"],np.zeros_like(virginica['petal_length']), 'o')\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.distplot(iris[iris['type']== 'Iris-setosa']['petal_length'])\n\n\n<AxesSubplot:xlabel='petal_length', ylabel='Density'>\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\",aspect = 2).map(sns.distplot, \"petal_length\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect = 2).map(sns.distplot, \"petal_width\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect=2).map(sns.distplot, \"sepal_length\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect=2).map(sns.distplot, \"sepal_width\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = False)\n\nprint(\"histogram counts\\n\",counts)\npdf = counts/(sum(counts))\nprint(\"Sum of count is\\n\",sum(counts))\nprint(\"bin edges \\n\",bin_edges)\nprint(\"PDF below \\n\",pdf);\n\nplt.gca().legend(('Pdf'))\nplt.title('PDF and PDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.plot(bin_edges[1:],pdf)\n\n\nhistogram counts\n [ 1  1  2  7 12 14  7  4  0  2]\nSum of count is\n 50\nbin edges \n [1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\nPDF below \n [0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n\n\n[<matplotlib.lines.Line2D at 0x1a5ae102100>]\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = False)\n\nprint(\"histogram counts\\n\",counts)\npdf = counts/(sum(counts))\nprint(\"Sum of count is\\n\",sum(counts))\nprint(\"bin edges \\n\",bin_edges)\nprint(\"PDF is below  \\n\",pdf);\n\ncdf = np.cumsum(pdf)\nprint(\"CDF is below\\n\",cdf)\nplt.gca().legend(('Cdf'))\nplt.title('CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\n\nplt.plot(bin_edges[1:],cdf)\n\n\nhistogram counts\n [ 1  1  2  7 12 14  7  4  0  2]\nSum of count is\n 50\nbin edges \n [1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\nPDF is below  \n [0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\nCDF is below\n [0.02 0.04 0.08 0.22 0.46 0.74 0.88 0.96 0.96 1.  ]\n\n\n[<matplotlib.lines.Line2D at 0x1a5ae1721c0>]\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\n\nprint(counts)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\n\n#compute CDF\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\nplt.gca().legend(('Pdf','Cdf'))\nplt.title('PDF and CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.show();\n\n\n[0.22222222 0.22222222 0.44444444 1.55555556 2.66666667 3.11111111\n 1.55555556 0.88888889 0.         0.44444444]\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\n\n\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=20,\n                                 density = True)\npdf = counts/(sum(counts))\nplt.plot(bin_edges[1:],pdf);\nplt.gca().legend(('Pdf','Cdf','bin edges'))\nplt.title('PDF and CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.show();\n\n\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\n\n# virginica\ncounts, bin_edges = np.histogram(virginica['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\n\n#versicolor\ncounts, bin_edges = np.histogram(versicolor['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\nplt.title('PDF and CDF For iris_versicolor')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\n\nplt.show();\n\n\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n[0.02 0.1  0.24 0.08 0.18 0.16 0.1  0.04 0.02 0.06]\n[4.5  4.74 4.98 5.22 5.46 5.7  5.94 6.18 6.42 6.66 6.9 ]\n[0.02 0.04 0.06 0.04 0.16 0.14 0.12 0.2  0.14 0.08]\n[3.   3.21 3.42 3.63 3.84 4.05 4.26 4.47 4.68 4.89 5.1 ]\n\n\n\n\n\n\n\n\n\n\nCode\n#Mean, Variance, Std-deviation,\nprint(\"Means:\")\nprint(np.mean(setosa[\"petal_length\"]))\n#Mean with an outlier.\nprint(np.mean(np.append(setosa[\"petal_length\"],50)));\nprint(np.mean(virginica[\"petal_length\"]))\nprint(np.mean(versicolor[\"petal_length\"]))\n\nprint(\"\\nStd-dev:\");\nprint(np.std(setosa[\"petal_length\"]))\nprint(np.std(virginica[\"petal_length\"]))\nprint(np.std(versicolor[\"petal_length\"]))\n\n\nMeans:\n1.464\n2.4156862745098038\n5.5520000000000005\n4.26\n\nStd-dev:\n0.17176728442867112\n0.546347874526844\n0.4651881339845203\n\n\n\n\n\n\n\nCode\n#Median, Quantiles, Percentiles, IQR.\nprint(\"\\nMedians:\")\nprint(np.median(setosa[\"petal_length\"]))\n#Median with an outlier\nprint(np.median(np.append(setosa[\"petal_length\"],50)));\nprint(np.median(virginica[\"petal_length\"]))\nprint(np.median(versicolor[\"petal_length\"]))\n\n\nprint(\"\\nQuantiles:\")\nprint(np.percentile(setosa[\"petal_length\"],np.arange(0, 100, 25)))\nprint(np.percentile(virginica[\"petal_length\"],np.arange(0, 100, 25)))\nprint(np.percentile(versicolor[\"petal_length\"], np.arange(0, 100, 25)))\n\nprint(\"\\n90th Percentiles:\")\nprint(np.percentile(setosa[\"petal_length\"],90))\nprint(np.percentile(virginica[\"petal_length\"],90))\nprint(np.percentile(versicolor[\"petal_length\"], 90))\n\nfrom statsmodels import robust\nprint (\"\\nMedian Absolute Deviation\")\nprint(robust.mad(setosa[\"petal_length\"]))\nprint(robust.mad(virginica[\"petal_length\"]))\nprint(robust.mad(versicolor[\"petal_length\"]))\n\n\n\nMedians:\n1.5\n1.5\n5.55\n4.35\n\nQuantiles:\n[1.    1.4   1.5   1.575]\n[4.5   5.1   5.55  5.875]\n[3.   4.   4.35 4.6 ]\n\n90th Percentiles:\n1.7\n6.31\n4.8\n\nMedian Absolute Deviation\n0.14826022185056031\n0.6671709983275211\n0.5189107764769602\n\n\n\n\n\n\n\nCode\nsns.boxplot(x='type',y='petal_length', data=iris)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.violinplot(x=\"type\", y=\"petal_length\", data=iris, size=8)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.jointplot(x=\"petal_length\", y=\"petal_width\", data=setosa, kind=\"kde\");\nplt.show();"
  },
  {
    "objectID": "posts/Fine Tuning Model/Fine Tuning Model.html",
    "href": "posts/Fine Tuning Model/Fine Tuning Model.html",
    "title": "Fine Tunning Model",
    "section": "",
    "text": "Fine Tunning Model\nAfter training models, you’ll learn how to assess them in this chapter. You’ll learn how to analyze classification model performance using scikit-learn by using several metrics and a visualization technique. Using hyperparameter tuning, you will also be able to optimize classification and regression models.\nThis Fine Tunning Model is part of Datacamp course: Supervised Learning with scikit-learn\nThis is my learning experience of data science through DataCamp\n\n\nOptimizing your model\nAfter training your model, we must evaluate its performance. In this section, we will explore some of the other metrics available in scikit-learn for assessing our model’s performance. Using hyperparameter tuning, you can optimize your classification and regression models.\n\nClassification metrics\nChapter 1 evaluated the accuracy of your k-NN classifier. As Andy discussed, accuracy is not always an informative metric. By computing a confusion matrix and generating a classification report, you will evaluate the performance of binary classifiers.\nThe classification report consisted of three rows and an additional support column, as shown in the video. In the video example, the support was the number of Republicans or Democrats in the test set used to compute the classification report. These columns gave the precision, recall, and f1-score metrics for that particular class.\nThis tutorial uses the PIMA Indians dataset available at the UCI Machine Learning Repository. Based on factors such as BMI, age, and number of pregnancies, we can predict whether or not a given female patient will develop diabetes. As a result, it is a binary classification problem. Diabetes is not present in a patient with a target value of 0, whereas diabetes is present in a patient with a target value of 1. To deal with missing values, the dataset has been preprocessed in earlier excercises.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n\n\n\nCode\ndf=pd.read_csv('diabetes.csv')\n\ndf.insulin.replace(0, np.nan, inplace=True)\ndf.triceps.replace(0, np.nan, inplace=True)\ndf.bmi.replace(0, np.nan, inplace=True)\n\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.fillna(x.mean()))\ny = df['diabetes']\nX = df.drop('diabetes', axis=1)\n\n# Create training and test set\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n\n# Instantiate a k-NN classifier: knn\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train,y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n[[176  30]\n [ 52  50]]\n              precision    recall  f1-score   support\n\n           0       0.77      0.85      0.81       206\n           1       0.62      0.49      0.55       102\n\n    accuracy                           0.73       308\n   macro avg       0.70      0.67      0.68       308\nweighted avg       0.72      0.73      0.72       308\n\n\n\nBy analyzing the confusion matrix and classification report, you can get a much better understanding of your classifier’s performance.\nAn ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate\n\nBuilding a logistic regression model\nTime to build your first logistic regression model! As Hugo showed in the video, scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as ‘estimators’. You’ll see this now for yourself as you train a logistic regression model on exactly the same data as in the previous exercise. Will it outperform k-NN? There’s only one way to find out!\n\n\nCode\n# Import the necessary modules\nfrom sklearn.linear_model import LogisticRegression\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n# Create the classifier: logreg\nlogreg = LogisticRegression(solver=\"liblinear\")\n\n# Fit the classifier to the training data\nlogreg.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n[[176  30]\n [ 35  67]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.85      0.84       206\n           1       0.69      0.66      0.67       102\n\n    accuracy                           0.79       308\n   macro avg       0.76      0.76      0.76       308\nweighted avg       0.79      0.79      0.79       308\n\n\n\nROC curve plotting\nThe previous exercise was a great success - you now have a new classifier in your toolbox!\nModel performance can be evaluated quantitatively using classification reports and confusion matrices, while visually using ROC curves. Most scikit-learn classifiers have a .predict_proba() method that returns the probability of a given sample being in a particular class, as Hugo demonstrated in the video. After building a logistic regression model, you will plot an ROC curve to evaluate its performance. As a result, you will become familiar with the .predict_proba() method.\nYou’ll continue working with the PIMA Indians diabetes dataset here\n\n\nCode\n# Import necessary modules\nfrom sklearn.metrics import roc_curve\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\n\n\n\n\nCurve of precision-recall\nWe may have noticed that your ROC curve’s y-axis (True positive rate) is also known as recall. ROC curves are not the only way to evaluate model performance visually. Precision-recall curves are generated by plotting precision and recall at different thresholds. Recall that precision and recall are defined as follows:\n\nThe precision-recall curve for the diabetes dataset can be seen below. IPython Shell displays the classification report and confusion matrix.\nTake a look at the precision-recall curve and then consider the following statements. Pick the statement that is not true. If the individual has diabetes, the class is positive (1).\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n#\n\ndf = pd.read_csv('diabetes.csv')\n\ndf.insulin.replace(0, np.nan, inplace=True)\ndf.triceps.replace(0, np.nan, inplace=True)\ndf.bmi.replace(0, np.nan, inplace=True)\n\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.fillna(x.mean()))\n\n#\ny = df['diabetes']\nX = df.drop('diabetes', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n#\nclf = LogisticRegression(solver=\"liblinear\")#add solver by Jinny\nclf.fit(X_train, y_train)\n\n#\nfrom sklearn.metrics import precision_recall_curve\n\n#\ny_pred_prob = clf.predict_proba(X_test)[:,1]\n\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n#\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n#\nplt.figure(figsize=(6, 6), dpi=None)\nplt.plot(recall, precision)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='best')\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n              precision    recall  f1-score   support\n\n           0       0.83      0.85      0.84       206\n           1       0.69      0.66      0.67       102\n\n    accuracy                           0.79       308\n   macro avg       0.76      0.76      0.76       308\nweighted avg       0.79      0.79      0.79       308\n\n[[176  30]\n [ 35  67]]\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#\n\n\ndf = pd.read_csv('diabetes.csv')\n\ndf.insulin.replace(0, np.nan, inplace=True)\ndf.triceps.replace(0, np.nan, inplace=True)\ndf.bmi.replace(0, np.nan, inplace=True)\n\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.fillna(x.mean()))\ny = df['diabetes']\nX = df.drop('diabetes', axis=1)\n# Import necessary modules\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n# Instantiate a k-NN classifier: knn\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n[[176  30]\n [ 52  50]]\n              precision    recall  f1-score   support\n\n           0       0.77      0.85      0.81       206\n           1       0.62      0.49      0.55       102\n\n    accuracy                           0.73       308\n   macro avg       0.70      0.67      0.68       308\nweighted avg       0.72      0.73      0.72       308\n\n\n\nCompute the AUC\nSuppose we have a binary classifier that makes guesses at random. It would be correct approximately 50% of the time, and the ROC curve would be a diagonal line where the True Positive Rate and False Positive Rate are always equal. This ROC curve has an Area under it of 0.5. Hugo discussed the AUC in the video as an informative metric for evaluating models. The model is better than random guessing if the AUC is greater than 0.5. Always a good sign!\nWe will calculate AUC scores on the diabetes dataset using the roc_auc_score() function from sklearn.metrics.\n\n\nCode\n# Import necessary modules\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc = cross_val_score(logreg,X,y,cv=5,scoring='roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n\n\nAUC: 0.8255282695602512\nAUC scores computed using 5-fold cross-validation: [0.80203704 0.80648148 0.81481481 0.86264151 0.8554717 ]\n\n\nHyperparameter tuning with GridSearchCV\nUsing GridSearchCV on the voting dataset, we tune the n_neighbors parameter of KNeighborsClassifier(). Now we will practice this thoroughly using logistic regression on the diabetes dataset instead!\nWe saw earlier that logistic regression also has a regularization parameter: C. This parameter controls the inverse of regularization strength. A large number can result in an overfit model, while a small number can result in an underfit model.\nYou have been provided with the hyperparameter space for C. We will use GridSearchCV and logistic regression to find the optimal C. X represents the feature array and Y represents the target variable array.\nThis is why we haven’t separated the data into training and test sets. I agree with your observation! We will focus on setting up the hyperparameter grid and performing grid-search cross-validation. In practice, we will indeed want to hold out a portion of your data for evaluation purposes, and you will learn all about this in the next video!\n\n\nCode\n# Import necessary modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Setup the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\n# Instantiate a logistic regression classifier: logreg\nlogreg = LogisticRegression(solver=\"liblinear\")\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X,y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Best score is {}\".format(logreg_cv.best_score_))\n\n\nTuned Logistic Regression Parameters: {'C': 3.727593720314938}\nBest score is 0.7708768355827178\n\n\nHyperparameter tuning with RandomizedSearchCV\nGridSearchCV is computationally expensive, especially when dealing with multiple hyperparameters and large hyperparameter spaces. To solve this problem, RandomizedSearchCV can be used, in which not all hyperparameter values are tested. A fixed number of hyperparameter settings is instead sampled from specified probability distributions. In this exercise, you will practice using RandomizedSearchCV.\nYou will also be introduced to a new model: the Decision Tree. You don’t need to worry about the specifics of how this model works. In scikit-learn, decision trees also have .fit() and .predict() methods, just like k-NN, linear regression, and logistic regression. In RandomizedSearchCV, decision trees are ideal because they have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf.\nThe diabetes dataset has been preloaded with the feature array X and target variable array Y. You have been given the hyperparameter settings. To determine the optimal hyperparameters, you will use RandomizedSearchCV\n\n\nCode\n# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X,y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\n\n\nTuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 5, 'min_samples_leaf': 5}\nBest score is 0.7422629657923776\n\n\nHold-out set in practice I: Classification\nWe will now practice evaluating a model with tuned hyperparameters on a hold-out set. X and Y have been preloaded from the diabetes dataset as feature arrays and target variable arrays, respectively.\nIn addition to CC, logistic regression also has a ‘penalty’ hyperparameter that specifies whether ‘l1’ or ‘l2’ regularization should be used. This exercise requires you to create a hold-out set, tune the ‘C’ and ‘penalty’ hyperparameters of a logistic regression classifier using GridSearchCV.\n\n\nCode\n# Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Create the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the logistic regression classifier: logreg\nlogreg = LogisticRegression(solver='liblinear')\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train,y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n\n\nTuned Logistic Regression Parameter: {'C': 0.4393970560760795, 'penalty': 'l1'}\nTuned Logistic Regression Accuracy: 0.7673913043478262\n\n\nHold-out set in practice II: Regression\nRemember lasso and ridge regression from the previous chapter? Lasso used the penalty to regularize, while ridge used the penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties:\n\nIn scikit-learn, this term is represented by the ‘l1_ratio’parameter: An ’l1_ratio’ of 1 corresponds to an L1L1penalty, and anything lower is a combination of L1 and L2.\nIn this exercise, you will GridSearchCV to tune the ‘l1_ratio’ of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model’s performance.\n\n\nCode\ndf = pd.read_csv('gapminder-clean.csv')\ny = df['life'].values\nX = df.drop('life', axis=1).values\n\n\n\n\nImport necessary modules\nfrom sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV from sklearn.model_selection import train_test_split\n\n\nCreate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n\n\nCreate the hyperparameter grid\nl1_space = np.linspace(0, 1, 30) param_grid = {‘l1_ratio’: l1_space}\n\n\nInstantiate the ElasticNet regressor: elastic_net\nelastic_net = ElasticNet()\n\n\nSetup the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n\n\nFit it to the training data\ngm_cv.fit(X_train,y_train)\n\n\nPredict on the test set and compute metrics\ny_pred = gm_cv.predict(X_test) r2 = gm_cv.score(X_test, y_test) mse = mean_squared_error(y_test, y_pred) print(“Tuned ElasticNet l1 ratio: {}”.format(gm_cv.best_params_)) print(“Tuned ElasticNet R squared: {}”.format(r2)) print(“Tuned ElasticNet MSE: {}”.format(mse))\nNow that we have basic understanding how to fine-tune your models, it’s time to learn about preprocessing techniques and how to piece together all the different stages of the machine learning process into a pipeline!"
  },
  {
    "objectID": "posts/Intoducting to sampling/Introduction to Sampling.html",
    "href": "posts/Intoducting to sampling/Introduction to Sampling.html",
    "title": "Introduction to sampling",
    "section": "",
    "text": "Get a better understanding of what sampling is and why it is so powerful. Additionally, We will learn about the problems associated with convenience sampling and what the difference between true randomness and pseudo-randomness is.\nThis Introduction to sampling is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\n\nPopulation: It is complete dataset\nSample: It is subset of data you calculate on\n\nPopulation parameter: It is a calculation on population dataset Points vs. flavor: population pts_vs_flavor_pop = coffee_ratings[[“total_cup_points”, “flavor”]] np.mean(pts_vs_flavor_pop[‘total_cup_points’])\nPoint estimate: Or sample statistic is a calculation made on sample dataset Points vs. flavor: 10 row sample pts_vs_flavor_samp = pts_vs_flavor_pop.sample(n=10) cup_points_samp = coffee_ratings[‘total_cup_points’].sample(n=10) np.mean(cup_points_samp)\n\n\n\nThe purpose of this exercise is to explore Spotify song data. There are over 40,000 rows in this population dataset, each representing a song. These columns include the title of the song, the artists who performed it, the release year, and attributes of the song, such as its duration, tempo, and danceability. To begin, you should examine the durations.\nThe Spotify dataset will be sampled and the mean duration of the sample will be compared with the mean duration of the population.\n\n\nCode\nspotify_population=pd.read_feather(\"dataset/spotify_2000_2020.feather\")\nspotify_population.head()\n\n\n\n\n\n\n  \n    \n      \n      acousticness\n      artists\n      danceability\n      duration_ms\n      duration_minutes\n      energy\n      explicit\n      id\n      instrumentalness\n      key\n      liveness\n      loudness\n      mode\n      name\n      popularity\n      release_date\n      speechiness\n      tempo\n      valence\n      year\n    \n  \n  \n    \n      0\n      0.97200\n      ['David Bauer']\n      0.567\n      313293.0\n      5.221550\n      0.227\n      0.0\n      0w0D8H1ubRerCXHWYJkinO\n      0.601000\n      10.0\n      0.110\n      -13.441\n      1.0\n      Shout to the Lord\n      47.0\n      2000\n      0.0290\n      136.123\n      0.0396\n      2000.0\n    \n    \n      1\n      0.32100\n      ['Etta James']\n      0.821\n      360240.0\n      6.004000\n      0.418\n      0.0\n      4JVeqfE2tpi7Pv63LJZtPh\n      0.000372\n      9.0\n      0.222\n      -9.841\n      0.0\n      Miss You\n      51.0\n      2000-12-12\n      0.0407\n      117.382\n      0.8030\n      2000.0\n    \n    \n      2\n      0.00659\n      ['Quasimoto']\n      0.706\n      202507.0\n      3.375117\n      0.602\n      1.0\n      5pxtdhLAi0RTh1gNqhGMNA\n      0.000138\n      11.0\n      0.400\n      -8.306\n      0.0\n      Real Eyes\n      44.0\n      2000-06-13\n      0.3420\n      89.692\n      0.4790\n      2000.0\n    \n    \n      3\n      0.00390\n      ['Millencolin']\n      0.368\n      173360.0\n      2.889333\n      0.977\n      0.0\n      3jRsoe4Vkxa4BMYqGHX8L0\n      0.000000\n      11.0\n      0.350\n      -2.757\n      0.0\n      Penguins & Polarbears\n      52.0\n      2000-02-22\n      0.1270\n      165.889\n      0.5480\n      2000.0\n    \n    \n      4\n      0.12200\n      ['Steve Chou']\n      0.501\n      344200.0\n      5.736667\n      0.511\n      0.0\n      4mronxcllhfyhBRqyZi8kU\n      0.000000\n      7.0\n      0.279\n      -9.836\n      0.0\n      黃昏\n      53.0\n      2000-12-25\n      0.0291\n      78.045\n      0.1130\n      2000.0\n    \n  \n\n\n\n\n\n\nCode\n# Sample 1000 rows from spotify_population\nspotify_sample = spotify_population.sample(n=1000)\n\n# Print the sample\nprint(spotify_sample)\n\n\n       acousticness                                            artists  \\\n7874        0.66400                                    ['The Walters']   \n2664        0.01020                    ['Beastie Boys', 'Fatboy Slim']   \n1683        0.00241                                          ['batta']   \n14491       0.11400            ['AJ Mitchell', 'Ava Max', 'Sam Feldt']   \n34495       0.96800                                         ['Yiruma']   \n...             ...                                                ...   \n25541       0.85400  ['Andrew Lloyd Webber', 'Patrick Wilson', 'Emm...   \n904         0.71900                                   ['Carl Carlton']   \n26932       0.02910                                    ['Cory Asbury']   \n30144       0.14900                              ['Twenty One Pilots']   \n12676       0.35000                                  ['Grupo Intenso']   \n\n       danceability  duration_ms  duration_minutes  energy  explicit  \\\n7874          0.747     151683.0          2.528050   0.422       0.0   \n2664          0.650     248507.0          4.141783   0.942       0.0   \n1683          0.389     145400.0          2.423333   0.988       0.0   \n14491         0.732     193548.0          3.225800   0.850       0.0   \n34495         0.287     218293.0          3.638217   0.292       0.0   \n...             ...          ...               ...     ...       ...   \n25541         0.194     294160.0          4.902667   0.119       0.0   \n904           0.546     153947.0          2.565783   0.828       0.0   \n26932         0.572     333386.0          5.556433   0.685       0.0   \n30144         0.550     277013.0          4.616883   0.625       0.0   \n12676         0.718     219960.0          3.666000   0.529       0.0   \n\n                           id  instrumentalness  key  liveness  loudness  \\\n7874   70QqoQ3krRFUHfEzit7vjT          0.002770  7.0    0.3920   -10.008   \n2664   2WGGxhsc2WtPNkhsXWVcYb          0.000000  1.0    0.1220    -6.609   \n1683   5V5akuBxKpIlTUPaueNpyy          0.000615  6.0    0.3460    -1.949   \n14491  2wenGTypSYHXl1sN1pNC7X          0.000002  1.0    0.0388    -5.999   \n34495  3xr8COed4nPPn6XWZ0iCGr          0.978000  9.0    0.0900   -19.285   \n...                       ...               ...  ...       ...       ...   \n25541  5klrh466oGToybceGHPGAX          0.000737  1.0    0.1090   -20.926   \n904    5i7rT8lbGzjj1n7TTXR5U8          0.030000  4.0    0.3720    -4.771   \n26932  0rH0mprtecH3grD9HFM5AD          0.000000  6.0    0.0963    -7.290   \n30144  4IN3imzEuTsiHO6tOwDQu5          0.000000  1.0    0.1610    -8.213   \n12676  0l7q7H1zYiJ9XHVqim2Uwc          0.000000  7.0    0.1510    -8.769   \n\n       mode                                           name  popularity  \\\n7874    1.0                                   Goodbye Baby        55.0   \n2664    1.0  Body Movin' - Fatboy Slim Remix/2005 Remaster        48.0   \n1683    0.0                                          chase        61.0   \n14491   1.0   Slow Dance (feat. Ava Max) - Sam Feldt Remix        72.0   \n34495   1.0                             River Flows in You        62.0   \n...     ...                                            ...         ...   \n25541   1.0                               All I Ask Of You        57.0   \n904     1.0                               Everlasting Love        48.0   \n26932   1.0                                  Reckless Love        71.0   \n30144   0.0                                       Trapdoor        56.0   \n12676   1.0                                         Y Volo        45.0   \n\n      release_date  speechiness    tempo  valence    year  \n7874    2015-10-20       0.0294  111.141   0.5950  2015.0  \n2664    2005-01-01       0.0754  101.786   0.7850  2005.0  \n1683    2016-07-27       0.1070  111.975   0.0996  2016.0  \n14491   2019-10-25       0.0444  124.024   0.3720  2019.0  \n34495   2011-12-09       0.0541  145.703   0.3460  2011.0  \n...            ...          ...      ...      ...     ...  \n25541   2004-12-10       0.0398   85.698   0.1400  2004.0  \n904     2009-01-01       0.0394  121.418   0.4010  2009.0  \n26932   2018-01-26       0.0356  110.698   0.2320  2018.0  \n30144   2009-12-29       0.0399  149.927   0.3170  2009.0  \n12676   2001-10-16       0.0325  142.069   0.9440  2001.0  \n\n[1000 rows x 20 columns]\n\n\n\n\nCode\n# Calculate the mean duration in mins from spotify_population\nmean_dur_pop = spotify_population['duration_minutes'].mean()\n\n# Calculate the mean duration in mins from spotify_sample\nmean_dur_samp = spotify_sample['duration_minutes'].mean()\n\n# Print the means\nprint(mean_dur_pop)\nprint(mean_dur_samp)\nprint(\"\\n Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.\")\n\n\n3.8521519140900073\n3.8048647333333334\n\n Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.\n\n\n\n\n\n\n\nCode\n# Subset the loudness column of spotify_population\nloudness_pop = spotify_population['loudness']\n\n# Sample 100 values of loudness_pop\nloudness_samp = loudness_pop.sample(n=100)\n\n# Print the sample\nprint(loudness_samp)\n\n\n28889   -6.697\n41542   -3.166\n24096   -8.327\n38021   -5.596\n29360   -3.779\n         ...  \n34958   -7.206\n41513   -7.560\n28413   -7.113\n41429   -6.073\n38857   -4.433\nName: loudness, Length: 100, dtype: float64\n\n\n\n\nCode\n# Calculate the mean of loudness_pop\nmean_loudness_pop = np.mean(loudness_pop)\n\n# Calculate the mean of loudness_samp\nmean_loudness_samp = np.mean(loudness_samp)\n\n# Print the means\nprint(mean_loudness_pop)\nprint(mean_loudness_samp)\nprint(\"\\n Again, notice that the calculated value (the mean) is close but not identical in each case\")\n\n\n-7.366856851353947\n-7.385839999999999\n\n Again, notice that the calculated value (the mean) is close but not identical in each case\n\n\n\n\n\nCollecting data by easiest method is convenience sampling\nSample bias: sample not true representation of population Selection bias\n\n\n\nIn your previous example, you saw that convenience sampling, which is the collection of data using the simplest method, can lead to samples that are not representative of the population. In other words, the findings of the sample cannot be generalized to the entire population. It is possible to determine whether or not a sample is representative of the population by examining the distributions of the population and the sample\n\n\nCode\n# Visualize the distribution of acousticness with a histogram\nwidth = 0.01\nspotify_population['acousticness'].hist(bins=np.arange(0,1.01,width))\nplt.show()\n\n\n\n\n\n\n\nCode\nspotify_mysterious_sample=spotify_population.sample(n=1107)\n# Update the histogram to use spotify_mysterious_sample\nspotify_mysterious_sample['acousticness'].hist(bins=np.arange(0, 1.01, 0.01))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Visualize the distribution of duration_minutes as a histogram\nspotify_population['duration_minutes'].hist(bins=np.arange(0,15.5,0.5))\nplt.show()\n\n\n\n\n\n\n\nCode\nspotify_mysterious_sample2=spotify_population.sample(n=50)\n# Update the histogram to use spotify_mysterious_sample2\nspotify_mysterious_sample2['duration_minutes'].hist(bins=np.arange(0, 15.5, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate random numbers from a Uniform(-3, 3)\nuniforms = np.random.uniform(low=-3, high=3, size=5000)\n\n# Print uniforms\nprint(uniforms)\n\n# Plot a histogram of uniform values, binwidth 0.25\nplt.hist(uniforms, bins=np.arange(-3,3.25,0.25))\nplt.show()\n\n\n[ 0.46978238 -1.66176314  1.31080161 ...  0.27384823  0.57683707\n  1.94834767]\n\n\n\n\n\n\n\nCode\n# Generate random numbers from a Normal(5, 2)\nnormals = np.random.normal(loc=5,scale=2,size=5000)\n\n# Print normals\nprint(normals)\n\n# Plot a histogram of normal values, binwidth 0.5\nplt.hist(normals,np.arange(-2,13.5,0.5))\nplt.show()\n\n\n[0.37088348 5.46510043 5.6804744  ... 5.06268094 7.1989964  4.26078515]"
  },
  {
    "objectID": "posts/Introduction to hypothesis testing/Introduction to Hypothesis Testing.html",
    "href": "posts/Introduction to hypothesis testing/Introduction to Hypothesis Testing.html",
    "title": "Introduction to Hypothesis Testing",
    "section": "",
    "text": "We will walk you through the steps of creating a one sample proportional test so that you will be able to better understand how hypothesis tests work and what problems they can solve. In doing so, we will also introduce important concepts such as z-scores, parabolae, and false negative and false positive errors.\nThis Introduction to Hypothesis Testing is part of Datacamp course: Hypothesis Testing in Python\nThis is my learning experience of data science through DataCamp\n\n\n\n\nA/B testing: also known as split testing, refers to random experiment to test variable / outcome on treatment & control group Hypothesis: a theory or assumption yet to be proved Point estimation: sample statistics or sample mean of population mean_samp = population[‘column’].mean() Standard error: standard deviation of sample statistics or sample mean in bootstrap distribution estimates standard error std_error = np.std(so_boot_distn, ddof=1)\n\n\n\nSince variables have different units & ranges, we need to standardize their value before testing our hypothesis standardized value = (value - mean) / standard deviation\nz = (sample statistics - hypothesis param value) / standard error\nStandard normal distribution: normal distribution with mean = 0 + standard deviation =1\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\n\n\n\n\nCode\nlate_shipments= pd.read_feather('dataset/late_shipments.feather')\nlate_shipments.head()\n\n\n\n\n\n\n  \n    \n      \n      id\n      country\n      managed_by\n      fulfill_via\n      vendor_inco_term\n      shipment_mode\n      late_delivery\n      late\n      product_group\n      sub_classification\n      ...\n      line_item_quantity\n      line_item_value\n      pack_price\n      unit_price\n      manufacturing_site\n      first_line_designation\n      weight_kilograms\n      freight_cost_usd\n      freight_cost_groups\n      line_item_insurance_usd\n    \n  \n  \n    \n      0\n      36203.0\n      Nigeria\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      1.0\n      Yes\n      HRDT\n      HIV test\n      ...\n      2996.0\n      266644.00\n      89.00\n      0.89\n      Alere Medical Co., Ltd.\n      Yes\n      1426.0\n      33279.83\n      expensive\n      373.83\n    \n    \n      1\n      30998.0\n      Botswana\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      HRDT\n      HIV test\n      ...\n      25.0\n      800.00\n      32.00\n      1.60\n      Trinity Biotech, Plc\n      Yes\n      10.0\n      559.89\n      reasonable\n      1.72\n    \n    \n      2\n      69871.0\n      Vietnam\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      ARV\n      Adult\n      ...\n      22925.0\n      110040.00\n      4.80\n      0.08\n      Hetero Unit III Hyderabad IN\n      Yes\n      3723.0\n      19056.13\n      expensive\n      181.57\n    \n    \n      3\n      17648.0\n      South Africa\n      PMO - US\n      Direct Drop\n      DDP\n      Ocean\n      0.0\n      No\n      ARV\n      Adult\n      ...\n      152535.0\n      361507.95\n      2.37\n      0.04\n      Aurobindo Unit III, India\n      Yes\n      7698.0\n      11372.23\n      expensive\n      779.41\n    \n    \n      4\n      5647.0\n      Uganda\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      HRDT\n      HIV test - Ancillary\n      ...\n      850.0\n      8.50\n      0.01\n      0.00\n      Inverness Japan\n      Yes\n      56.0\n      360.00\n      reasonable\n      0.01\n    \n  \n\n5 rows × 27 columns\n\n\n\n\n\n\nWe’ll begin our analysis by calculating a point estimate (or sample statistic), namely the proportion of late shipments.\n\n\nCode\n# Print the late_shipments dataset\nprint(late_shipments)\n\n# Calculate the proportion of late shipments\nlate_prop_samp = (late_shipments['late']=='Yes').mean()\n\n# Print the results\nprint(late_prop_samp)\nprint(\"\\nThe proportion of late shipments in the sample is 0.061, or 6.1%\")\n\n\n          id       country managed_by  fulfill_via vendor_inco_term  \\\n0    36203.0       Nigeria   PMO - US  Direct Drop              EXW   \n1    30998.0      Botswana   PMO - US  Direct Drop              EXW   \n2    69871.0       Vietnam   PMO - US  Direct Drop              EXW   \n3    17648.0  South Africa   PMO - US  Direct Drop              DDP   \n4     5647.0        Uganda   PMO - US  Direct Drop              EXW   \n..       ...           ...        ...          ...              ...   \n995  13608.0        Uganda   PMO - US  Direct Drop              DDP   \n996  80394.0    Congo, DRC   PMO - US  Direct Drop              EXW   \n997  61675.0        Zambia   PMO - US  Direct Drop              EXW   \n998  39182.0  South Africa   PMO - US  Direct Drop              DDP   \n999   5645.0      Botswana   PMO - US  Direct Drop              EXW   \n\n    shipment_mode  late_delivery late product_group    sub_classification  \\\n0             Air            1.0  Yes          HRDT              HIV test   \n1             Air            0.0   No          HRDT              HIV test   \n2             Air            0.0   No           ARV                 Adult   \n3           Ocean            0.0   No           ARV                 Adult   \n4             Air            0.0   No          HRDT  HIV test - Ancillary   \n..            ...            ...  ...           ...                   ...   \n995           Air            0.0   No           ARV                 Adult   \n996           Air            0.0   No          HRDT              HIV test   \n997           Air            1.0  Yes          HRDT              HIV test   \n998         Ocean            0.0   No           ARV                 Adult   \n999           Air            0.0   No          HRDT              HIV test   \n\n     ... line_item_quantity line_item_value pack_price unit_price  \\\n0    ...             2996.0       266644.00      89.00       0.89   \n1    ...               25.0          800.00      32.00       1.60   \n2    ...            22925.0       110040.00       4.80       0.08   \n3    ...           152535.0       361507.95       2.37       0.04   \n4    ...              850.0            8.50       0.01       0.00   \n..   ...                ...             ...        ...        ...   \n995  ...              121.0         9075.00      75.00       0.62   \n996  ...              292.0         9344.00      32.00       1.60   \n997  ...             2127.0       170160.00      80.00       0.80   \n998  ...           191011.0       861459.61       4.51       0.15   \n999  ...              200.0        14398.00      71.99       0.72   \n\n               manufacturing_site first_line_designation  weight_kilograms  \\\n0         Alere Medical Co., Ltd.                    Yes            1426.0   \n1            Trinity Biotech, Plc                    Yes              10.0   \n2    Hetero Unit III Hyderabad IN                    Yes            3723.0   \n3       Aurobindo Unit III, India                    Yes            7698.0   \n4                 Inverness Japan                    Yes              56.0   \n..                            ...                    ...               ...   \n995     Janssen-Cilag, Latina, IT                    Yes              43.0   \n996          Trinity Biotech, Plc                    Yes              99.0   \n997       Alere Medical Co., Ltd.                    Yes             881.0   \n998     Aurobindo Unit III, India                    Yes           16234.0   \n999               Inverness Japan                    Yes              46.0   \n\n     freight_cost_usd  freight_cost_groups  line_item_insurance_usd  \n0            33279.83            expensive                   373.83  \n1              559.89           reasonable                     1.72  \n2            19056.13            expensive                   181.57  \n3            11372.23            expensive                   779.41  \n4              360.00           reasonable                     0.01  \n..                ...                  ...                      ...  \n995            199.00           reasonable                    12.72  \n996           2162.55           reasonable                    13.10  \n997          14019.38            expensive                   210.49  \n998          14439.17            expensive                  1421.41  \n999           1028.18           reasonable                    23.04  \n\n[1000 rows x 27 columns]\n0.061\n\nThe proportion of late shipments in the sample is 0.061, or 6.1%\n\n\n\n\nCode\nlate_prop_Yes=late_shipments[late_shipments['late']=='Yes']\nlate_prop_Yes.head(10)\nlate_prop_Yes.shape\n\n\n(61, 27)\n\n\n\n\n\n\n\nCode\nlate_shipments_boot_distn=[0.064,\n 0.049,\n 0.06,\n 0.066,\n 0.052,\n 0.066,\n 0.071,\n 0.061,\n 0.051,\n 0.06,\n 0.053,\n 0.066,\n 0.069,\n 0.068,\n 0.063,\n 0.061,\n 0.052,\n 0.045,\n 0.054,\n 0.054,\n 0.064,\n 0.064,\n 0.058,\n 0.062,\n 0.05,\n 0.053,\n 0.064,\n 0.058,\n 0.071,\n 0.064,\n 0.052,\n 0.063,\n 0.056,\n 0.05,\n 0.058,\n 0.06,\n 0.068,\n 0.065,\n 0.056,\n 0.052,\n 0.061,\n 0.059,\n 0.054,\n 0.071,\n 0.067,\n 0.079,\n 0.069,\n 0.069,\n 0.05,\n 0.059,\n 0.062,\n 0.046,\n 0.068,\n 0.057,\n 0.067,\n 0.042,\n 0.074,\n 0.063,\n 0.056,\n 0.063,\n 0.068,\n 0.06,\n 0.068,\n 0.064,\n 0.052,\n 0.045,\n 0.058,\n 0.072,\n 0.078,\n 0.055,\n 0.069,\n 0.048,\n 0.047,\n 0.061,\n 0.066,\n 0.062,\n 0.059,\n 0.062,\n 0.054,\n 0.063,\n 0.061,\n 0.059,\n 0.057,\n 0.059,\n 0.058,\n 0.068,\n 0.067,\n 0.059,\n 0.054,\n 0.064,\n 0.047,\n 0.054,\n 0.065,\n 0.063,\n 0.057,\n 0.062,\n 0.058,\n 0.046,\n 0.052,\n 0.065,\n 0.053,\n 0.069,\n 0.068,\n 0.065,\n 0.052,\n 0.061,\n 0.058,\n 0.042,\n 0.064,\n 0.063,\n 0.068,\n 0.067,\n 0.061,\n 0.056,\n 0.061,\n 0.044,\n 0.058,\n 0.051,\n 0.075,\n 0.064,\n 0.073,\n 0.058,\n 0.056,\n 0.055,\n 0.063,\n 0.056,\n 0.067,\n 0.075,\n 0.061,\n 0.063,\n 0.051,\n 0.065,\n 0.069,\n 0.066,\n 0.05,\n 0.066,\n 0.057,\n 0.064,\n 0.065,\n 0.062,\n 0.071,\n 0.062,\n 0.065,\n 0.062,\n 0.066,\n 0.071,\n 0.058,\n 0.053,\n 0.062,\n 0.051,\n 0.056,\n 0.061,\n 0.074,\n 0.054,\n 0.059,\n 0.069,\n 0.073,\n 0.066,\n 0.052,\n 0.065,\n 0.072,\n 0.071,\n 0.059,\n 0.065,\n 0.06,\n 0.055,\n 0.053,\n 0.059,\n 0.066,\n 0.061,\n 0.053,\n 0.053,\n 0.06,\n 0.058,\n 0.074,\n 0.05,\n 0.059,\n 0.067,\n 0.06,\n 0.064,\n 0.061,\n 0.072,\n 0.06,\n 0.048,\n 0.066,\n 0.059,\n 0.08,\n 0.062,\n 0.066,\n 0.065,\n 0.06,\n 0.048,\n 0.064,\n 0.07,\n 0.053,\n 0.035,\n 0.071,\n 0.061,\n 0.051,\n 0.052,\n 0.051,\n 0.069,\n 0.052,\n 0.052,\n 0.065,\n 0.053,\n 0.055,\n 0.063,\n 0.066,\n 0.062,\n 0.067,\n 0.079,\n 0.062,\n 0.056,\n 0.058,\n 0.068,\n 0.062,\n 0.045,\n 0.063,\n 0.069,\n 0.054,\n 0.065,\n 0.061,\n 0.057,\n 0.05,\n 0.048,\n 0.069,\n 0.058,\n 0.052,\n 0.056,\n 0.057,\n 0.071,\n 0.059,\n 0.062,\n 0.064,\n 0.053,\n 0.065,\n 0.056,\n 0.06,\n 0.062,\n 0.042,\n 0.054,\n 0.051,\n 0.061,\n 0.049,\n 0.071,\n 0.072,\n 0.059,\n 0.063,\n 0.049,\n 0.074,\n 0.063,\n 0.052,\n 0.055,\n 0.072,\n 0.054,\n 0.067,\n 0.067,\n 0.067,\n 0.055,\n 0.073,\n 0.064,\n 0.069,\n 0.06,\n 0.053,\n 0.057,\n 0.056,\n 0.058,\n 0.067,\n 0.065,\n 0.064,\n 0.053,\n 0.055,\n 0.069,\n 0.058,\n 0.07,\n 0.068,\n 0.062,\n 0.062,\n 0.05,\n 0.069,\n 0.061,\n 0.057,\n 0.066,\n 0.056,\n 0.053,\n 0.055,\n 0.062,\n 0.064,\n 0.055,\n 0.056,\n 0.061,\n 0.058,\n 0.068,\n 0.079,\n 0.057,\n 0.049,\n 0.052,\n 0.063,\n 0.064,\n 0.059,\n 0.071,\n 0.064,\n 0.052,\n 0.066,\n 0.063,\n 0.069,\n 0.056,\n 0.057,\n 0.062,\n 0.057,\n 0.055,\n 0.062,\n 0.06,\n 0.064,\n 0.057,\n 0.062,\n 0.069,\n 0.067,\n 0.052,\n 0.061,\n 0.056,\n 0.055,\n 0.056,\n 0.055,\n 0.064,\n 0.068,\n 0.051,\n 0.054,\n 0.057,\n 0.054,\n 0.07,\n 0.049,\n 0.058,\n 0.063,\n 0.07,\n 0.046,\n 0.059,\n 0.064,\n 0.059,\n 0.061,\n 0.066,\n 0.06,\n 0.073,\n 0.08,\n 0.069,\n 0.061,\n 0.071,\n 0.068,\n 0.065,\n 0.063,\n 0.054,\n 0.07,\n 0.061,\n 0.053,\n 0.059,\n 0.047,\n 0.064,\n 0.071,\n 0.068,\n 0.049,\n 0.063,\n 0.057,\n 0.057,\n 0.059,\n 0.061,\n 0.048,\n 0.084,\n 0.07,\n 0.077,\n 0.043,\n 0.065,\n 0.057,\n 0.057,\n 0.054,\n 0.064,\n 0.062,\n 0.067,\n 0.068,\n 0.06,\n 0.054,\n 0.066,\n 0.048,\n 0.048,\n 0.06,\n 0.054,\n 0.067,\n 0.064,\n 0.064,\n 0.067,\n 0.058,\n 0.066,\n 0.06,\n 0.048,\n 0.058,\n 0.054,\n 0.056,\n 0.055,\n 0.068,\n 0.077,\n 0.06,\n 0.061,\n 0.055,\n 0.065,\n 0.064,\n 0.058,\n 0.058,\n 0.058,\n 0.055,\n 0.067,\n 0.061,\n 0.063,\n 0.065,\n 0.071,\n 0.051,\n 0.066,\n 0.066,\n 0.066,\n 0.07,\n 0.068,\n 0.061,\n 0.062,\n 0.054,\n 0.058,\n 0.066,\n 0.059,\n 0.061,\n 0.058,\n 0.057,\n 0.065,\n 0.053,\n 0.053,\n 0.06,\n 0.068,\n 0.067,\n 0.068,\n 0.061,\n 0.067,\n 0.059,\n 0.057,\n 0.055,\n 0.067,\n 0.058,\n 0.055,\n 0.055,\n 0.054,\n 0.061,\n 0.074,\n 0.071,\n 0.057,\n 0.056,\n 0.047,\n 0.07,\n 0.054,\n 0.052,\n 0.072,\n 0.054,\n 0.064,\n 0.063,\n 0.075,\n 0.064,\n 0.051,\n 0.061,\n 0.064,\n 0.047,\n 0.067,\n 0.061,\n 0.06,\n 0.057,\n 0.059,\n 0.058,\n 0.07,\n 0.06,\n 0.056,\n 0.064,\n 0.056,\n 0.066,\n 0.051,\n 0.064,\n 0.054,\n 0.058,\n 0.064,\n 0.041,\n 0.057,\n 0.055,\n 0.06,\n 0.06,\n 0.051,\n 0.054,\n 0.07,\n 0.053,\n 0.063,\n 0.058,\n 0.066,\n 0.059,\n 0.051,\n 0.067,\n 0.078,\n 0.056,\n 0.068,\n 0.057,\n 0.059,\n 0.062,\n 0.053,\n 0.064,\n 0.067,\n 0.068,\n 0.071,\n 0.066,\n 0.057,\n 0.063,\n 0.067,\n 0.059,\n 0.057,\n 0.064,\n 0.049,\n 0.066,\n 0.055,\n 0.071,\n 0.061,\n 0.078,\n 0.062,\n 0.052,\n 0.058,\n 0.066,\n 0.06,\n 0.054,\n 0.058,\n 0.054,\n 0.062,\n 0.072,\n 0.068,\n 0.057,\n 0.059,\n 0.066,\n 0.066,\n 0.065,\n 0.067,\n 0.071,\n 0.064,\n 0.072,\n 0.067,\n 0.064,\n 0.064,\n 0.051,\n 0.061,\n 0.047,\n 0.07,\n 0.073,\n 0.06,\n 0.066,\n 0.058,\n 0.056,\n 0.064,\n 0.059,\n 0.062,\n 0.046,\n 0.07,\n 0.07,\n 0.071,\n 0.056,\n 0.061,\n 0.066,\n 0.058,\n 0.055,\n 0.073,\n 0.068,\n 0.073,\n 0.055,\n 0.074,\n 0.063,\n 0.049,\n 0.063,\n 0.063,\n 0.056,\n 0.061,\n 0.065,\n 0.066,\n 0.06,\n 0.057,\n 0.07,\n 0.06,\n 0.053,\n 0.055,\n 0.066,\n 0.07,\n 0.069,\n 0.051,\n 0.067,\n 0.055,\n 0.06,\n 0.074,\n 0.06,\n 0.057,\n 0.06,\n 0.054,\n 0.054,\n 0.058,\n 0.06,\n 0.057,\n 0.059,\n 0.065,\n 0.061,\n 0.073,\n 0.067,\n 0.063,\n 0.079,\n 0.063,\n 0.063,\n 0.051,\n 0.074,\n 0.06,\n 0.07,\n 0.063,\n 0.072,\n 0.066,\n 0.058,\n 0.046,\n 0.059,\n 0.064,\n 0.058,\n 0.071,\n 0.055,\n 0.062,\n 0.05,\n 0.055,\n 0.061,\n 0.052,\n 0.059,\n 0.063,\n 0.058,\n 0.044,\n 0.052,\n 0.069,\n 0.056,\n 0.057,\n 0.064,\n 0.067,\n 0.058,\n 0.07,\n 0.065,\n 0.068,\n 0.061,\n 0.055,\n 0.06,\n 0.053,\n 0.066,\n 0.052,\n 0.064,\n 0.051,\n 0.076,\n 0.069,\n 0.056,\n 0.057,\n 0.068,\n 0.07,\n 0.065,\n 0.062,\n 0.066,\n 0.063,\n 0.066,\n 0.054,\n 0.061,\n 0.061,\n 0.055,\n 0.053,\n 0.054,\n 0.065,\n 0.073,\n 0.064,\n 0.054,\n 0.065,\n 0.06,\n 0.059,\n 0.056,\n 0.064,\n 0.057,\n 0.06,\n 0.07,\n 0.063,\n 0.064,\n 0.067,\n 0.061,\n 0.053,\n 0.06,\n 0.064,\n 0.064,\n 0.057,\n 0.046,\n 0.057,\n 0.065,\n 0.074,\n 0.062,\n 0.063,\n 0.054,\n 0.074,\n 0.064,\n 0.077,\n 0.068,\n 0.06,\n 0.063,\n 0.059,\n 0.06,\n 0.068,\n 0.052,\n 0.064,\n 0.057,\n 0.059,\n 0.069,\n 0.061,\n 0.064,\n 0.047,\n 0.062,\n 0.069,\n 0.054,\n 0.069,\n 0.063,\n 0.077,\n 0.06,\n 0.061,\n 0.055,\n 0.069,\n 0.061,\n 0.06,\n 0.061,\n 0.067,\n 0.05,\n 0.061,\n 0.062,\n 0.081,\n 0.071,\n 0.057,\n 0.055,\n 0.054,\n 0.07,\n 0.068,\n 0.063,\n 0.056,\n 0.081,\n 0.049,\n 0.07,\n 0.048,\n 0.046,\n 0.069,\n 0.056,\n 0.066,\n 0.058,\n 0.058,\n 0.062,\n 0.052,\n 0.065,\n 0.043,\n 0.062,\n 0.063,\n 0.053,\n 0.073,\n 0.058,\n 0.064,\n 0.071,\n 0.073,\n 0.059,\n 0.08,\n 0.052,\n 0.053,\n 0.053,\n 0.053,\n 0.057,\n 0.061,\n 0.069,\n 0.046,\n 0.063,\n 0.078,\n 0.06,\n 0.06,\n 0.064,\n 0.063,\n 0.065,\n 0.069,\n 0.059,\n 0.068,\n 0.061,\n 0.066,\n 0.064,\n 0.064,\n 0.058,\n 0.046,\n 0.073,\n 0.06,\n 0.056,\n 0.073,\n 0.07,\n 0.058,\n 0.056,\n 0.064,\n 0.069,\n 0.065,\n 0.063,\n 0.063,\n 0.054,\n 0.081,\n 0.044,\n 0.048,\n 0.059,\n 0.058,\n 0.046,\n 0.063,\n 0.072,\n 0.063,\n 0.059,\n 0.063,\n 0.047,\n 0.063,\n 0.065,\n 0.071,\n 0.061,\n 0.05,\n 0.063,\n 0.065,\n 0.054,\n 0.053,\n 0.061,\n 0.054,\n 0.063,\n 0.056,\n 0.071,\n 0.057,\n 0.058,\n 0.049,\n 0.074,\n 0.057,\n 0.058,\n 0.07,\n 0.063,\n 0.057,\n 0.052,\n 0.064,\n 0.074,\n 0.047,\n 0.071,\n 0.051,\n 0.059,\n 0.05,\n 0.059,\n 0.05,\n 0.05,\n 0.057,\n 0.075,\n 0.053,\n 0.07,\n 0.062,\n 0.062,\n 0.075,\n 0.058,\n 0.057,\n 0.05,\n 0.062,\n 0.061,\n 0.067,\n 0.062,\n 0.059,\n 0.059,\n 0.049,\n 0.052,\n 0.062,\n 0.069,\n 0.062,\n 0.054,\n 0.05,\n 0.063,\n 0.052,\n 0.063,\n 0.069,\n 0.057,\n 0.067,\n 0.064,\n 0.057,\n 0.057,\n 0.057,\n 0.05,\n 0.062,\n 0.069,\n 0.075,\n 0.075,\n 0.05,\n 0.06,\n 0.065,\n 0.051,\n 0.063,\n 0.075,\n 0.06,\n 0.058,\n 0.063,\n 0.069,\n 0.055,\n 0.062,\n 0.06,\n 0.057,\n 0.079,\n 0.046,\n 0.059,\n 0.07,\n 0.055,\n 0.08,\n 0.048,\n 0.061,\n 0.042,\n 0.068,\n 0.082,\n 0.044,\n 0.054,\n 0.063,\n 0.054,\n 0.071,\n 0.053,\n 0.061,\n 0.06,\n 0.065,\n 0.072,\n 0.063,\n 0.062,\n 0.053,\n 0.072,\n 0.067,\n 0.058,\n 0.075,\n 0.07,\n 0.052,\n 0.056,\n 0.056,\n 0.082,\n 0.055,\n 0.056,\n 0.057,\n 0.056,\n 0.054,\n 0.073,\n 0.081,\n 0.063,\n 0.063,\n 0.054,\n 0.058,\n 0.062,\n 0.065,\n 0.063,\n 0.062,\n 0.056,\n 0.063,\n 0.06,\n 0.061,\n 0.068,\n 0.067,\n 0.07,\n 0.059,\n 0.06,\n 0.063,\n 0.057,\n 0.052,\n 0.062,\n 0.064,\n 0.065,\n 0.07,\n 0.063,\n 0.062,\n 0.052,\n 0.055,\n 0.055,\n 0.053,\n 0.057,\n 0.058,\n 0.062,\n 0.06,\n 0.056,\n 0.064,\n 0.074,\n 0.071,\n 0.059,\n 0.056,\n 0.063,\n 0.059,\n 0.058,\n 0.054,\n 0.058,\n 0.069,\n 0.06,\n 0.063,\n 0.054,\n 0.047,\n 0.061,\n 0.057,\n 0.059,\n 0.057,\n 0.063,\n 0.06,\n 0.071,\n 0.062,\n 0.06,\n 0.071,\n 0.059,\n 0.049,\n 0.077]\n\n\n\n\nCode\nlate_shipments['late'].unique()\n\n\narray(['Yes', 'No'], dtype=object)\n\n\n\n\nCode\n# Hypothesize that the proportion is 6%\nlate_prop_hyp = 0.06\n\n#for i in range(5000):\n    #np.mean(late_shipments_boot_distn.append(late_shipments.sample(frac=1, replace=True)['late']))\n\n#print(late_shipments_boot_distn)\n# Calculate the standard error\nstd_error = np.std(late_shipments_boot_distn,ddof=1)\n\n# Find z-score of late_prop_samp\nz_score = (late_prop_samp - late_prop_hyp) / std_error\n\n# Print z_score\nprint(z_score)\nprint(\"\\nThe z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic\")\n\n\n0.13387997080083944\n\nThe z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic\n\n\n\n\n\nHypothesis tests check if the sample statistics lie in the tails of the null distribution\nalternative different from null : Two Tail test alternative greater than null : right tail test alternative lower than null: left tail test\n\n\n\np-values measure the strength of support for the null hypothesis, or in other words, they measure the probability of obtaining a result, assuming the null hypothesis is true. Large p-values mean our statistic is producing a result that is likely not in a tail of our null distribution, and chance could be a good explanation for the result. Small p-values mean our statistic is producing a result likely in the tail of our null distribution. Because p-values are probabilities, they are always between zero and one\nCalculating p-value Left tail test: norm.cdf() right tail test: 1-norm.cdf()\np-values quantify evidence for the null hypothesis large p-value => fail to reject null hypothesis small p-value => reject null hypothesis\n\n\nCode\n# Calculate the z-score of late_prop_samp\nz_score = (late_prop_samp - late_prop_hyp) / std_error\n\n# Calculate the p-value\np_value = 1-norm.cdf(z_score, loc=0, scale=1)\n\n# Print the p-value\nprint(p_value)\n\n\n0.44674874433656875\n\n\n\n\n\nType I and type II errors\nFor hypothesis tests and for criminal trials, there are two states of truth and two possible outcomes. Two combinations are correct test outcomes, and there are two ways it can go wrong.\nThe errors are known as false positives (or “type I errors”), and false negatives (or “type II errors”)."
  },
  {
    "objectID": "posts/Predictions and model objects/Predictions and model objects.html",
    "href": "posts/Predictions and model objects/Predictions and model objects.html",
    "title": "Predictions and model objects in linear regression",
    "section": "",
    "text": "This article explores how linear regression models can be used to predict Taiwanese house prices and Facebook advert clicks. Our regression skills will also be developed through the use of hands-on model objects, as well as the concept of “regression to the mean” and how to transform variables within a dataset.\nThis Simple Linear Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nPredictions can be made using statistical models like linear regression. In other words, you specify each explanatory variable, feed it into the model, and get a prediction.\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847\n    \n  \n\n\n\n\n\n\nCode\n# Create the explanatory_data\nexplanatory_data = pd.DataFrame({'n_convenience': np.arange(0,10)})\n\n# Print it\nprint(explanatory_data)\n\n\n   n_convenience\n0              0\n1              1\n2              2\n3              3\n4              4\n5              5\n6              6\n7              7\n8              8\n9              9\n\n\n\n\nCode\nmdl_price_vs_conv=ols(\"price_twd_msq ~ n_convenience\",data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Create explanatory_data\nexplanatory_data = pd.DataFrame({'n_convenience': np.arange(0, 11)})\n\n# Use mdl_price_vs_conv to predict with explanatory_data, call it price_twd_msq\nprice_twd_msq = mdl_price_vs_conv.predict(explanatory_data)\n\n# Print it\nprint(price_twd_msq)\n\n\n0      8.224237\n1      9.022317\n2      9.820397\n3     10.618477\n4     11.416556\n5     12.214636\n6     13.012716\n7     13.810795\n8     14.608875\n9     15.406955\n10    16.205035\ndtype: float64\n\n\n\n\nCode\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_conv.predict(explanatory_data))\n\n# Print the result\nprint(prediction_data)\n\n\n    n_convenience  price_twd_msq\n0               0       8.224237\n1               1       9.022317\n2               2       9.820397\n3               3      10.618477\n4               4      11.416556\n5               5      12.214636\n6               6      13.012716\n7               7      13.810795\n8               8      14.608875\n9               9      15.406955\n10             10      16.205035\n\n\n\n\n\nThe prediction DataFrame you created contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values\n\n\nCode\n# Create a new figure, fig\nfig = plt.figure()\n\nsns.regplot(x=\"n_convenience\",\n            y=\"price_twd_msq\",\n            data=taiwan_real_estate,\n            ci=None)\n# Add a scatter plot layer to the regplot\nsns.scatterplot(x='n_convenience',y='price_twd_msq',data=prediction_data,color='red',marker='s')\n\n# Show the layered plot\nplt.show()\nprint(\"\\n the predicted points lie on the trend lin\")\n\n\n\n\n\n\n the predicted points lie on the trend lin\n\n\n\n\n\nThe model object created by ols() contains many elements. In order to perform further analysis on the model results, we need to extract its useful bits. The model coefficients, the fitted values, and the residuals are perhaps the most important pieces of the linear model object.\n\n\nCode\n# Print the model parameters of mdl_price_vs_conv\nprint(mdl_price_vs_conv.params)\n\n\nIntercept        8.224237\nn_convenience    0.798080\ndtype: float64\n\n\n\n\nCode\n# Print the fitted values of mdl_price_vs_conv\nprint(mdl_price_vs_conv.fittedvalues)\n\n\n0      16.205035\n1      15.406955\n2      12.214636\n3      12.214636\n4      12.214636\n         ...    \n409     8.224237\n410    15.406955\n411    13.810795\n412    12.214636\n413    15.406955\nLength: 414, dtype: float64\n\n\n\n\nCode\n# Print the residuals of mdl_price_vs_conv\nprint(mdl_price_vs_conv.resid)\n\n\n0     -4.737561\n1     -2.638422\n2      2.097013\n3      4.366302\n4      0.826211\n         ...   \n409   -3.564631\n410   -0.278362\n411   -1.526378\n412    3.670387\n413    3.927387\nLength: 414, dtype: float64\n\n\n\n\nCode\n# Print a summary of mdl_price_vs_conv\nprint(mdl_price_vs_conv.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          price_twd_msq   R-squared:                       0.326\nModel:                            OLS   Adj. R-squared:                  0.324\nMethod:                 Least Squares   F-statistic:                     199.3\nDate:                Thu, 12 Jan 2023   Prob (F-statistic):           3.41e-37\nTime:                        17:11:19   Log-Likelihood:                -1091.1\nNo. Observations:                 414   AIC:                             2186.\nDf Residuals:                     412   BIC:                             2194.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         8.2242      0.285     28.857      0.000       7.664       8.784\nn_convenience     0.7981      0.057     14.118      0.000       0.687       0.909\n==============================================================================\nOmnibus:                      171.927   Durbin-Watson:                   1.993\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1417.242\nSkew:                           1.553   Prob(JB):                    1.78e-308\nKurtosis:                      11.516   Cond. No.                         8.87\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nUsing the model coefficients, you can manually calculate predictions. It’s better to use .predict() when making predictions in real life, but doing it manually is helpful for reassuring yourself that predictions aren’t magic.\nFor simple linear regressions, the predicted value is the intercept plus the slope times the explanatory variable.\nresponse = intercept + slope * explanatory\n\n\nCode\n# Get the coefficients of mdl_price_vs_conv\ncoeffs = mdl_price_vs_conv.params\n\n# Get the intercept\nintercept = coeffs[0]\n\n# Get the slope\nslope = coeffs[1]\n\n# Manually calculate the predictions\nprice_twd_msq = intercept + slope * explanatory_data\nprint(price_twd_msq)\n\n# Compare to the results from .predict()\nprint(price_twd_msq.assign(predictions_auto=mdl_price_vs_conv.predict(explanatory_data)))\n\n\n    n_convenience\n0        8.224237\n1        9.022317\n2        9.820397\n3       10.618477\n4       11.416556\n5       12.214636\n6       13.012716\n7       13.810795\n8       14.608875\n9       15.406955\n10      16.205035\n    n_convenience  predictions_auto\n0        8.224237          8.224237\n1        9.022317          9.022317\n2        9.820397          9.820397\n3       10.618477         10.618477\n4       11.416556         11.416556\n5       12.214636         12.214636\n6       13.012716         13.012716\n7       13.810795         13.810795\n8       14.608875         14.608875\n9       15.406955         15.406955\n10      16.205035         16.205035\n\n\n\n\n\n\nResponse value = fitted value + residual\n“The stuff one can explain” + “the stuff once couldn’t explain”\nResiduals exist due to problems in model and fundamental randomness\nExtreme cases are often due to randomness\nRegression to mean indicated extreme cases don’t persist over time\n\n\n\nCode\nsp500_yearly_returns=pd.read_csv(\"dataset/sp500_yearly_returns.csv\")\nsp500_yearly_returns.head()\n\n\n\n\n\n\n  \n    \n      \n      symbol\n      return_2018\n      return_2019\n    \n  \n  \n    \n      0\n      AAPL\n      -0.053902\n      0.889578\n    \n    \n      1\n      MSFT\n      0.207953\n      0.575581\n    \n    \n      2\n      AMZN\n      0.284317\n      0.230278\n    \n    \n      3\n      FB\n      -0.257112\n      0.565718\n    \n    \n      4\n      GOOGL\n      -0.008012\n      0.281762\n    \n  \n\n\n\n\n\n\nCode\n# Create a new figure, fig\nfig = plt.figure()\n\n# Plot the first layer: y = x\nplt.axline(xy1=(0,0), slope=1, linewidth=2, color=\"green\")\n\n# Add scatter plot with linear regression trend line\nsns.regplot(x='return_2018',y='return_2019',data=sp500_yearly_returns,ci=None,line_kws={'color':'black'})\n\n# Set the axes so that the distances along the x and y axes look the same\nplt.axis(\"equal\")\n\n# Show the plot\nplt.show()\nprint('\\n The regression trend line looks very different to the y equals x line. As the financial advisors say, \"Past performance is no guarantee of future results.\"')\n\n\n\n\n\n\n The regression trend line looks very different to the y equals x line. As the financial advisors say, \"Past performance is no guarantee of future results.\"\n\n\n\n\n\nLet’s quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019\n\n\nCode\n# Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns\nmdl_returns = ols(\"return_2019 ~ return_2018\",data=sp500_yearly_returns).fit()\n\n# Print the parameters\nprint(mdl_returns.params)\n\n# Create a DataFrame with return_2018 at -1, 0, and 1\nexplanatory_data = pd.DataFrame({'return_2018':[-1,0,1]})\n\n# Use mdl_returns to predict with explanatory_data\nprint(mdl_returns.predict(explanatory_data))\n\nprint(\"\\n Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019\")\n\n\nIntercept      0.321321\nreturn_2018    0.020069\ndtype: float64\n0    0.301251\n1    0.321321\n2    0.341390\ndtype: float64\n\n Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019\n\n\n\n\n\nWhen there is no straight-line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both. Let’s transform the explanatory variable.\nWe’ll look at the Taiwan real estate dataset again, but we’ll use the distance to the nearest MRT (metro) station as the explanatory variable. By taking the square root, you’ll shorten the distance to the metro station for commuters.\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\nplt.figure()\n\n# Plot using the transformed variable\nsns.regplot(x='sqrt_dist_to_mrt_m',y='price_twd_msq',data=taiwan_real_estate)\nplt.show()\n\n\n\n\n\n\n\nCode\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\nprint(mdl_price_vs_dist.params)\n\n\nIntercept             16.709799\nsqrt_dist_to_mrt_m    -0.182843\ndtype: float64\n\n\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\nexplanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n\n# Create prediction_data by adding a column of predictions to explantory_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n)\n\n# Print the result\nprint(prediction_data)\n\n\n   sqrt_dist_to_mrt_m  dist_to_mrt_m  price_twd_msq\n0                 0.0              0      16.709799\n1                10.0            100      14.881370\n2                20.0            400      13.052942\n3                30.0            900      11.224513\n4                40.0           1600       9.396085\n5                50.0           2500       7.567656\n6                60.0           3600       5.739227\n7                70.0           4900       3.910799\n8                80.0           6400       2.082370\n\n\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\n# Use this explanatory data\nexplanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n\n# Use mdl_price_vs_dist to predict explanatory_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n)\n\nfig = plt.figure()\nsns.regplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n\n# Add a layer of your prediction points\nsns.scatterplot(data=prediction_data, x='sqrt_dist_to_mrt_m', y='price_twd_msq', color='red')\nplt.show()\n\nprint(\"\\n By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate mode\")\n\n\n\n\n\n\n By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate mode\n\n\n\n\n\nThe response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. That is, you “back transform” the predictions\n\n\nCode\nad_conversion=pd.read_csv('dataset/ad_conversion.csv')\nad_conversion.head()\n\n\n\n\n\n\n  \n    \n      \n      spent_usd\n      n_impressions\n      n_clicks\n    \n  \n  \n    \n      0\n      1.43\n      7350\n      1\n    \n    \n      1\n      1.82\n      17861\n      2\n    \n    \n      2\n      1.25\n      4259\n      1\n    \n    \n      3\n      1.29\n      4133\n      1\n    \n    \n      4\n      4.77\n      15615\n      3\n    \n  \n\n\n\n\n\n\nCode\n# Create qdrt_n_impressions and qdrt_n_clicks\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion['n_impressions'] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion['n_clicks'] ** 0.25\n\nplt.figure()\n\n# Plot using the transformed variables\nsns.regplot(x='qdrt_n_impressions',y='qdrt_n_clicks',data=ad_conversion,ci=None)\nplt.show()\n\n\n\n\n\n\n\nCode\n# From previous step\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n\n# Run a linear regression of your transformed variables\nmdl_click_vs_impression = ols('qdrt_n_clicks ~ qdrt_n_impressions', data=ad_conversion).fit()\nprint(mdl_click_vs_impression.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          qdrt_n_clicks   R-squared:                       0.945\nModel:                            OLS   Adj. R-squared:                  0.944\nMethod:                 Least Squares   F-statistic:                 1.590e+04\nDate:                Thu, 12 Jan 2023   Prob (F-statistic):               0.00\nTime:                        17:11:20   Log-Likelihood:                 193.90\nNo. Observations:                 936   AIC:                            -383.8\nDf Residuals:                     934   BIC:                            -374.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.0717      0.017      4.171      0.000       0.038       0.106\nqdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n==============================================================================\nOmnibus:                       11.447   Durbin-Watson:                   0.568\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\nSkew:                          -0.216   Prob(JB):                      0.00490\nKurtosis:                       2.707   Cond. No.                         52.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# From previous steps\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n\nmdl_click_vs_impression = ols(\"qdrt_n_clicks ~ qdrt_n_impressions\", data=ad_conversion, ci=None).fit()\n\n# Use this explanatory data\nexplanatory_data = pd.DataFrame({\"qdrt_n_impressions\": np.arange(0, 3e6+1, 5e5) ** .25,\n                                 \"n_impressions\": np.arange(0, 3e6+1, 5e5)})\n\n# Complete prediction_data\nprediction_data = explanatory_data.assign(\n    qdrt_n_clicks = mdl_click_vs_impression.predict(explanatory_data)\n)\n\n# Print the result\nprint(prediction_data)\nprint(\"\\n Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your result\")\n\n\n   qdrt_n_impressions  n_impressions  qdrt_n_clicks\n0            0.000000            0.0       0.071748\n1           26.591479       500000.0       3.037576\n2           31.622777      1000000.0       3.598732\n3           34.996355      1500000.0       3.974998\n4           37.606031      2000000.0       4.266063\n5           39.763536      2500000.0       4.506696\n6           41.617915      3000000.0       4.713520\n\n Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your result\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\base\\model.py:127: ValueWarning: unknown kwargs ['ci']\n  warnings.warn(msg, ValueWarning)\n\n\n\n\n\nIn the previous section, we transformed the response variable, ran a regression, and made predictions. However, we are not yet finished! We will need to perform a back-transformation in order to interpret and visualize your predictions correctly.\n\n\nCode\n# Back transform qdrt_n_clicks\nprediction_data[\"n_clicks\"] = prediction_data['qdrt_n_clicks'] ** 4\nprint(prediction_data)\n\n\n   qdrt_n_impressions  n_impressions  qdrt_n_clicks    n_clicks\n0            0.000000            0.0       0.071748    0.000026\n1           26.591479       500000.0       3.037576   85.135121\n2           31.622777      1000000.0       3.598732  167.725102\n3           34.996355      1500000.0       3.974998  249.659131\n4           37.606031      2000000.0       4.266063  331.214159\n5           39.763536      2500000.0       4.506696  412.508546\n6           41.617915      3000000.0       4.713520  493.607180\n\n\n\n\nCode\n# Plot the transformed variables\nfig = plt.figure()\nsns.regplot(x=\"qdrt_n_impressions\", y=\"qdrt_n_clicks\", data=ad_conversion, ci=None)\n\n# Add a layer of your prediction points\nsns.scatterplot(data=prediction_data, x='qdrt_n_impressions', y='qdrt_n_clicks', color='red')\nplt.show()\nprint(\"\\n Notice that your back-transformed predictions nicely follow the trend line and allow you to make more accurate predictions\")\n\n\n\n\n\n\n Notice that your back-transformed predictions nicely follow the trend line and allow you to make more accurate predictions"
  },
  {
    "objectID": "posts/Preprocessing and Pipelines/Preprocessing and pipelines.html",
    "href": "posts/Preprocessing and Pipelines/Preprocessing and pipelines.html",
    "title": "Preprocessing and Pipelines",
    "section": "",
    "text": "Preprocessing and Pipelines\nLearn how to impute missing values, convert categorical data to numeric values, scale data, evaluate multiple supervised learning models simultaneously, and build pipelines to streamline your workflow!\nThis Preprocessing and Pipelines is part of Datacamp course: Supervised Learning with scikit-learn\nThis is my learning experience of data science through DataCamp\nExploring categorical features\nGapminder datasets that you worked with in previous chapters also contained a categorical ‘Region’ feature, which we dropped since we didn’t have the tools. We have added it back in now that you know about it!\n\n\nCode\nimport matplotlib.pyplot as plt\n# Import pandas\nimport pandas as pd\n\n# Read 'gapminder.csv' into a DataFrame: df\ndf = pd.read_csv(\"gm_2008_region.csv\")\n\n# Create a boxplot of life expectancy per region\ndf.boxplot('life', 'Region', rot=60)\n\n# Show the plot\nplt.show()\n\n\n\n\n\nCreating dummy variables\nScikit-learn does not accept non-numerical features. Earlier we have learned that the ‘Region’ feature contains useful information for predicting life expectancy. Compared to Europe and Central Asia, Sub-Saharan Africa has a lower life expectancy. Thus, retaining the ‘Region’ feature is preferable if we are trying to predict life expectancy. In this exercise, we\n\n\nCode\n# Create dummy variables: df_region\ndf_region = pd.get_dummies(df)\n\n# Print the columns of df_region\nprint(df_region.columns)\nprint(df.sample(10))\n\n# Create dummy variables with drop_first=True: df_region\ndf_region = pd.get_dummies(df,drop_first=True)\n\n# Print the new columns of df_region\nprint(df_region.columns)\nprint(df.sample(10))\n\n\nIndex(['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n       'BMI_female', 'life', 'child_mortality', 'Region_America',\n       'Region_East Asia & Pacific', 'Region_Europe & Central Asia',\n       'Region_Middle East & North Africa', 'Region_South Asia',\n       'Region_Sub-Saharan Africa'],\n      dtype='object')\n      population  fertility  HIV        CO2  BMI_male      GDP  BMI_female  \\\n106  143123163.0       1.49  1.0  11.982718  26.01131  22506.0    128.4903   \n79      406392.0       1.38  0.1   6.182771  27.68361  27872.0    124.1571   \n121    9226333.0       1.92  0.1   5.315688  26.37629  43421.0    122.9473   \n109    9109535.0       1.41  0.1   5.271223  26.51495  12522.0    130.3755   \n37     6004199.0       2.32  0.8   1.067765  26.36751   7450.0    119.9321   \n89    16519862.0       1.77  0.2  10.533028  26.01541  47388.0    121.6950   \n30    19261647.0       4.91  3.7   0.361897  22.56469   2854.0    131.5237   \n83     4111168.0       1.49  0.4   1.313321  24.23690   3890.0    129.9424   \n80     3414552.0       4.94  0.7   0.613104  22.62295   3356.0    129.9875   \n76    27197419.0       2.05  0.5   7.752234  24.73069  19968.0    123.8593   \n\n     life  child_mortality                 Region  \n106  67.6             13.5  Europe & Central Asia  \n79   81.4              6.6  Europe & Central Asia  \n121  81.1              3.2  Europe & Central Asia  \n109  76.4              8.0  Europe & Central Asia  \n37   74.1             21.6                America  \n89   80.3              4.8  Europe & Central Asia  \n30   55.9            116.9     Sub-Saharan Africa  \n83   69.6             17.6  Europe & Central Asia  \n80   63.6            103.0     Sub-Saharan Africa  \n76   74.5              8.0    East Asia & Pacific  \nIndex(['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n       'BMI_female', 'life', 'child_mortality', 'Region_East Asia & Pacific',\n       'Region_Europe & Central Asia', 'Region_Middle East & North Africa',\n       'Region_South Asia', 'Region_Sub-Saharan Africa'],\n      dtype='object')\n     population  fertility    HIV       CO2  BMI_male      GDP  BMI_female  \\\n69    4109389.0       1.57   0.10  3.996722  27.20117  14158.0    127.5037   \n103  10577458.0       1.36   0.50  5.486926  26.68445  27747.0    127.2631   \n61    4480145.0       2.00   0.20  9.882531  27.65325  47713.0    124.7801   \n79     406392.0       1.38   0.10  6.182771  27.68361  27872.0    124.1571   \n114   9132589.0       7.06   0.60  0.068219  21.96917    615.0    131.5318   \n22   19570418.0       5.17   5.30  0.295542  23.68173   2571.0    127.2823   \n128  10408091.0       2.04   0.06  2.440669  25.15699   9938.0    128.6291   \n110   5521838.0       5.13   1.60  0.118256  22.53139   1289.0    134.7160   \n137  13114579.0       5.88  13.60  0.148982  20.68321   3039.0    132.4493   \n56   10050699.0       1.33   0.06  5.453232  27.11568  23334.0    128.6968   \n\n     life  child_mortality                      Region  \n69   77.6             11.3  Middle East & North Africa  \n103  79.2              4.1       Europe & Central Asia  \n61   79.4              4.5       Europe & Central Asia  \n79   81.4              6.6       Europe & Central Asia  \n114  56.7            168.5          Sub-Saharan Africa  \n22   56.6            113.8          Sub-Saharan Africa  \n128  76.5             19.4  Middle East & North Africa  \n110  55.9            179.1          Sub-Saharan Africa  \n137  52.0             94.9          Sub-Saharan Africa  \n56   73.8              7.2       Europe & Central Asia  \n\n\nUsing categorical features in regression\nWe can now build regression models using the dummy variables we have created using the ‘Region’ feature. We will perform 5-fold cross-validation here using ridge regression.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\ndf=pd.read_csv('gm_2008_region.csv')\ndf_region=pd.get_dummies(df)\ndf_region=df_region.drop('Region_America',axis=1)\nX=df_region.drop('life',axis=1).values\ny=df_region['life'].values\n\n\n\n\nCode\n# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate a ridge regressor: ridge\nridge = Ridge(alpha=0.5, normalize=True)\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge,X,y,cv=5)\n\n# Print the cross-validated scores\nprint(ridge_cv)\n\n\n[0.86808336 0.80623545 0.84004203 0.7754344  0.87503712]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\n\n\nDropping missing data\nEarlier excercise voting dataset contained a bunch of missing values that we handled behind the scenes. Now it’s your turn!\nDataFrame df loaded with unprocessed dataset. Use the .head() method in the IPython Shell. Some data points are labeled with ‘?’. Missing values are here. Different datasets encode missing values differently. Data in real life can be messy - sometimes a 9999, sometimes a 0. It’s possible that the missing values are already encoded as NaN. By using NaN, we can take advantage of pandas methods like .dropna() and .fillna(), as well as scikit-learn’s Imputation transformer Imputer().\nThis exercise requires you to convert ’? to NaNs, and then drop the rows that contain them.\n\n\nCode\ndf=pd.read_csv('house-votes-84.csv', header=None, names = ['infants', 'water', 'budget', 'physician', 'salvador', 'religious',\n 'satellite', 'aid', 'missile', 'immigration', 'synfuels', 'education',\n 'superfund', 'crime', 'duty_free_exports', 'eaa_rsa'])\ndf=df.reset_index()\ndf.rename(columns ={'index':'party'},inplace=True)\n\ndf[df=='y']=1\ndf[df=='n']=0\n\n\n\n\nCode\n# Convert '?' to NaN\ndf[df == '?'] = np.nan\n\n# Print the number of NaNs\nprint(df.isnull().sum())\n\n# Print shape of original DataFrame\nprint(\"Shape of Original DataFrame: {}\".format(df.shape))\n\n# Drop missing values and print shape of new DataFrame\ndf = df.dropna()\n\n# Print shape of new DataFrame\nprint(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))\n\n\nparty                  0\ninfants               12\nwater                 48\nbudget                11\nphysician             11\nsalvador              15\nreligious             11\nsatellite             14\naid                   15\nmissile               22\nimmigration            7\nsynfuels              21\neducation             31\nsuperfund             25\ncrime                 17\nduty_free_exports     28\neaa_rsa              104\ndtype: int64\nShape of Original DataFrame: (435, 17)\nShape of DataFrame After Dropping All Rows with Missing Values: (232, 17)\n\n\nWhen many values in your dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It’s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in\nImputing missing data in a ML Pipeline I\nThe process of building a model involves many steps, such as creating training and test sets, fitting a classifier or regressor, tuning its parameters, and evaluating its performance. In this machine learning process, imputation is the first step, which is viewed as part of a pipeline. With Scikit-learn, we can piece together these steps into one process and simplify your workflow.\nWe will now practice setting up a pipeline with two steps: imputation and classifier instantiation. We have so far tried three classifiers: k-NN, logistic regression, and decision trees. The Support Vector Machine, or SVM, is the fourth. Don’t worry about how it works. As with the scikit-learn estimators we have worked with before, it has the same .fit() and .predict() methods.\n\n\nCode\n# Import the Imputer module\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import SVC\n\n# Setup the Imputation transformer: imp\nimp = SimpleImputer(missing_values='NaN', strategy='most_frequent')\n\n# Instantiate the SVC classifier: clf\nclf = SVC()\n\n# Setup the pipeline with the required steps: steps\nsteps = [('imputation', imp),\n        ('SVM', clf)]\n\n\nImputing missing data in a ML Pipeline II\nHaving setup the steps of the pipeline in the previous exercise, you will now use it on the voting dataset to classify a Congressman’s party affiliation. What makes pipelines so incredibly useful is the simple interface that they provide. You can use the .fit() and .predict() methods on pipelines just as you did with your classifiers and regressors!\n\n\nCode\nimport pandas as pd\nimport numpy as np\n#\ndf = pd.read_csv('votes-ch1.csv')\n# Create arrays for the features and the response variable. As a reminder, the response variable is 'party'\ny = df['party'].values\nX = df.drop('party', axis=1).values\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Import necessary modules\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\n# Setup the pipeline steps: steps\nsteps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n        ('SVM', SVC())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n    democrat       0.98      0.96      0.97        85\n  republican       0.94      0.96      0.95        46\n\n    accuracy                           0.96       131\n   macro avg       0.96      0.96      0.96       131\nweighted avg       0.96      0.96      0.96       131\n\n\n\nCentering and scaling your data\nIn the video, Hugo demonstrated how significantly the performance of a model can improve if the features are scaled. Note that this is not always the case: In the Congressional voting records dataset, for example, all of the features are binary. In such a situation, scaling will have minimal impact.\nYou will now explore scaling for yourself on a new dataset - White Wine Quality! Hugo used the Red Wine Quality dataset in the video. We have used the ‘quality’ feature of the wine to create a binary target variable: If ’quality’is less than 5, the target variable is 1, and otherwise, it is 0.\nThe DataFrame has been pre-loaded as df, along with the feature and target variable arrays X and y. Explore it in the IPython Shell. Notice how some features seem to have different units of measurement. ‘density’, for instance, takes values between 0.98 and 1.04, while ‘total sulfur dioxide’ ranges from 9 to 440. As a result, it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features.\n\n\nCode\ndf = pd.read_csv('white-wine.csv')\nX = df.drop('quality' , 1).values # drop target variable\ny1 = df['quality'].values\ny = y1 <= 5\n\n# Import scale\nfrom sklearn.preprocessing import scale\n\n# Scale the features: X_scaled\nX_scaled = scale(X)\n\n# Print the mean and standard deviation of the unscaled features\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X)))\nprint(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n\n# Print the mean and standard deviation of the scaled features\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled)))\nprint(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))\n\n\nMean of Unscaled Features: 18.432687072460002\nStandard Deviation of Unscaled Features: 41.54494764094571\nMean of Scaled Features: 2.7452128118308485e-15\nStandard Deviation of Scaled Features: 0.9999999999999999\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_23428\\525489148.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  X = df.drop('quality' , 1).values # drop target variable\n\n\nCentering and scaling in a pipeline\nWith regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided.\n\n\nCode\n# modified/added by Jinny\nimport pandas as pd\ndf = pd.read_csv('white-wine.csv')\nX = df.drop('quality' , 1).values # drop target variable\ny1 = df['quality'].values\ny = y1 <= 5\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# Import the necessary modules\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled = pipeline.fit(X_train,y_train)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_23428\\594620727.py:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  X = df.drop('quality' , 1).values # drop target variable\n\n\nAccuracy with Scaling: 0.7700680272108843\nAccuracy without Scaling: 0.6979591836734694\n\n\nFantastic! It looks like scaling has significantly improved model performance!\nBringing it all together I: Pipeline for classification\nIt is time now to piece together everything you have learned so far into a pipeline for classification! Your job in this exercise is to build a pipeline that includes scaling and hyperparameter tuning to classify wine quality.\nYou’ll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the you tuned for logistic regression in Chapter 3, while gammagamma controls the kernel coefficient: Do not worry about this now as it is beyond the scope of this course.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('white-wine.csv')\nX = df.drop('quality' , 1).values # drop target variable\ny1 = df['quality'].values\ny = y1 <= 5\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_23428\\2362924875.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  X = df.drop('quality' , 1).values # drop target variable\n\n\n\n\nCode\n# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('SVM', SVC())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=21)\n\n# Instantiate the GridSearchCV object: cv\ncv = GridSearchCV(pipeline,parameters,cv=3)\n\n# Fit to the training set\ncv.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n\n\nBringing it all together II: Pipeline for regression\nFor this final exercise, you will return to the Gapminder dataset. Guess what? Even this dataset has missing values that we dealt with for you in earlier chapters! Now, you have all the tools to take care of them yourself!\nYour job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV.\n\n\nCode\n\n\n# modified/added by Jinny\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndf = pd.read_csv('gapminder-clean.csv')\n\ny = df['life'].values\nX = df.drop('life', axis=1).values\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\n\n\n# Setup the pipeline steps: steps\n# modified by Jinny: Imputer -> SimpleImputer\nsteps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='mean')),\n         ('scaler', StandardScaler()),\n         ('elasticnet', ElasticNet())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ngm_cv.fit(X_train, y_train)\n\n# Compute and print the metrics\nr2 = gm_cv.score(X_test, y_test)\nprint(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2))"
  },
  {
    "objectID": "posts/Quantifying model fit/Quantifying model fit.html",
    "href": "posts/Quantifying model fit/Quantifying model fit.html",
    "title": "Assessing model fit",
    "section": "",
    "text": "What questions to ask your model to determine its fit. We will discuss how to quantify how well a linear regression model fits, how to diagnose problems with the model using visualizations, and how each observation impacts the model.\nThis Assessing model fit is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n\n\n\nAnalyze and assess the accuracy of model predictions.\nCoefficient of determination: R-squared (1 is the best, 0 is as good as randomness).\nThe proportion of variance in the response variable that is predictable (explainable) by the explanatory variable. This information indicates whether the model at hand is effective in resuming our data or not. Data, context, and the way we transform variables heavily impact r-squared interpretation.\nAccessible inside .summary() or .rsquared\nResidual standard error (RSE)\nThe residual is the difference between the predicted and observed response values (the distance). It has the same unit as the response.\nMSE = RSE**2 RSE = np.sqrt(MSE)\nAccessible with .mse_resid()\nRSE is calculated manually by taking the square of each residual. The degrees of freedom are calculated (# of observations minus # of model coefficients). Then we take the square root of the sum divided by the deg_freedom.\nRoot mean square error\nUnlike MSE, we do not remove degrees of freedom (we divide only by the number of observations).\n\n\n\nA coefficient of determination measures how well the linear regression line fits the observed values. It is equal to the square root of the correlation between the explanatory and response variables in a simple linear regression.\n\n\nCode\n# fetch data for which model is created\nad_conversion=pd.read_csv('dataset/ad_conversion.csv')\nad_conversion.head()\n\n\n\n\n\n\n  \n    \n      \n      spent_usd\n      n_impressions\n      n_clicks\n    \n  \n  \n    \n      0\n      1.43\n      7350\n      1\n    \n    \n      1\n      1.82\n      17861\n      2\n    \n    \n      2\n      1.25\n      4259\n      1\n    \n    \n      3\n      1.29\n      4133\n      1\n    \n    \n      4\n      4.77\n      15615\n      3\n    \n  \n\n\n\n\n\n\nCode\n# click vs impression model\nmdl_click_vs_impression_orig = ols('n_clicks ~ n_impressions' , data = ad_conversion).fit()\n# Print a summary of mdl_click_vs_impression_orig\nprint(mdl_click_vs_impression_orig.summary())\n\n# Create qdrt_n_impressions and qdrt_n_clicks\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion['n_impressions'] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion['n_clicks'] ** 0.25\n\n# qdrnt click vs impression model\nmdl_click_vs_impression_trans = ols('qdrt_n_clicks ~ qdrt_n_impressions  ' , data = ad_conversion).fit()\n# Print a summary of mdl_click_vs_impression_trans\nprint(mdl_click_vs_impression_trans.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               n_clicks   R-squared:                       0.892\nModel:                            OLS   Adj. R-squared:                  0.891\nMethod:                 Least Squares   F-statistic:                     7683.\nDate:                Fri, 13 Jan 2023   Prob (F-statistic):               0.00\nTime:                        09:10:19   Log-Likelihood:                -4126.7\nNo. Observations:                 936   AIC:                             8257.\nDf Residuals:                     934   BIC:                             8267.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         1.6829      0.789      2.133      0.033       0.135       3.231\nn_impressions     0.0002   1.96e-06     87.654      0.000       0.000       0.000\n==============================================================================\nOmnibus:                      247.038   Durbin-Watson:                   0.870\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            13215.277\nSkew:                          -0.258   Prob(JB):                         0.00\nKurtosis:                      21.401   Cond. No.                     4.88e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.88e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          qdrt_n_clicks   R-squared:                       0.945\nModel:                            OLS   Adj. R-squared:                  0.944\nMethod:                 Least Squares   F-statistic:                 1.590e+04\nDate:                Fri, 13 Jan 2023   Prob (F-statistic):               0.00\nTime:                        09:10:19   Log-Likelihood:                 193.90\nNo. Observations:                 936   AIC:                            -383.8\nDf Residuals:                     934   BIC:                            -374.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.0717      0.017      4.171      0.000       0.038       0.106\nqdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n==============================================================================\nOmnibus:                       11.447   Durbin-Watson:                   0.568\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\nSkew:                          -0.216   Prob(JB):                      0.00490\nKurtosis:                       2.707   Cond. No.                         52.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# Print the coeff of determination for mdl_click_vs_impression_orig\nprint(mdl_click_vs_impression_orig.rsquared)\n\n# Print the coeff of determination for mdl_click_vs_impression_trans\nprint(mdl_click_vs_impression_trans.rsquared)\n\nprint(\"\\n The transformed model has a higher coefficient of determination than the original model, suggesting that it gives a better fit to the data.\")\n\n\n0.8916134973508041\n0.9445272817143905\n\n The transformed model has a higher coefficient of determination than the original model, suggesting that it gives a better fit to the data.\n\n\n\n\n\nThe residual standard error (RSE) measures the typical residual size. Predictions are measured by how wrong they can be. The data fits better with smaller numbers, with zero being perfect\n\n\nCode\n# Calculate mse_orig for mdl_click_vs_impression_orig\nmse_orig = mdl_click_vs_impression_orig.mse_resid\n\n# Calculate rse_orig for mdl_click_vs_impression_orig and print it\nrse_orig = np.sqrt(mse_orig)\nprint(\"RSE of original model: \", rse_orig)\n\n# Calculate mse_trans for mdl_click_vs_impression_trans\nmse_trans = mdl_click_vs_impression_trans.mse_resid\n\n# Calculate rse_trans for mdl_click_vs_impression_trans and print it\nrse_trans = np.sqrt(mse_trans)\nprint(\"RSE of transformed model: \", rse_trans)\n\n\nRSE of original model:  19.905838862478134\nRSE of transformed model:  0.19690640896875722\n\n\n\n\n\nIf the model is well fitted, the residuals should be normally distributed along the line/curve, and the mean should be zero. In addition, it indicates when the fitted residuals are positive or negative (above/below the straight line).\nResidual VS fitted values chart\nTrends can be visualized using this tool. The best accuracy is achieved by following the y=0 line. There is a problem if the curve goes all over the place.\nsns.residplot()\nQ-Q Plot\nThe best conditions are validated if the points track along a straight line and are normally distributed. Otherwise, they are not.\nqqplot() (from statsmodels.api import qqplot)\nSquare root of Standardized Residuals VS fitted values, Scale-location plot\nAs the fitted values change, the residuals change in size and whether they become smaller or larger. If it bounces all over or is irregular, it means residuals tend to vary randomly or in an inconsistent manner as fitted values change.\n\n\n\nLet’s draw diagnostic plots using the Taiwan real estate dataset and the model of house prices versus number of convenience stores.\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847\n    \n  \n\n\n\n\n\n\nCode\nmdl_price_vs_conv=ols(\"price_twd_msq ~ n_convenience\",data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Plot the residuals vs. fitted values\nsns.residplot(x='n_convenience', y='price_twd_msq', data=taiwan_real_estate, lowess=True)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Import qqplot\nfrom statsmodels.api import qqplot\n\n# Create the Q-Q plot of the residuals\nqqplot(data=mdl_price_vs_conv.resid, fit=True, line=\"45\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Preprocessing steps\nmodel_norm_residuals = mdl_price_vs_conv.get_influence().resid_studentized_internal\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# Create the scale-location plot\nsns.regplot(x=mdl_price_vs_conv.fittedvalues, y=model_norm_residuals_abs_sqrt, ci=None, lowess=True)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Sqrt of abs val of stdized residuals\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\nprint(\"Above three diagnostic plots are excellent for sanity-checking the quality of your models\")\n\n\nAbove three diagnostic plots are excellent for sanity-checking the quality of your models"
  },
  {
    "objectID": "posts/Quantifying model fit/Quantifying model fit.html#extracting-leverage-and-influence",
    "href": "posts/Quantifying model fit/Quantifying model fit.html#extracting-leverage-and-influence",
    "title": "Assessing model fit",
    "section": "Extracting leverage and influence",
    "text": "Extracting leverage and influence\nLets find leverage and influence for taiwan real estate data\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Create summary_info\nsummary_info = mdl_price_vs_dist.get_influence().summary_frame()\nprint(summary_info.head(n=10))\n\n\n   dfb_Intercept  dfb_sqrt_dist_to_mrt_m       cooks_d  standard_resid  \\\n0      -0.094893                0.073542  4.648246e-03       -1.266363   \n1      -0.013981                0.008690  1.216711e-04       -0.262996   \n2       0.025510               -0.009963  6.231096e-04        0.688143   \n3       0.055525               -0.021686  2.939394e-03        1.494602   \n4      -0.000932                0.000518  6.055123e-07       -0.019716   \n5      -0.012257                0.029560  7.976174e-04        0.544490   \n6       0.000592               -0.000187  3.896928e-07        0.017531   \n7       0.010115               -0.006428  6.232088e-05        0.185284   \n8      -0.087118                0.126666  9.060428e-03        0.915959   \n9       0.009041               -0.033610  1.378024e-03       -0.818660   \n\n   hat_diag  dffits_internal  student_resid    dffits  \n0  0.005764        -0.096418      -1.267294 -0.096489  \n1  0.003506        -0.015599      -0.262699 -0.015582  \n2  0.002625         0.035302       0.687703  0.035279  \n3  0.002625         0.076673       1.496850  0.076789  \n4  0.003106        -0.001100      -0.019692 -0.001099  \n5  0.005352         0.039940       0.544024  0.039906  \n6  0.002530         0.000883       0.017509  0.000882  \n7  0.003618         0.011164       0.185067  0.011151  \n8  0.021142         0.134614       0.915780  0.134587  \n9  0.004095        -0.052498      -0.818332 -0.052477  \n\n\n\n\nCode\n# Add the hat_diag column to taiwan_real_estate, name it leverage\ntaiwan_real_estate[\"leverage\"] = summary_info['hat_diag']\n\n# Sort taiwan_real_estate by leverage in descending order and print the head\nprint(taiwan_real_estate.sort_values(by='leverage', ascending=False).head())\n\n\n     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \\\n347       6488.021              1        15 to 30       3.388805   \n116       6396.283              1        30 to 45       3.691377   \n249       6306.153              1        15 to 30       4.538578   \n255       5512.038              1        30 to 45       5.264750   \n8         5512.038              1        30 to 45       5.688351   \n\n     sqrt_dist_to_mrt_m  leverage  \n347           80.548253  0.026665  \n116           79.976765  0.026135  \n249           79.411290  0.025617  \n255           74.243101  0.021142  \n8             74.243101  0.021142  \n\n\n\n\nCode\n# Add the cooks_d column to taiwan_real_estate, name it cooks_dist\ntaiwan_real_estate['cooks_dist'] = summary_info['cooks_d']\n\n# Sort taiwan_real_estate by cooks_dist in descending order and print the head.\nprint(taiwan_real_estate.sort_values(\"cooks_dist\", ascending=False).head())\n\n\n     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \\\n270       252.5822              1         0 to 15      35.552194   \n148      3780.5900              0        15 to 30      13.645991   \n228      3171.3290              0         0 to 15      14.099849   \n220       186.5101              9        30 to 45      23.691377   \n113       393.2606              6         0 to 15       2.299546   \n\n     sqrt_dist_to_mrt_m  leverage  cooks_dist  \n270           15.892835  0.003849    0.115549  \n148           61.486503  0.012147    0.052440  \n228           56.314554  0.009332    0.035384  \n220           13.656870  0.004401    0.025123  \n113           19.830799  0.003095    0.022813"
  },
  {
    "objectID": "posts/Random numbers and probability/Random numbers and probability.html",
    "href": "posts/Random numbers and probability/Random numbers and probability.html",
    "title": "Random Numbers and Probability",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nRandom Numbers and Probability\nThis Random Numbers and Probability is part of Datacamp course: Introduction to Statistic in Python\nHere we’ll explore how to generate random samples and measure chance using probability.We will work with real-world sales data to calculate the probability of a salesperson being successful. Finally, we will try to use the binomial distribution to model events with binary outcomes.\nThis is my learning experience of data science through DataCamp\n\nMeasuring chance\n\\[ P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total # of possible outcomes}} \\]\nSampling with replacement vs sampling without replacement\nsampling without replacement, in which a subset of the observations are selected randomly, and once an observation is selected it cannot be selected again. sampling with replacement, in which a subset of observations are selected randomly, and an observation may be selected more than once\n\n\nCalculating probabilities\nYou’re in charge of the sales team, and it’s time for performance reviews, starting with Amir. As part of the review, you want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals.\nRecall that the probability of an event can be calculated by\n\\[ P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total # of possible outcomes}} \\]\n\n\nCode\namir_deals = pd.read_csv('amir_deals.csv', index_col=0)\namir_deals.head()\n\n\n\n\n\n\n  \n    \n      \n      product\n      client\n      status\n      amount\n      num_users\n    \n  \n  \n    \n      1\n      Product F\n      Current\n      Won\n      7389.52\n      19\n    \n    \n      2\n      Product C\n      New\n      Won\n      4493.01\n      43\n    \n    \n      3\n      Product B\n      New\n      Won\n      5738.09\n      87\n    \n    \n      4\n      Product I\n      Current\n      Won\n      2591.24\n      83\n    \n    \n      5\n      Product E\n      Current\n      Won\n      6622.97\n      17\n    \n  \n\n\n\n\n\n\nCode\ncounts = amir_deals['product'].value_counts()\nprint(counts)\n\n# Calculate probability of picking a deal with each product\nprobs = counts / len(amir_deals['product'])\nprint(probs)\n\n\nProduct B    62\nProduct D    40\nProduct A    23\nProduct C    15\nProduct F    11\nProduct H     8\nProduct I     7\nProduct E     5\nProduct N     3\nProduct G     2\nProduct J     2\nName: product, dtype: int64\nProduct B    0.348315\nProduct D    0.224719\nProduct A    0.129213\nProduct C    0.084270\nProduct F    0.061798\nProduct H    0.044944\nProduct I    0.039326\nProduct E    0.028090\nProduct N    0.016854\nProduct G    0.011236\nProduct J    0.011236\nName: product, dtype: float64\n\n\n\n\nSampling deals\nIn the previous exercise, you counted the deals Amir worked on. Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement.\nAdditionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed before sampling from the deals.\n\n\nCode\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals without replacement\nsample_without_replacement = amir_deals.sample(5,replace=False)\nprint(sample_without_replacement)\n\n\n       product   client status   amount  num_users\n128  Product B  Current    Won  2070.25          7\n149  Product D  Current    Won  3485.48         52\n78   Product B  Current    Won  6252.30         27\n105  Product D  Current    Won  4110.98         39\n167  Product C      New   Lost  3779.86         11\n\n\n\n\nCode\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals with replacement\nsample_with_replacement = amir_deals.sample(5,replace=True)\nprint(sample_with_replacement)\n\n\n       product   client status   amount  num_users\n163  Product D  Current    Won  6755.66         59\n132  Product B  Current    Won  6872.29         25\n88   Product C  Current    Won  3579.63          3\n146  Product A  Current    Won  4682.94         63\n146  Product A  Current    Won  4682.94         63\n\n\n\n\nDiscrete distributions\n\nProbability distribution\n\nDescribe probability of each possible outcome in a scenario\nExpected value: mean of probability distribution\n\nLaw of large number (LLN): as size of sample increases, sample mean will approach expected value.\n\n\n\nCreating a probability distribution\nRestaurant management wants to optimize seating space based on the size of the groups that come most often to a new restaurant. One night, 10 groups of people are waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. This exercise examines the probability of picking groups of different sizes.\nRemember that expected value can be calculated by multiplying each possible outcome with its corresponding probability and taking the sum\n\n\nCode\nrestaurant_groups = pd.read_csv('restaurant_groups.csv')\nrestaurant_groups.head()\n\n\n\n\n\n\n  \n    \n      \n      group_id\n      group_size\n    \n  \n  \n    \n      0\n      A\n      2\n    \n    \n      1\n      B\n      4\n    \n    \n      2\n      C\n      6\n    \n    \n      3\n      D\n      2\n    \n    \n      4\n      E\n      2\n    \n  \n\n\n\n\n\n\nCode\n# Create a histogram of restaurant_groups and show plot\nrestaurant_groups['group_size'].hist(bins=[2, 3, 4, 5, 6])\n\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nCode\n# Create probability distribution\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\nprint(size_dist)\n\n# Expected value\nexpected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\nprint(expected_value)\n\n# Subset groups of size 4 or more\ngroups_4_or_more = size_dist[size_dist['group_size'] >=4]\n\n# Sum the probabilities of groups_4_or_more\nprob_4_or_more = groups_4_or_more['prob'].sum()\nprint(prob_4_or_more)\n\n\n   group_size  prob\n0           2   0.6\n1           4   0.2\n2           6   0.1\n3           3   0.1\n2.9000000000000004\n0.30000000000000004\n\n\n\n\nContinuous distributions\nData back-ups\nYour company’s sales software backs itself up automatically, but no one knows exactly when the back-ups take place. It is known, however, that back-ups occur every 30 minutes. Amir updates the client data after sales meetings at random times. When will his newly-entered data be backed up? Answer Amir’s questions using your new knowledge of continuous uniform distributions\n\n\nCode\nfrom scipy.stats import uniform\n\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 = uniform.cdf(5, min_time, max_time)\nprint(prob_less_than_5)\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\nprint(prob_greater_than_5)\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - \\\n                        uniform.cdf(10, min_time, max_time)\nprint(prob_between_10_and_20)\n\n\n0.16666666666666666\n0.8333333333333334\n0.3333333333333333\n\n\n\n\nSimulating wait times\nTo give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\n\n\nCode\nnp.random.seed(334)\n\n# Generates 1000 wait times between 0 and 30 mins\nwait_times = uniform.rvs(min_time, max_time, 1000)\nprint(wait_times[:10])\n\n# Create a histogram of simulated times and show plot\nplt.hist(wait_times);\nprint (\"Unless Amir figures out exactly what time each backup happens, he won't be able to time his data entry so it gets backed up sooner, but it looks like he'll wait about 15 minutes on average\\n\")\n\n\n[ 7.144097    0.97455866  3.72802787  5.11644319  8.70602482 24.69140099\n 23.98012075  3.19592668 25.1985306  17.89048629]\nUnless Amir figures out exactly what time each backup happens, he won't be able to time his data entry so it gets backed up sooner, but it looks like he'll wait about 15 minutes on average\n\n\n\n\n\n\n\n\nThe binomial distribution\n\nBinomial distribution\n\nProbability distribution of number of successes in a sequence of independent trials\nDescribed by n and p\n\nn: total number of trials\np: probability of success\n\nExpected value: n * p\nIndependence: The binomial distribution is a probability distribution of number of successes in a sequence of independent trials\n\n\n\n\nSimulating sales deals\nAssume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance.\n\n\nCode\n# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate a single deal\nprint(binom.rvs(1, 0.3, size=1))\n\n# Simulate 1 week of 3 deals\nprint(binom.rvs(3,0.3,size=1))\n\n# Simulate 52 weeks of 3 deals\ndeals = binom.rvs(3,0.3,size=52)\n\n# Print mean deals won per week\nprint(np.mean(deals))\n\nprint('\\nIn this simulated year, Amir won 0.83 deals on average each week')\n\n\n[1]\n[0]\n0.8461538461538461\n\nIn this simulated year, Amir won 0.83 deals on average each week\n\n\n\n\nCalculating binomial probabilities\nJust as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution.\n\n\nCode\n# Probability of closing 3 out of 3 deals\nprob_3 = binom.pmf(3,3,0.3)\nprint(prob_3)\n\n# Probability of closing <= 1 deal out of 3 deals\nprob_less_than_or_equal_1 = binom.cdf(1,3,0.3)\nprint(prob_less_than_or_equal_1)\n\n# Probability of closing > 1 deal out of 3 deals\nprob_greater_than_1 =1- binom.cdf(1,3,0.3)\nprint(prob_greater_than_1)\n\nprint(\"\\nAmir has about a 22% chance of closing more than one deal in a week.\")\n\n\n0.026999999999999996\n0.784\n0.21599999999999997\n\nAmir has about a 22% chance of closing more than one deal in a week.\n\n\n\n\nHow many sales will be won?\nNow Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by n*p\n\n\nCode\n# Expected number won with 30% win rate\nwon_30pct = 3 * 0.3\nprint(won_30pct)\n\n# Expected number won with 25% win rate\nwon_25pct = 3 * 0.25\nprint(won_25pct)\n\n# Expected number won with 35% win rate\nwon_35pct = 3 * 0.35\nprint(won_35pct)\n\nprint('\\nIf Amirs win rate goes up by 5%, he can expect to close more than 1 deal on average each week')\n\n\n0.8999999999999999\n0.75\n1.0499999999999998\n\nIf Amirs win rate goes up by 5%, he can expect to close more than 1 deal on average each week"
  },
  {
    "objectID": "posts/Regression/Regression.html",
    "href": "posts/Regression/Regression.html",
    "title": "Kakamana's Blogs",
    "section": "",
    "text": "Data import for supervised learning\nThe data from Gapminder has been consolidated into a single CSV file called ‘gapminder.csv’ in the workspace. Based on features such as a country’s GDP, fertility rate, and population, you will use this data to predict life expectancy in that country. The dataset has been preprocessed as in Chapter 1.\nThe target variable here is quantitative, so this is a regression problem. First, you will fit a linear regression with one feature: ‘fertility’, which is the average number of children a woman gives birth to in a given country. Regression models will be built using all the features in later exercises.\nFirst, you must import the data and prepare it for scikit-learn. Feature and target variable arrays must be created. Additionally, since you will only use one feature to begin with, you’ll need to reshape it using NumPy’s .reshape() method.\n\n# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv('gm_2008_region.csv')\n\n# Create arrays for features and target variable\ny = df['life'].values\nX = df['fertility'].values\n\n# Print the dimensions of y and X before reshaping\nprint(\"Dimensions of y before reshaping: \", y.shape)\nprint(\"Dimensions of X before reshaping: \", X.shape)\n\n# Reshape X and y\ny_reshaped = y.reshape(-1,1)\nX_reshaped = X.reshape(-1,1)\n\n# Print the dimensions of y_reshaped and X_reshaped\nprint(\"Dimensions of y after reshaping: \", y_reshaped.shape)\nprint(\"Dimensions of X after reshaping: \", X_reshaped.shape)\n\nDimensions of y before reshaping:  (139,)\nDimensions of X before reshaping:  (139,)\nDimensions of y after reshaping:  (139, 1)\nDimensions of X after reshaping:  (139, 1)\n\n\n\nsns.heatmap(df.corr(),square=True,cmap='RdYlGn')\n\n<AxesSubplot:>\n\n\n\n\n\n\nFit & predict for regression\nNow, you will fit a linear regression and predict life expectancy using just one feature. You saw Andy do this earlier using the ‘RM’ feature of the Boston housing dataset. In this exercise, you will use the ‘fertility’ feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is ‘life’. The array for the target variable has been pre-loaded as y and the array for ‘fertility’ has been pre-loaded as X_fertility.\nA scatter plot with ‘fertility’ on the x-axis and ‘life’ on the y-axis has been generated. As you can see, there is a strongly negative correlation, so a linear regression should be able to capture this trend. Your job is to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. You will also compute and print the score using scikit-learn’s .score() method.\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n#This need to be imported because its is cleaned and processed data\ndf=pd.read_csv(\"gapminder-clean.csv\")\n\ny=df['life'].values\nX=df.drop('life',axis=1)\n\n#reshape to 1-D\ny=y.reshape(-1,1)\nX_fertility=X['fertility'].values.reshape(-1,1)\n\n_ = plt.scatter(X['fertility'],y,color='blue')\n_=plt.ylabel('Life Expectancy')\n_=plt.xlabel('Fertility')\n\n# Import LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create the regression\nreg=LinearRegression()\n\n# Create the prediction space\nprediction_space = np.linspace(min(X_fertility),max(X_fertility)).reshape(-1,1)\n\n# Fit model to the data\nreg.fit(X_fertility,y)\n\n# Compute predictions over the prediction space: y_pred\ny_pred = reg.predict(prediction_space)\n\n# print R^2\nprint(reg.score(X_fertility,y))\n\n# plot regression line\nplt.plot(prediction_space,y_pred, color='black',linewidth=3)\nplt.show()\n\n0.6192442167740035\n\n\n\n\n\nNotice how the line captures the underlying trend in the data. And the performance is quite decent for this basic regression model with only one feature!\nRegression training/testing split\nYour supervised learning model must be able to generalize well to new data, as you learned in Chapter 1. In both classification and linear regression models, this is true.\nYou will divide the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. You will also compute the Root Mean Squared Error (RMSE) when evaluating regression models in addition to the R2R2score.\n\n# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n\n# Create the regressor: reg_all\nreg_all = LinearRegression()\n\n# Fit the regressor to the training data\nreg_all.fit(X_train,y_train)\n\n# Predict on the test data: y_pred\ny_pred = reg_all.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n\nR^2: 0.8380468731429362\nRoot Mean Squared Error: 3.24760108003772\n\n\nUsing all features has improved the model score. This makes sense, as the model has more information to learn from. However, there is one potential pitfall to this process. Can you spot it? You’ll learn about this as well how to better validate your models in the below code flow\n5-fold cross-validation\nIn order to evaluate a model, cross-validation is essential. A maximum amount of data is used to train the model, since the model is not only trained, but also tested on all available data during training.\nUsing Gapminder data, we will practice 5-fold cross validation. The cross_val_score() function in scikit-learn uses\nIn regression, this is the metric of choice. As we are performing 5-fold cross-validation, the function will return 5 scores. The average of these five scores is what we need to do.\n\n# Import the necessary modules\n# Import the necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(reg,X,y,cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n\n[0.81720569 0.82917058 0.90214134 0.80633989 0.94495637]\nAverage 5-Fold CV Score: 0.8599627722793401\n\n\nK-Fold CV comparison\nCross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset.\nIn the IPython Shell, you can use %timeit to see how long each 3-fold CV takes compared to 10-fold CV by executing the following cv=3 and cv=10:\n%timeit cross_val_score(reg, X, y, cv = ____)\n\n# Import the necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Perform 3-fold CV\ncvscores_3 = cross_val_score(reg,X,y,cv=3)\nprint(np.mean(cvscores_3))\n\n# Perform 10-fold CV\ncvscores_10 = cross_val_score(reg,X,y,cv=10)\nprint(np.mean(cvscores_10))\n\n0.8718712782622058\n0.8436128620131151\n\n\nRegularization I: Lasso\nIn this study, we examine how Lasso selected RM as the most important feature for predicting Boston house prices, while shrinking the coefficients of certain other features to 0. When dealing with data involving thousands of features, its ability to perform feature selection in this manner becomes even more useful.\nWe will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Similarly to Boston data, you will find that some coefficients are shrunk to 0, leaving only the most significant ones.\nAccording to the lasso algorithm, it seems like ‘child_mortality’ is the most important feature when predicting life expectancy.\nRegularization II: Ridge\nYou should use Ridge regression when building regression models rather than Lasso when selecting features.\nLASSO performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. Also known as\nThe regularization term is the regularization term\nThe coefficients’ norm. However, this is not the only way to regularize.\nAlternatively, you could take the squared values of the coefficients multiplied by some alpha - like Ridge regression - and compute\nAs a rule. As part of this exercise, you will fit ridge regression models with a range of different alphas, and plot cross-validated scores for each using this function, which plots the score and standard error for each alpha:\n\ndef display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std / np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()\n\n\n# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Setup the array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n\n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge,X,y,cv=10)\n\n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n\n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\n\n\n\n\n\nNotice how the cross-validation scores change with different alphas. Which alpha should you pick? How can you fine-tune your model? You’ll learn all about this in the next section"
  },
  {
    "objectID": "posts/Sampling Distribution/Sampling Distributions.html",
    "href": "posts/Sampling Distribution/Sampling Distributions.html",
    "title": "Sampling Distribution",
    "section": "",
    "text": "We will discover how to quantify the accuracy of sample statistics using relative errors, and measure variation in your estimates by generating sampling distributions.\nThis Sampling Distribution is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nThe size of the sample has a significant impact on the accuracy of the point estimates.\nRelative error vs. sample size\n\nReally noise, particularly for small samples\nAmplitude is initially steep, then flattens\nRelative error decreases to zero (when the sample size = population)\n\n\n\nIt is important for a sample mean to be close to the population mean when it is calculated. There is, however, a possibility that this may not be the case if your sample size is too small.\nRelative error is the most common metric for assessing accuracy. This is the difference between the population parameter and the point estimate, divided by the population parameter. Sometimes it is expressed as a percentage.\n\n\nCode\nattrition_pop=pd.read_feather('dataset/attrition.feather')\nattrition_pop.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      Attrition\n      BusinessTravel\n      DailyRate\n      Department\n      DistanceFromHome\n      Education\n      EducationField\n      EnvironmentSatisfaction\n      Gender\n      ...\n      PerformanceRating\n      RelationshipSatisfaction\n      StockOptionLevel\n      TotalWorkingYears\n      TrainingTimesLastYear\n      WorkLifeBalance\n      YearsAtCompany\n      YearsInCurrentRole\n      YearsSinceLastPromotion\n      YearsWithCurrManager\n    \n  \n  \n    \n      0\n      21\n      0.0\n      Travel_Rarely\n      391\n      Research_Development\n      15\n      College\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      6\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      1\n      19\n      1.0\n      Travel_Rarely\n      528\n      Sales\n      22\n      Below_College\n      Marketing\n      Very_High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      2\n      Good\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18\n      1.0\n      Travel_Rarely\n      230\n      Research_Development\n      3\n      Bachelor\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      High\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18\n      0.0\n      Travel_Rarely\n      812\n      Sales\n      10\n      Bachelor\n      Medical\n      Very_High\n      Female\n      ...\n      Excellent\n      Low\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18\n      1.0\n      Travel_Frequently\n      1306\n      Sales\n      5\n      Bachelor\n      Marketing\n      Medium\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      3\n      Better\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 31 columns\n\n\n\n\n\nCode\n# Generate a simple random sample of 50 rows, with seed 2022\nattrition_srs50 = attrition_pop.sample(n=50, random_state=2022)\n\n# Calculate the mean employee attrition in the sample\nmean_attrition_srs50 = attrition_srs50['Attrition'].mean()\nmean_attrition_pop= attrition_pop['Attrition'].mean()\n\n# Calculate the relative error percentage\nrel_error_pct50 = 100 * abs(mean_attrition_pop - mean_attrition_srs50) / mean_attrition_pop\n\n# Print rel_error_pct50\nprint(rel_error_pct50)\n\n\n62.78481012658227\n\n\n\n\nCode\n# Generate a simple random sample of 100 rows, with seed 2022\nattrition_srs100 = attrition_pop.sample(n=100, random_state=2022)\n\n# Calculate the mean employee attrition in the sample\nmean_attrition_srs100 = attrition_srs100['Attrition'].mean()\n\n# Calculate the relative error percentage\nrel_error_pct100 = 100 * abs(mean_attrition_pop - mean_attrition_srs100) / mean_attrition_pop\n\n# Print rel_error_pct100\nprint(rel_error_pct100)\nprint(\"\\n As you increase the sample size, the sample mean generally gets closer to the population mean, and the relative error decreases\")\n\n\n6.962025316455695\n\n As you increase the sample size, the sample mean generally gets closer to the population mean, and the relative error decreases\n\n\n\n\n\nReplicating samples\nWhenever you calculate a point estimate, such as a sample mean, the value you calculate depends on the rows included in the sample. As a result, there is some randomness in the answer. A sample mean (or another statistic) can be calculated for each sample in order to quantify the variation caused by this randomness.\n\n\nCode\n# Create an empty list\nmean_attritions=[]\n# Loop 500 times to create 500 sample means\nfor i in range(500):\n    mean_attritions.append(\n        attrition_pop.sample(n=60)['Attrition'].mean()\n    )\n\n# Print out the first few entries of the list\nprint(mean_attritions[0:5])\n\n\n[0.13333333333333333, 0.11666666666666667, 0.13333333333333333, 0.18333333333333332, 0.2]\n\n\n\n\nCode\n# Create a histogram of the 500 sample means\nplt.hist(mean_attritions,bins=16)\nplt.show()\n\n\n\n\n\n\n\n\nEarlier we saw that while increasing the number of replicates didn’t affect the relative error of the sample means; it did result in a more consistent shape to the distribution.\n\n\nCode\nimport itertools\ndef expand_grid(data_dict):\n    rows = itertools.product(*data_dict.values())\n    return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n\n\n\n\nCode\n# Expand a grid representing 5 8-sided dice\ndice = expand_grid(\n  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n  })\n\n# Add a column of mean rolls and convert to a categorical\ndice['mean_roll'] = (dice['die1'] + dice['die2'] +\n                    dice['die3'] + dice['die4'] +\n                    dice['die5'] ) / 5\n\n\ndice['mean_roll'] = dice['mean_roll'].astype('category')\n\n# Print result\nprint(dice)\n\n\n       die1  die2  die3  die4  die5 mean_roll\n0         1     1     1     1     1       1.0\n1         1     1     1     1     2       1.2\n2         1     1     1     1     3       1.4\n3         1     1     1     1     4       1.6\n4         1     1     1     1     5       1.8\n...     ...   ...   ...   ...   ...       ...\n32763     8     8     8     8     4       7.2\n32764     8     8     8     8     5       7.4\n32765     8     8     8     8     6       7.6\n32766     8     8     8     8     7       7.8\n32767     8     8     8     8     8       8.0\n\n[32768 rows x 6 columns]\n\n\n\n\nCode\n# Draw a bar plot of mean_roll\ndice['mean_roll'].value_counts(sort=False).plot(kind='bar')\nplt.show()\n\n\n\n\n\n\n\n\nIt is only possible to calculate the exact sampling distribution in very simple situations. If just five eight-sided dice are used, the number of possible rolls is 8**5, which is over thirty thousand. The number of possible outcomes becomes too difficult to calculate when the dataset is more complicated, such as when a variable has hundreds or thousands of categories.\nYou can calculate an approximate sampling distribution by simulating the exact sampling distribution. You can repeat a procedure repeatedly to simulate both the sampling process and the sample statistic calculation process.\n\n\nCode\n# Sample one to eight, five times, with replacement\nfive_rolls = np.random.choice(list(range(1,9)), size=5, replace=True)\n\n# Print the mean of five_rolls\nprint(five_rolls.mean())\n\n\n6.2\n\n\n\n\nCode\n# Replicate the sampling code 1000 times\nsample_means_1000 = []\nfor i in range(1000):\n    sample_means_1000.append(\n        np.random.choice(list(range(1, 9)), size=5, replace=True).mean())\n\n\n# Print the first 10 entries of the result\nprint(sample_means_1000[0:10])\n\n\n[4.0, 4.8, 5.4, 2.8, 3.4, 3.4, 4.0, 4.8, 5.0, 5.4]\n\n\n\n\nCode\n# Draw a histogram of sample_means_1000 with 20 bins\nplt.hist(sample_means_1000, bins=20)\nplt.show()\n\n\n\n\n\n\n\n\nIn statistics, the Gaussian distribution (also known as the normal distribution) is very important. It has a distinctive bell-shaped curve\nthe central limit theorem: Averages of independent samples have approximately normal distributions As the sample size increases, * The distribution of the averages gets closer to being normally distributed * The width of the sampling distribution gets narrower\nStandard error: * Standard deviation of the sampling distribution * Important tool in understanding sampling variability"
  },
  {
    "objectID": "posts/Sampling Methods/Sampling Methods.html",
    "href": "posts/Sampling Methods/Sampling Methods.html",
    "title": "Sampling Methods",
    "section": "",
    "text": "It’s time to get hands-on and perform the four random sampling methods in Python: simple, systematic, stratified, and cluster.\nThis Sampling Methods is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nAlthough there are several sampling methods such as: * Simple random sampling * Systematic random sampling * Stratified & weight random sampling * Cluster sampling\n\nSimple random sampling: Work like raffle or lottery & consider simplest method of sampling a population. involves picking rows at random, one at a time, where each row has the same chance of being picked as any other\nSystematic random sampling: This samples the population at regular intervals and this method avoid randomness\n\n\n\n\n\nCode\nattrition_pop=pd.read_feather('dataset/attrition.feather')\nattrition_pop.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      Attrition\n      BusinessTravel\n      DailyRate\n      Department\n      DistanceFromHome\n      Education\n      EducationField\n      EnvironmentSatisfaction\n      Gender\n      ...\n      PerformanceRating\n      RelationshipSatisfaction\n      StockOptionLevel\n      TotalWorkingYears\n      TrainingTimesLastYear\n      WorkLifeBalance\n      YearsAtCompany\n      YearsInCurrentRole\n      YearsSinceLastPromotion\n      YearsWithCurrManager\n    \n  \n  \n    \n      0\n      21\n      0.0\n      Travel_Rarely\n      391\n      Research_Development\n      15\n      College\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      6\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      1\n      19\n      1.0\n      Travel_Rarely\n      528\n      Sales\n      22\n      Below_College\n      Marketing\n      Very_High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      2\n      Good\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18\n      1.0\n      Travel_Rarely\n      230\n      Research_Development\n      3\n      Bachelor\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      High\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18\n      0.0\n      Travel_Rarely\n      812\n      Sales\n      10\n      Bachelor\n      Medical\n      Very_High\n      Female\n      ...\n      Excellent\n      Low\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18\n      1.0\n      Travel_Frequently\n      1306\n      Sales\n      5\n      Bachelor\n      Marketing\n      Medium\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      3\n      Better\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 31 columns\n\n\n\n\n\nCode\n# Sample 70 rows using simple random sampling and set the seed\nattrition_samp = attrition_pop.sample(n=70, random_state=18900217)\n\n# Print the sample\nprint(attrition_samp)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1134   35        0.0      Travel_Rarely        583  Research_Development   \n1150   52        0.0         Non-Travel        585                 Sales   \n531    33        0.0      Travel_Rarely        931  Research_Development   \n395    31        0.0      Travel_Rarely       1332  Research_Development   \n392    29        0.0      Travel_Rarely        942  Research_Development   \n...   ...        ...                ...        ...                   ...   \n361    27        0.0  Travel_Frequently       1410                 Sales   \n1180   36        0.0      Travel_Rarely        530                 Sales   \n230    26        0.0      Travel_Rarely       1443                 Sales   \n211    29        0.0  Travel_Frequently        410  Research_Development   \n890    30        0.0  Travel_Frequently       1312  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1134                25         Master           Medical   \n1150                29         Master     Life_Sciences   \n531                 14       Bachelor           Medical   \n395                 11        College           Medical   \n392                 15  Below_College     Life_Sciences   \n...                ...            ...               ...   \n361                  3  Below_College           Medical   \n1180                 2         Master     Life_Sciences   \n230                 23       Bachelor         Marketing   \n211                  2  Below_College     Life_Sciences   \n890                  2         Master  Technical_Degree   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1134                    High  Female  ...          Excellent   \n1150                     Low    Male  ...          Excellent   \n531                Very_High  Female  ...          Excellent   \n395                     High    Male  ...          Excellent   \n392                   Medium  Female  ...          Excellent   \n...                      ...     ...  ...                ...   \n361                Very_High  Female  ...        Outstanding   \n1180                    High  Female  ...          Excellent   \n230                     High  Female  ...          Excellent   \n211                Very_High  Female  ...          Excellent   \n890                Very_High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1134                     High                 1                16   \n1150                   Medium                 2                16   \n531                 Very_High                 1                 8   \n395                 Very_High                 0                 6   \n392                       Low                 1                 6   \n...                       ...               ...               ...   \n361                    Medium                 2                 6   \n1180                     High                 0                17   \n230                      High                 1                 5   \n211                      High                 3                 4   \n890                 Very_High                 0                10   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1134                     3            Good              16   \n1150                     3            Good               9   \n531                      5          Better               8   \n395                      2            Good               6   \n392                      2            Good               5   \n...                    ...             ...             ...   \n361                      3          Better               6   \n1180                     2            Good              13   \n230                      2            Good               2   \n211                      3          Better               3   \n890                      2          Better               9   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1134                  10                       10                    1  \n1150                   8                        0                    0  \n531                    7                        1                    6  \n395                    5                        0                    1  \n392                    4                        1                    3  \n...                  ...                      ...                  ...  \n361                    5                        0                    4  \n1180                   7                        6                    7  \n230                    2                        0                    0  \n211                    2                        0                    2  \n890                    7                        0                    7  \n\n[70 rows x 31 columns]\n\n\n\n\n\n\n\nCode\n# Set the sample size to 70\nsample_size = 70\n\n# Calculate the population size from attrition_pop\npop_size = len(attrition_pop)\n\n# Calculate the interval\ninterval = pop_size // sample_size\n\n# Systematically sample 70 rows\nattrition_sys_samp = attrition_pop.iloc[::interval]\n\n# Print the sample\nprint(attrition_sys_samp)\n\n\n      Age  Attrition BusinessTravel  DailyRate            Department  \\\n0      21        0.0  Travel_Rarely        391  Research_Development   \n21     19        0.0  Travel_Rarely       1181  Research_Development   \n42     45        0.0  Travel_Rarely        252  Research_Development   \n63     23        0.0  Travel_Rarely        373  Research_Development   \n84     30        1.0  Travel_Rarely        945                 Sales   \n...   ...        ...            ...        ...                   ...   \n1365   48        0.0  Travel_Rarely        715  Research_Development   \n1386   48        0.0  Travel_Rarely       1355  Research_Development   \n1407   50        0.0  Travel_Rarely        989  Research_Development   \n1428   50        0.0     Non-Travel        881  Research_Development   \n1449   52        0.0  Travel_Rarely        699  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n0                   15        College  Life_Sciences                    High   \n21                   3  Below_College        Medical                  Medium   \n42                   2       Bachelor  Life_Sciences                  Medium   \n63                   1        College  Life_Sciences               Very_High   \n84                   9       Bachelor        Medical                  Medium   \n...                ...            ...            ...                     ...   \n1365                 1       Bachelor  Life_Sciences               Very_High   \n1386                 4         Master  Life_Sciences                    High   \n1407                 7        College        Medical                  Medium   \n1428                 2         Master  Life_Sciences                     Low   \n1449                 1         Master  Life_Sciences                    High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n0       Male  ...          Excellent                Very_High   \n21    Female  ...          Excellent                Very_High   \n42    Female  ...          Excellent                Very_High   \n63      Male  ...        Outstanding                Very_High   \n84      Male  ...          Excellent                     High   \n...      ...  ...                ...                      ...   \n1365    Male  ...          Excellent                     High   \n1386    Male  ...          Excellent                   Medium   \n1407  Female  ...          Excellent                Very_High   \n1428    Male  ...          Excellent                Very_High   \n1449    Male  ...          Excellent                      Low   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n0                    0                 0                     6   \n21                   0                 1                     3   \n42                   0                 1                     3   \n63                   1                 1                     2   \n84                   0                 1                     3   \n...                ...               ...                   ...   \n1365                 0                25                     3   \n1386                 0                27                     3   \n1407                 1                29                     2   \n1428                 1                31                     3   \n1449                 1                34                     5   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n0             Better               0                   0   \n21            Better               1                   0   \n42            Better               1                   0   \n63            Better               1                   0   \n84              Good               1                   0   \n...              ...             ...                 ...   \n1365            Best               1                   0   \n1386          Better              15                  11   \n1407            Good              27                   3   \n1428          Better              31                   6   \n1449          Better              33                  18   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n0                           0                    0  \n21                          0                    0  \n42                          0                    0  \n63                          0                    1  \n84                          0                    0  \n...                       ...                  ...  \n1365                        0                    0  \n1386                        4                    8  \n1407                       13                    8  \n1428                       14                    7  \n1449                       11                    9  \n\n[70 rows x 31 columns]\n\n\n\n\n\nIn the case of systematic sampling, there is a problem: if the data has been sorted or there is a pattern or meaning behind the row order, then the resulting sample may not be representative of the entire population. If the rows are shuffled, the problem can be solved, but then systematic sampling becomes equivalent to simple random sampling.\n\n\nCode\n# Add an index column to attrition_pop\nattrition_pop_id = attrition_pop.reset_index()\n\n# Plot YearsAtCompany vs. index for attrition_pop_id\nattrition_pop_id.plot(x='index',y='YearsAtCompany',kind='scatter')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Shuffle the rows of attrition_pop\nattrition_shuffled = attrition_pop.sample(frac=1)\n\n# Reset the row indexes and create an index column\nattrition_shuffled = attrition_shuffled.reset_index(drop=True).reset_index()\n\n# Plot YearsAtCompany vs. index for attrition_shuffled\nattrition_shuffled.plot(x='index',y='YearsAtCompany',kind='scatter')\nplt.show()\n\n\n\n\n\n\n\n\nStratified sampling is a technique that allows us to sample a population that contains subgroups\n\nWeighted random sampling\n\nA close relative of stratified sampling that provides even more flexibility is weighted random sampling. In this variant, we create a column of weights that adjust the relative probability of sampling each row.\n\n\n\nYou may need to carefully control the counts of each subgroup within the population if you are interested in subgroups within the population. As a result of proportional stratified sampling, the subgroup sizes within the sample are representative of the subgroup sizes within the population as a whole.\n\n\nCode\n# Proportion of employees by Education level\neducation_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n\n# Print education_counts_pop\nprint(education_counts_pop)\n\n\nBachelor         0.389116\nMaster           0.270748\nCollege          0.191837\nBelow_College    0.115646\nDoctor           0.032653\nName: Education, dtype: float64\n\n\n\n\nCode\n# Proportional stratified sampling for 40% of each Education group\nattrition_strat = attrition_pop.groupby('Education').sample(frac=0.4, random_state=2022)\n\n# Print the sample\nprint(attrition_strat)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1191   53        0.0      Travel_Rarely        238                 Sales   \n407    29        0.0  Travel_Frequently        995  Research_Development   \n1233   59        0.0  Travel_Frequently       1225                 Sales   \n366    37        0.0      Travel_Rarely        571  Research_Development   \n702    31        0.0  Travel_Frequently        163  Research_Development   \n...   ...        ...                ...        ...                   ...   \n733    38        0.0  Travel_Frequently        653  Research_Development   \n1061   44        0.0  Travel_Frequently        602       Human_Resources   \n1307   41        0.0      Travel_Rarely       1276                 Sales   \n1060   33        0.0      Travel_Rarely        516  Research_Development   \n177    29        0.0      Travel_Rarely        738  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1191                 1  Below_College           Medical   \n407                  2  Below_College     Life_Sciences   \n1233                 1  Below_College     Life_Sciences   \n366                 10  Below_College     Life_Sciences   \n702                 24  Below_College  Technical_Degree   \n...                ...            ...               ...   \n733                 29         Doctor     Life_Sciences   \n1061                 1         Doctor   Human_Resources   \n1307                 2         Doctor     Life_Sciences   \n1060                 8         Doctor     Life_Sciences   \n177                  9         Doctor             Other   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1191               Very_High  Female  ...        Outstanding   \n407                      Low    Male  ...          Excellent   \n1233                     Low  Female  ...          Excellent   \n366                Very_High  Female  ...          Excellent   \n702                Very_High  Female  ...        Outstanding   \n...                      ...     ...  ...                ...   \n733                Very_High  Female  ...          Excellent   \n1061                     Low    Male  ...          Excellent   \n1307                  Medium  Female  ...          Excellent   \n1060               Very_High    Male  ...          Excellent   \n177                   Medium    Male  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1191                Very_High                 0                18   \n407                 Very_High                 1                 6   \n1233                Very_High                 0                20   \n366                    Medium                 2                 6   \n702                 Very_High                 0                 9   \n...                       ...               ...               ...   \n733                 Very_High                 0                10   \n1061                     High                 0                14   \n1307                   Medium                 1                22   \n1060                      Low                 0                14   \n177                      High                 0                 4   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1191                     2            Best              14   \n407                      0            Best               6   \n1233                     2            Good               4   \n366                      3            Good               5   \n702                      3            Good               5   \n...                    ...             ...             ...   \n733                      2          Better              10   \n1061                     3          Better              10   \n1307                     2          Better              18   \n1060                     6          Better               0   \n177                      2          Better               3   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1191                   7                        8                   10  \n407                    4                        1                    3  \n1233                   3                        1                    3  \n366                    3                        4                    3  \n702                    4                        1                    4  \n...                  ...                      ...                  ...  \n733                    3                        9                    9  \n1061                   7                        0                    2  \n1307                  16                       11                    8  \n1060                   0                        0                    0  \n177                    2                        2                    2  \n\n[588 rows x 31 columns]\n\n\n\n\nCode\n# Calculate the Education level proportions from attrition_strat\neducation_counts_strat = attrition_strat['Education'].value_counts(normalize=True)\n\n# Print education_counts_strat\nprint(education_counts_strat)\nprint('\\nBy grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population.')\n\n\nBachelor         0.389456\nMaster           0.270408\nCollege          0.192177\nBelow_College    0.115646\nDoctor           0.032313\nName: Education, dtype: float64\n\nBy grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population.\n\n\n\n\n\nWhen one subgroup is larger than another in the population, but you do not want to factor this difference into your analysis, you can use equal counts stratified sampling to generate samples in which each subgroup has the same amount of data.\n\n\nCode\n# Get 30 employees from each Education group\nattrition_eq = attrition_pop.groupby('Education').sample(n=30, random_state=2022)\n\n# Print the sample\nprint(attrition_eq)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1191   53        0.0      Travel_Rarely        238                 Sales   \n407    29        0.0  Travel_Frequently        995  Research_Development   \n1233   59        0.0  Travel_Frequently       1225                 Sales   \n366    37        0.0      Travel_Rarely        571  Research_Development   \n702    31        0.0  Travel_Frequently        163  Research_Development   \n...   ...        ...                ...        ...                   ...   \n774    33        0.0      Travel_Rarely        922  Research_Development   \n869    45        0.0      Travel_Rarely       1015  Research_Development   \n530    32        0.0      Travel_Rarely        120  Research_Development   \n1049   48        0.0      Travel_Rarely        163                 Sales   \n350    29        1.0      Travel_Rarely        408  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1191                 1  Below_College           Medical   \n407                  2  Below_College     Life_Sciences   \n1233                 1  Below_College     Life_Sciences   \n366                 10  Below_College     Life_Sciences   \n702                 24  Below_College  Technical_Degree   \n...                ...            ...               ...   \n774                  1         Doctor           Medical   \n869                  5         Doctor           Medical   \n530                  6         Doctor     Life_Sciences   \n1049                 2         Doctor         Marketing   \n350                 25         Doctor  Technical_Degree   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1191               Very_High  Female  ...        Outstanding   \n407                      Low    Male  ...          Excellent   \n1233                     Low  Female  ...          Excellent   \n366                Very_High  Female  ...          Excellent   \n702                Very_High  Female  ...        Outstanding   \n...                      ...     ...  ...                ...   \n774                      Low  Female  ...          Excellent   \n869                     High  Female  ...          Excellent   \n530                     High    Male  ...        Outstanding   \n1049                  Medium  Female  ...          Excellent   \n350                     High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1191                Very_High                 0                18   \n407                 Very_High                 1                 6   \n1233                Very_High                 0                20   \n366                    Medium                 2                 6   \n702                 Very_High                 0                 9   \n...                       ...               ...               ...   \n774                      High                 1                10   \n869                       Low                 0                10   \n530                       Low                 0                 8   \n1049                      Low                 1                14   \n350                    Medium                 0                 6   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1191                     2            Best              14   \n407                      0            Best               6   \n1233                     2            Good               4   \n366                      3            Good               5   \n702                      3            Good               5   \n...                    ...             ...             ...   \n774                      2          Better               6   \n869                      3          Better              10   \n530                      2          Better               5   \n1049                     2          Better               9   \n350                      2            Best               2   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1191                   7                        8                   10  \n407                    4                        1                    3  \n1233                   3                        1                    3  \n366                    3                        4                    3  \n702                    4                        1                    4  \n...                  ...                      ...                  ...  \n774                    1                        0                    5  \n869                    7                        1                    4  \n530                    4                        1                    4  \n1049                   7                        6                    7  \n350                    2                        1                    1  \n\n[150 rows x 31 columns]\n\n\n\n\nCode\n# Get the proportions from attrition_eq\neducation_counts_eq = attrition_eq['Education'].value_counts(normalize=True)\n\n# Print the results\nprint(education_counts_eq)\n\n\nBelow_College    0.2\nCollege          0.2\nBachelor         0.2\nMaster           0.2\nDoctor           0.2\nName: Education, dtype: float64\n\n\n\n\n\nThe stratified sampling method determines the probability of picking rows from your dataset based on the subgroups within your dataset. A generalization of this is weighted sampling, which allows you to specify rules regarding the probability of selecting rows at the row level. A row’s probability of being selected is proportional to its weight value.\n\n\nCode\n# Plot YearsAtCompany from attrition_pop as a histogram\nattrition_pop['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Sample 400 employees weighted by YearsAtCompany\nattrition_weight = attrition_pop.sample(n=400, weights='YearsAtCompany')\n\n# Print the sample\nprint(attrition_weight)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n853    36        0.0      Travel_Rarely        172  Research_Development   \n481    34        0.0  Travel_Frequently        618  Research_Development   \n1148   38        0.0      Travel_Rarely       1321                 Sales   \n1430   51        0.0  Travel_Frequently        237                 Sales   \n517    39        0.0      Travel_Rarely        835  Research_Development   \n...   ...        ...                ...        ...                   ...   \n1351   45        0.0      Travel_Rarely       1038  Research_Development   \n1412   54        0.0      Travel_Rarely        971  Research_Development   \n1248   47        0.0  Travel_Frequently       1379  Research_Development   \n1210   36        0.0  Travel_Frequently        688  Research_Development   \n1328   55        0.0  Travel_Frequently       1091  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n853                  4         Master  Life_Sciences                     Low   \n481                  3  Below_College  Life_Sciences                     Low   \n1148                 1         Master  Life_Sciences               Very_High   \n1430                 9       Bachelor  Life_Sciences               Very_High   \n517                 19         Master          Other               Very_High   \n...                ...            ...            ...                     ...   \n1351                20       Bachelor        Medical                  Medium   \n1412                 1       Bachelor        Medical               Very_High   \n1248                16         Master        Medical                    High   \n1210                 4        College  Life_Sciences               Very_High   \n1328                 2  Below_College  Life_Sciences               Very_High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n853     Male  ...          Excellent                     High   \n481     Male  ...          Excellent                     High   \n1148    Male  ...          Excellent                      Low   \n1430    Male  ...        Outstanding                      Low   \n517     Male  ...          Excellent                   Medium   \n...      ...  ...                ...                      ...   \n1351    Male  ...          Excellent                   Medium   \n1412  Female  ...          Excellent                Very_High   \n1248    Male  ...          Excellent                     High   \n1210  Female  ...          Excellent                   Medium   \n1328    Male  ...          Excellent                   Medium   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n853                  0                10                     2   \n481                  0                 7                     1   \n1148                 2                16                     3   \n1430                 1                31                     5   \n517                  3                 7                     2   \n...                ...               ...                   ...   \n1351                 1                24                     2   \n1412                 0                29                     3   \n1248                 0                20                     3   \n1210                 3                18                     3   \n1328                 1                23                     4   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n853             Good              10                   4   \n481             Good               6                   2   \n1148          Better              15                  13   \n1430            Good              29                  10   \n517           Better               2                   2   \n...              ...             ...                 ...   \n1351          Better               7                   7   \n1412            Good              20                   7   \n1248            Best              19                  10   \n1210          Better               4                   2   \n1328          Better               3                   2   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n853                         1                    8  \n481                         0                    4  \n1148                        5                    8  \n1430                       11                   10  \n517                         2                    2  \n...                       ...                  ...  \n1351                        0                    7  \n1412                       12                    7  \n1248                        2                    7  \n1210                        0                    2  \n1328                        1                    2  \n\n[400 rows x 31 columns]\n\n\n\n\nCode\n# Plot YearsAtCompany from attrition_weight as a histogram\nattrition_weight['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Plot YearsAtCompany from attrition_pop as a histogram\nattrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\nplt.show()\n\n# Sample 400 employees weighted by YearsAtCompany\nattrition_weight = attrition_pop.sample(n=400, weights=\"YearsAtCompany\")\n\n# Plot YearsAtCompany from attrition_weight as a histogram\nattrition_weight['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStratified sampling vs. cluster sampling * Stratified sampling: * Split the population into subgroups * Use simple random sampling on every subgroup * Cluster sampling * Use simple random sampling to pick some subgroups * Use simple random sampling on only those subgroups\n\n\nCode\nimport random\n# Create a list of unique JobRole values\njob_roles_pop = list(attrition_pop['JobRole'].unique())\n\n# Randomly sample four JobRole values\njob_roles_samp = random.sample(job_roles_pop,k=4)\n\n# Print the result\nprint(job_roles_samp)\n\n\n['Research_Director', 'Sales_Executive', 'Sales_Representative', 'Laboratory_Technician']\n\n\n\n\nCode\n# Filter for rows where JobRole is in job_roles_samp\njobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\nattrition_filtered = attrition_pop[jobrole_condition]\n\n# Print the result\nprint(attrition_filtered)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1      19        1.0      Travel_Rarely        528                 Sales   \n2      18        1.0      Travel_Rarely        230  Research_Development   \n3      18        0.0      Travel_Rarely        812                 Sales   \n4      18        1.0  Travel_Frequently       1306                 Sales   \n7      18        1.0         Non-Travel        247  Research_Development   \n...   ...        ...                ...        ...                   ...   \n1457   55        0.0      Travel_Rarely        692  Research_Development   \n1458   56        0.0  Travel_Frequently        906                 Sales   \n1459   54        0.0      Travel_Rarely        685  Research_Development   \n1467   58        0.0      Travel_Rarely        682                 Sales   \n1469   58        1.0      Travel_Rarely        286  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n1                   22  Below_College      Marketing               Very_High   \n2                    3       Bachelor  Life_Sciences                    High   \n3                   10       Bachelor        Medical               Very_High   \n4                    5       Bachelor      Marketing                  Medium   \n7                    8  Below_College        Medical                    High   \n...                ...            ...            ...                     ...   \n1457                14         Master        Medical                    High   \n1458                 6       Bachelor  Life_Sciences                    High   \n1459                 3       Bachelor  Life_Sciences               Very_High   \n1467                10         Master        Medical               Very_High   \n1469                 2         Master  Life_Sciences               Very_High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n1       Male  ...          Excellent                Very_High   \n2       Male  ...          Excellent                     High   \n3     Female  ...          Excellent                      Low   \n4       Male  ...          Excellent                Very_High   \n7       Male  ...          Excellent                Very_High   \n...      ...  ...                ...                      ...   \n1457    Male  ...          Excellent                Very_High   \n1458  Female  ...          Excellent                Very_High   \n1459    Male  ...          Excellent                      Low   \n1467    Male  ...          Excellent                     High   \n1469    Male  ...          Excellent                Very_High   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n1                    0                 0                     2   \n2                    0                 0                     2   \n3                    0                 0                     2   \n4                    0                 0                     3   \n7                    0                 0                     0   \n...                ...               ...                   ...   \n1457                 0                36                     3   \n1458                 3                36                     0   \n1459                 0                36                     2   \n1467                 0                38                     1   \n1469                 0                40                     2   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n1               Good               0                   0   \n2             Better               0                   0   \n3             Better               0                   0   \n4             Better               0                   0   \n7             Better               0                   0   \n...              ...             ...                 ...   \n1457          Better              24                  15   \n1458            Good               7                   7   \n1459          Better              10                   9   \n1467            Good              37                  10   \n1469          Better              31                  15   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n1                           0                    0  \n2                           0                    0  \n3                           0                    0  \n4                           0                    0  \n7                           0                    0  \n...                       ...                  ...  \n1457                        2                   15  \n1458                        7                    7  \n1459                        0                    9  \n1467                        1                    8  \n1469                       13                    8  \n\n[748 rows x 31 columns]\n\n\n\n\nCode\n# Remove categories with no rows\nattrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n\n# Randomly sample 10 employees from each sampled job role\nattrition_clust = attrition_filtered.groupby('JobRole').sample(n=10,random_state=2022)\n\n# Print the sample\nprint(attrition_clust)\n\nprint(\"\\n The two-stage sampling technique gives you control over sampling both between subgroups and within subgroups.\")\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1124   36        0.0      Travel_Rarely       1396  Research_Development   \n576    45        0.0      Travel_Rarely        974  Research_Development   \n995    42        0.0  Travel_Frequently        748  Research_Development   \n1243   50        0.0      Travel_Rarely       1207  Research_Development   \n869    45        0.0      Travel_Rarely       1015  Research_Development   \n599    33        0.0      Travel_Rarely       1099  Research_Development   \n117    24        0.0      Travel_Rarely        350  Research_Development   \n472    30        0.0      Travel_Rarely        921  Research_Development   \n149    27        0.0         Non-Travel       1277  Research_Development   \n49     20        1.0      Travel_Rarely        129  Research_Development   \n1302   40        0.0      Travel_Rarely       1416  Research_Development   \n1126   42        0.0      Travel_Rarely        810  Research_Development   \n1216   38        0.0      Travel_Rarely        849  Research_Development   \n1362   43        0.0      Travel_Rarely        982  Research_Development   \n1327   46        0.0      Travel_Rarely        430  Research_Development   \n664    27        0.0      Travel_Rarely        269  Research_Development   \n1284   40        0.0      Travel_Rarely       1308  Research_Development   \n1440   50        0.0  Travel_Frequently        333  Research_Development   \n790    36        0.0         Non-Travel        427  Research_Development   \n1432   55        0.0      Travel_Rarely       1136  Research_Development   \n941    36        0.0      Travel_Rarely        329                 Sales   \n454    27        0.0  Travel_Frequently       1242                 Sales   \n460    37        0.0      Travel_Rarely        228                 Sales   \n636    45        0.0      Travel_Rarely       1268                 Sales   \n293    33        0.0  Travel_Frequently        430                 Sales   \n976    39        1.0      Travel_Rarely       1162                 Sales   \n813    30        0.0      Travel_Rarely        231                 Sales   \n288    35        1.0  Travel_Frequently        662                 Sales   \n1111   53        1.0      Travel_Rarely       1168                 Sales   \n1075   40        0.0      Travel_Rarely        630                 Sales   \n133    34        1.0  Travel_Frequently        296                 Sales   \n725    36        1.0      Travel_Rarely       1218                 Sales   \n4      18        1.0  Travel_Frequently       1306                 Sales   \n169    41        1.0      Travel_Rarely       1356                 Sales   \n1      19        1.0      Travel_Rarely        528                 Sales   \n48     19        1.0      Travel_Rarely        419                 Sales   \n150    31        0.0  Travel_Frequently        793                 Sales   \n130    21        0.0         Non-Travel        895                 Sales   \n3      18        0.0      Travel_Rarely        812                 Sales   \n99     31        0.0  Travel_Frequently        444                 Sales   \n\n      DistanceFromHome      Education    EducationField  \\\n1124                 5        College     Life_Sciences   \n576                  1         Master           Medical   \n995                  9        College           Medical   \n1243                28  Below_College           Medical   \n869                  5         Doctor           Medical   \n599                  4         Master           Medical   \n117                 21        College  Technical_Degree   \n472                  1       Bachelor     Life_Sciences   \n149                  8         Doctor     Life_Sciences   \n49                   4       Bachelor  Technical_Degree   \n1302                 2        College           Medical   \n1126                23         Doctor     Life_Sciences   \n1216                25        College     Life_Sciences   \n1362                12       Bachelor     Life_Sciences   \n1327                 1         Master           Medical   \n664                  5  Below_College  Technical_Degree   \n1284                14       Bachelor           Medical   \n1440                22         Doctor           Medical   \n790                  8       Bachelor     Life_Sciences   \n1432                 1         Master           Medical   \n941                 16         Master         Marketing   \n454                 20       Bachelor     Life_Sciences   \n460                  6         Master           Medical   \n636                  4        College     Life_Sciences   \n293                  7       Bachelor           Medical   \n976                  3        College           Medical   \n813                  8        College             Other   \n288                 18         Master         Marketing   \n1111                24         Master     Life_Sciences   \n1075                 4         Master         Marketing   \n133                  6        College         Marketing   \n725                  9         Master     Life_Sciences   \n4                    5       Bachelor         Marketing   \n169                 20        College         Marketing   \n1                   22  Below_College         Marketing   \n48                  21       Bachelor             Other   \n150                 20       Bachelor     Life_Sciences   \n130                  9        College           Medical   \n3                   10       Bachelor           Medical   \n99                   5       Bachelor         Marketing   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1124               Very_High    Male  ...          Excellent   \n576                Very_High  Female  ...          Excellent   \n995                      Low  Female  ...          Excellent   \n1243               Very_High    Male  ...          Excellent   \n869                     High  Female  ...          Excellent   \n599                      Low  Female  ...          Excellent   \n117                     High    Male  ...          Excellent   \n472                Very_High    Male  ...        Outstanding   \n149                      Low    Male  ...          Excellent   \n49                       Low    Male  ...          Excellent   \n1302                     Low    Male  ...          Excellent   \n1126                     Low  Female  ...          Excellent   \n1216                     Low  Female  ...          Excellent   \n1362                     Low    Male  ...          Excellent   \n1327               Very_High    Male  ...          Excellent   \n664                     High    Male  ...          Excellent   \n1284                    High    Male  ...          Excellent   \n1440                    High    Male  ...          Excellent   \n790                      Low  Female  ...          Excellent   \n1432                  Medium    Male  ...          Excellent   \n941                     High  Female  ...          Excellent   \n454                Very_High  Female  ...          Excellent   \n460                     High    Male  ...          Excellent   \n636                     High  Female  ...          Excellent   \n293                Very_High    Male  ...          Excellent   \n976                Very_High  Female  ...          Excellent   \n813                     High    Male  ...          Excellent   \n288                Very_High  Female  ...          Excellent   \n1111                     Low    Male  ...          Excellent   \n1075                    High    Male  ...          Excellent   \n133                Very_High  Female  ...          Excellent   \n725                     High    Male  ...        Outstanding   \n4                     Medium    Male  ...          Excellent   \n169                   Medium  Female  ...        Outstanding   \n1                  Very_High    Male  ...          Excellent   \n48                 Very_High    Male  ...          Excellent   \n150                     High    Male  ...          Excellent   \n130                      Low    Male  ...        Outstanding   \n3                  Very_High  Female  ...          Excellent   \n99                 Very_High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1124                Very_High                 0                16   \n576                 Very_High                 2                 8   \n995                      High                 0                12   \n1243                     High                 3                20   \n869                       Low                 0                10   \n599                 Very_High                 0                 8   \n117                    Medium                 3                 2   \n472                      High                 2                 7   \n149                 Very_High                 3                 3   \n49                     Medium                 0                 1   \n1302                Very_High                 1                22   \n1126                   Medium                 0                16   \n1216                     High                 1                19   \n1362                     High                 1                25   \n1327                Very_High                 2                23   \n664                    Medium                 1                 9   \n1284                      Low                 0                21   \n1440                Very_High                 0                32   \n790                       Low                 1                10   \n1432                Very_High                 2                31   \n941                       Low                 2                11   \n454                 Very_High                 0                 7   \n460                    Medium                 1                 7   \n636                       Low                 1                 9   \n293                       Low                 2                 5   \n976                       Low                 0                12   \n813                       Low                 1                10   \n288                      High                 1                 5   \n1111                   Medium                 0                15   \n1075                      Low                 1                15   \n133                 Very_High                 1                 3   \n725                    Medium                 0                10   \n4                   Very_High                 0                 0   \n169                 Very_High                 0                 4   \n1                   Very_High                 0                 0   \n48                     Medium                 0                 1   \n150                       Low                 1                 3   \n130                      High                 0                 3   \n3                         Low                 0                 0   \n99                       High                 1                 2   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1124                     3            Best              13   \n576                      2          Better               5   \n995                      3          Better              12   \n1243                     3          Better              20   \n869                      3          Better              10   \n599                      5          Better               5   \n117                      3          Better               1   \n472                      2          Better               2   \n149                      4          Better               3   \n49                       2          Better               1   \n1302                     5          Better              21   \n1126                     2          Better               1   \n1216                     2          Better              10   \n1362                     3          Better              25   \n1327                     0          Better               2   \n664                      3          Better               9   \n1284                     2            Best              20   \n1440                     2          Better              32   \n790                      2          Better               8   \n1432                     4            Best               7   \n941                      3            Good               3   \n454                      2          Better               7   \n460                      5            Best               5   \n636                      3            Best               5   \n293                      2          Better               4   \n976                      3            Good               1   \n813                      2            Best               8   \n288                      0            Good               4   \n1111                     2            Good               2   \n1075                     2            Good              12   \n133                      3            Good               2   \n725                      4          Better               5   \n4                        3          Better               0   \n169                      5            Good               4   \n1                        2            Good               0   \n48                       3            Best               1   \n150                      4          Better               2   \n130                      3            Good               3   \n3                        2          Better               0   \n99                       5            Good               2   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1124                  11                        3                    7  \n576                    3                        0                    2  \n995                    9                        5                    8  \n1243                   8                        3                    8  \n869                    7                        1                    4  \n599                    4                        0                    2  \n117                    1                        0                    0  \n472                    2                        0                    2  \n149                    2                        1                    2  \n49                     0                        0                    0  \n1302                   7                        3                    9  \n1126                   0                        0                    0  \n1216                   8                        0                    1  \n1362                  10                        3                    9  \n1327                   2                        2                    2  \n664                    8                        0                    8  \n1284                   7                        4                    9  \n1440                   6                       13                    9  \n790                    7                        0                    5  \n1432                   7                        0                    0  \n941                    2                        0                    2  \n454                    7                        0                    7  \n460                    4                        0                    1  \n636                    4                        0                    3  \n293                    3                        0                    3  \n976                    0                        0                    0  \n813                    4                        7                    7  \n288                    2                        3                    2  \n1111                   2                        2                    2  \n1075                  11                        2                   11  \n133                    2                        1                    0  \n725                    3                        0                    3  \n4                      0                        0                    0  \n169                    3                        0                    2  \n1                      0                        0                    0  \n48                     0                        0                    0  \n150                    2                        2                    2  \n130                    2                        2                    2  \n3                      0                        0                    0  \n99                     2                        2                    2  \n\n[40 rows x 31 columns]\n\n The two-stage sampling technique gives you control over sampling both between subgroups and within subgroups.\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_28748\\2564666783.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n\n\n\n\n\nYou’re going to compare the performance of point estimates using simple, stratified, and cluster sampling. Before doing that, you’ll have to set up the samples\n\n\nCode\n# Perform simple random sampling to get 0.25 of the population\nattrition_srs = attrition_pop.sample(frac=1/4, random_state=2022)\nattrition_srs.shape\n\n\n(368, 31)\n\n\n\n\nCode\n# Perform stratified sampling to get 0.25 of each relationship group\nattrition_strat = attrition_pop.groupby('RelationshipSatisfaction').sample(frac=1/4, random_state=2022)\nattrition_strat.shape\n\n\n(368, 31)\n\n\n\n\nCode\n# Create a list of unique RelationshipSatisfaction values\nsatisfaction_unique = list(attrition_pop['RelationshipSatisfaction'].unique())\n\n# Randomly sample 2 unique satisfaction values\nsatisfaction_samp = random.sample(satisfaction_unique, k=2)\n\n# Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction\nsatis_condition = attrition_pop['RelationshipSatisfaction'].isin(satisfaction_samp)\nattrition_clust_prep = attrition_pop[satis_condition]\nattrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n\n# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop\nattrition_clust = attrition_clust_prep.groupby(\"RelationshipSatisfaction\")\\\n    .sample(n=len(attrition_pop) // 4, random_state=2022)\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_28748\\1225069142.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  attrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n\n\nValueError: Cannot take a larger sample than population when 'replace=False'\n\n\n\n\n\n\n\nCode\n# Mean Attrition by RelationshipSatisfaction group\nmean_attrition_pop = attrition_pop.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_pop)\n\n\nRelationshipSatisfaction\nLow          0.206522\nMedium       0.148515\nHigh         0.154684\nVery_High    0.148148\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the simple random sample\nmean_attrition_srs = attrition_srs.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_srs)\n\n\nRelationshipSatisfaction\nLow          0.134328\nMedium       0.164179\nHigh         0.160000\nVery_High    0.155963\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the stratified sample\nmean_attrition_strat = attrition_strat.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_strat)\n\n\nRelationshipSatisfaction\nLow          0.144928\nMedium       0.078947\nHigh         0.165217\nVery_High    0.129630\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the cluster sample\nmean_attrition_clust = attrition_clust.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_clust)\n\n\nRelationshipSatisfaction\nLow          0.090909\nMedium       0.500000\nHigh         0.125000\nVery_High    0.307692\nName: Attrition, dtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html",
    "title": "Simple Linear Regression Modeling",
    "section": "",
    "text": "We will learn the basics of this popular statistical model, what regression is, and how linear and logistic regressions differ. We’ll then learn how to fit simple linear regression models with numeric and categorical explanatory variables, and how to describe the relationship between the response and explanatory variables using model coefficients\nThis Simple Linear Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#fitting-a-linear-regression",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#fitting-a-linear-regression",
    "title": "Simple Linear Regression Modeling",
    "section": "Fitting a linear regression",
    "text": "Fitting a linear regression\nStraight lines are defined by two things:\n\nIntercept: The y value at the point when x is zero.\nSlope: The amount the y value increases if you increase x by one.\nEquation: y = intercept + slope ∗ x"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#linear-regression-with-ols",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#linear-regression-with-ols",
    "title": "Simple Linear Regression Modeling",
    "section": "Linear regression with ols()",
    "text": "Linear regression with ols()\nWhile sns.regplot() can display a linear regression trend line, it doesn’t give you access to the intercept and slope as variables, or allow you to work with the model results as variables. That means that sometimes you’ll need to run a linear regression yourself.\n\n\nCode\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n# Create the model object\nmdl_price_vs_conv = ols(\"price_twd_msq ~ n_convenience\", data=taiwan_real_estate)\n\n# Fit the model\nmdl_price_vs_conv = mdl_price_vs_conv.fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_conv.params)\n\n\nIntercept        8.224237\nn_convenience    0.798080\ndtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#categorical-explanatory-variables",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#categorical-explanatory-variables",
    "title": "Simple Linear Regression Modeling",
    "section": "Categorical explanatory variables",
    "text": "Categorical explanatory variables\nVariables that categorize observations are known as categorical variables. Known as levels, they have a limited number of values. Gender is a categorical variable that can take two levels: Male or Female.\nNumbers are required for regression analysis. It is therefore necessary to make the results interpretable when a categorical variable is included in a regression model.\nA set of binary variables is created by recoding categorical variables. The recoding process creates a contrast matrix table by “dummy coding”\nThere are two type of data variables: * Quantitative data: refers to amount * Data collected quantitatively represents actual amounts that can be added, subtracted, divided, etc. Quantitative variables can be: * discrete (integer variables): count of individual items in record e.g. No. of players * continuous (ratio variables): continuous / non-finite value measurements e.g. distance, age etc * Categorical: refers to grouping There are three types of categorical variables: * binary: yes / no e.g. head/tail of coin flip * nominal: group with no rank or order b/w them e.g. color, brand, species etc * ordinal: group that can be ranked in specific order e.g. rating scale in survey result"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#visualizing-numeric-vs.-categorical",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#visualizing-numeric-vs.-categorical",
    "title": "Simple Linear Regression Modeling",
    "section": "Visualizing numeric vs. categorical",
    "text": "Visualizing numeric vs. categorical\nIf the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn’t make sense. Instead, a good option is to draw a histogram for each category.\n\n\nCode\n# Histograms of price_twd_msq with 10 bins, split by the age of each house\nsns.displot(data=taiwan_real_estate,\n         x='price_twd_msq',\n         col='house_age_years',\n         bins=10)\n\n# Show the plot\nplt.show()\nprint(\"\\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest.\")\n\n\n\n\n\n\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest."
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#using-categories-to-calculate-means",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#using-categories-to-calculate-means",
    "title": "Simple Linear Regression Modeling",
    "section": "Using categories to calculate means",
    "text": "Using categories to calculate means\nUsing summary statistics for each category is a good way to explore categorical variables further. Using a categorical variable, you can calculate the mean and median of your response variable. Therefore, you can compare each category in more detail.\n\n\nCode\n# Calculate the mean of price_twd_msq, grouped by house age\nmean_price_by_age = taiwan_real_estate.groupby('house_age_years')['price_twd_msq'].mean()\n\n# Print the result\nprint(mean_price_by_age)\n\n\nhouse_age_years\n0 to 15     12.637471\n15 to 30     9.876743\n30 to 45    11.393264\nName: price_twd_msq, dtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#is-coefficient-of-linear-regression-model-is-mean-of-each-category",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#is-coefficient-of-linear-regression-model-is-mean-of-each-category",
    "title": "Simple Linear Regression Modeling",
    "section": "Is coefficient of linear regression model is mean of each category?",
    "text": "Is coefficient of linear regression model is mean of each category?\nWhile calculating linear regression with categorical explanatory variable, means of each category will also coefficient of linear regression but this hold true in case with only one categorical variable. Lets verify this\n\n\nCode\n# Create the model, fit it\nmdl_price_vs_age = ols(\"price_twd_msq ~ house_age_years\", data=taiwan_real_estate).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_age.params)\n\n\nIntercept                      12.637471\nhouse_age_years[T.15 to 30]    -2.760728\nhouse_age_years[T.30 to 45]    -1.244207\ndtype: float64\n\n\n\n\nCode\n# Update the model formula to remove the intercept\nmdl_price_vs_age0 = ols(\"price_twd_msq ~ house_age_years + 0\", data=taiwan_real_estate).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_age0.params)\nprint(\"\\n The coefficients of the model are just the means of each category you calculated previously. Fantastic job! \")\n\n\nhouse_age_years[0 to 15]     12.637471\nhouse_age_years[15 to 30]     9.876743\nhouse_age_years[30 to 45]    11.393264\ndtype: float64\n\n The coefficients of the model are just the means of each category you calculated previously. Fantastic job!"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html",
    "title": "Simple Logistic Regression Modeling",
    "section": "",
    "text": "Get a better understanding of logistic regression models. We will analyze real-world data to predict the likelihood of a customer closing their bank account in terms of probabilities of success and odds ratios, and quantify the performance of your model using confusion matrices.\nThis Simple Logistic Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression",
    "title": "Simple Logistic Regression Modeling",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nHow would the response variable be affected if it were binary or logical? It can be Yes/No, 1/0, Blue/Red, etc.\nFor categorical responses, a logistic regression model is another type of generalized linear model.\nAn S curve is drawn to represent the response. Probabilities can be considered to be the fitted values between 0 and 1."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#exploring-the-explanatory-variables",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#exploring-the-explanatory-variables",
    "title": "Simple Logistic Regression Modeling",
    "section": "Exploring the explanatory variables",
    "text": "Exploring the explanatory variables\nIn the case of a logical response variable, all points lie on the y=0 and y=1 lines, making it difficult to determine what is occurring. It was unclear how the explanatory variable was distributed on each line before you saw the trend line. A histogram of the explanatory variable, grouped by the response, can be used to resolve this problem.\nThese histograms will be used to gain an understanding of the financial services churn dataset\n\n\nCode\nchurn = pd.read_csv('dataset/churn.csv')\nprint(churn.head())\n\n\n   has_churned  time_since_first_purchase  time_since_last_purchase\n0            0                  -1.089221                 -0.721322\n1            0                   1.182983                  3.634435\n2            0                  -0.846156                 -0.427582\n3            0                   0.086942                 -0.535672\n4            0                  -1.166642                 -0.672640\n\n\n\n\nCode\n# Create the histograms of time_since_last_purchase split by has_churned\nsns.displot(data=churn,x='time_since_last_purchase',col='has_churned')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Redraw the plot with time_since_first_purchase\nsns.displot(data=churn,x='time_since_first_purchase', col='has_churned')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create the histograms of time_since_last_purchase split by has_churned\nsns.distplot(churn['time_since_last_purchase'])\n\n# Show the plot\nplt.show()\n\nprint(churn['has_churned'].unique())\n\nfor x in churn['has_churned'].unique():\n    values = churn.time_since_last_purchase[churn['has_churned'] == x]\n    sns.distplot(values, hist=False, rug=False)\n\ng = sns.FacetGrid(churn, col='has_churned', hue='has_churned')\np1 = g.map(sns.distplot, 'time_since_last_purchase', hist=True, rug=True).add_legend()\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(churn['time_since_last_purchase'])\n\n\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\n\n\n[0 1]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\nCode\n# Create the histograms of time_since_first_purchase split by has_churned\nsns.distplot(churn['time_since_first_purchase'])\n\n# Show the plot\nplt.show()\n\nprint(churn['has_churned'].unique())\n\nfor x in churn['has_churned'].unique():\n    values = churn.time_since_last_purchase[churn['has_churned'] == x]\n    sns.distplot(values, hist=False, rug=False)\n\ng = sns.FacetGrid(churn, col='has_churned', hue='has_churned')\np1 = g.map(sns.distplot, 'time_since_first_purchase', hist=True, rug=True).add_legend()\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(churn['time_since_first_purchase'])\n\n\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\n\n\n[0 1]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#visualizing-liner-and-logistic-model",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#visualizing-liner-and-logistic-model",
    "title": "Simple Logistic Regression Modeling",
    "section": "Visualizing liner and logistic model",
    "text": "Visualizing liner and logistic model\nA logistic regression model can be drawn using regplot() in the same manner as a linear regression without you having to concern yourself with the modeling code. Try drawing both trend lines side by side to see how linear and logistic regressions make different predictions. From the linear model, you should see a linear trend (straight line), whereas from the logistic model, you should see a logistic trend (S-shaped).\n\n\nCode\n# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(data=churn,x='time_since_first_purchase',y='has_churned'\n            ,line_kws={\"color\": \"red\"})\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(x=\"time_since_first_purchase\",\n            y=\"has_churned\",\n            data=churn,\n            ci=None,\n            line_kws={\"color\": \"red\"})\n\n# Draw a logistic regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(data=churn,x='time_since_first_purchase', y='has_churned',ci=None,logistic=True,line_kws={\"color\": \"blue\"})\n\nplt.show()\n\nprint(\"\\n The two models give similar predictions in some places, but notice the slight curve in the logistic model trend.\")\n\n\n\n\n\n\n The two models give similar predictions in some places, but notice the slight curve in the logistic model trend."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression-with-logit",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression-with-logit",
    "title": "Simple Logistic Regression Modeling",
    "section": "Logistic regression with logit()",
    "text": "Logistic regression with logit()\nLogistic regression requires another function from statsmodels.formula.api: logit(). It takes the same arguments as ols(): a formula and data argument. You then use .fit() to fit the model to the data.\n\n\nCode\n# Import logit\nfrom statsmodels.formula.api import logit\n\n# Fit a logistic regression of churn vs. length of relationship using the churn dataset\nmdl_churn_vs_relationship = logit('has_churned ~ time_since_first_purchase', data=churn).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_churn_vs_relationship.params)\n\n\nOptimization terminated successfully.\n         Current function value: 0.679663\n         Iterations 4\nIntercept                   -0.015185\ntime_since_first_purchase   -0.354795\ndtype: float64"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#predictions-and-odds-ratios",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#predictions-and-odds-ratios",
    "title": "Simple Logistic Regression Modeling",
    "section": "Predictions and odds ratios",
    "text": "Predictions and odds ratios\nOdds ratios\nTaking the probability that something will happen and dividing it by the probability that it will not happen. It is equal to (P/(1-P)). Probability in favor of / against. The data cannot be compared with the original data, but can instead be plotted using a special chart. This unit represents the probability of … occurring (3 times the probability of…). It is easy to interpret, the data cannot be altered easily, and it is precise.\nLog odds ratio\nIt is a nice property of odds ratios that they can be passed into a log() = linear regression. Data changes that are easy to interpret and precise.\nMost likely Outcome\nAccording to logistic regression, we discuss the rounded most likely outcome (response > 0.5 chance of churning, etc.) since response values can be interpreted as probabilities. This data is very easy to interpret, easy to change, and not precise (rounded).\nProbability\nOriginal data. Easy to interpret, not easy to change data on the fly, and precise.\nProbabilities\nWe will examine each of the four main ways of expressing a logistic regression model’s prediction in the following four exercises. Since the response variable is either “yes” or “no”, you can predict the probability of a “yes”. These probabilities will be calculated and visualized here.\n\n\nCode\nexplanatory_data = pd.DataFrame({'time_since_first_purchase': np.arange(-1.5, 4, .35)})\n\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n  has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n)\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned\n0                      -1.50     0.626448\n1                      -1.15     0.596964\n2                      -0.80     0.566762\n3                      -0.45     0.536056\n4                      -0.10     0.505074\n\n\n\n\nCode\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n    has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n)\n\nfig = plt.figure()\n\n# Create a scatter plot with logistic trend line\nsns.regplot(x='time_since_first_purchase', y='has_churned', data=churn, ci=None, logistic=True)\n\n# Overlay with prediction_data, colored red\nsns.scatterplot(x='time_since_first_purchase',y='has_churned',\ndata=prediction_data, color='red')\n\nplt.show()"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#most-likely-outcome",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#most-likely-outcome",
    "title": "Simple Logistic Regression Modeling",
    "section": "Most likely outcome",
    "text": "Most likely outcome\nA non-technical audience may appreciate you not discussing probabilities and simply explaining the most likely outcome. Thus, instead of stating that there is a 60% chance of a customer leaving, you state that churn is the most likely outcome. There is a trade-off here between easier interpretation and nuance.\n\n\nCode\n# Update prediction data by adding most_likely_outcome\nprediction_data[\"most_likely_outcome\"] = np.round(prediction_data['has_churned'])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome\n0                      -1.50     0.626448                  1.0\n1                      -1.15     0.596964                  1.0\n2                      -0.80     0.566762                  1.0\n3                      -0.45     0.536056                  1.0\n4                      -0.10     0.505074                  1.0\n\n\n\n\nCode\n# Update prediction data by adding most_likely_outcome\nprediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n\nfig = plt.figure()\n\n# Create a scatter plot with logistic trend line (from previous exercise)\nsns.regplot(x=\"time_since_first_purchase\",\n            y=\"has_churned\",\n            data=churn,\n            ci=None,\n            logistic=True)\n\n# Overlay with prediction_data, colored red\nsns.scatterplot(x='time_since_first_purchase', y='most_likely_outcome', data=prediction_data, color='red')\n\nplt.show()\nprint(\"\\nThe most likely outcome is that you will master logistic regression! Providing the most likely response is a great way to share the model results with a non-technical audience.\")\n\n\n\n\n\n\nThe most likely outcome is that you will master logistic regression! Providing the most likely response is a great way to share the model results with a non-technical audience."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#odds-ratio",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#odds-ratio",
    "title": "Simple Logistic Regression Modeling",
    "section": "Odds ratio",
    "text": "Odds ratio\nAn odds ratio is a measure of the probability of something occurring compared to the probability that it will not occur. Often, this is easier to understand than probabilities, particularly when making decisions regarding choices. If, for example, a customer has a 20% chance of churning, it may be more intuitive to state “the chances of them not churning are four times higher than the chances of them churning.”.\n\n\nCode\n# Update prediction data with odds_ratio\nprediction_data[\"odds_ratio\"] =  prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio\n0                      -1.50     0.626448                  1.0    1.677003\n1                      -1.15     0.596964                  1.0    1.481166\n2                      -0.80     0.566762                  1.0    1.308199\n3                      -0.45     0.536056                  1.0    1.155431\n4                      -0.10     0.505074                  1.0    1.020502\n\n\n\n\nCode\n# Update prediction data with odds_ratio\nprediction_data[\"odds_ratio\"] = prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n\nfig = plt.figure()\n\n# Create a line plot of odds_ratio vs time_since_first_purchase\nsns.lineplot(x='time_since_first_purchase', y='odds_ratio',data=prediction_data)\n\n# Add a dotted horizontal line at odds_ratio = 1\nplt.axhline(y=1, linestyle=\"dotted\")\n\nplt.show()\n\nprint(\"\\nOdds ratios provide an alternative to probabilities that make it easier to compare positive and negative responses\")\n\n\n\n\n\n\nOdds ratios provide an alternative to probabilities that make it easier to compare positive and negative responses"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#log-odds-ratio",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#log-odds-ratio",
    "title": "Simple Logistic Regression Modeling",
    "section": "Log odds ratio",
    "text": "Log odds ratio\nThe disadvantage of probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. It is therefore difficult to understand what happens to the prediction when the explanatory variable is changed. The logarithm of the odds ratio (the “log odds ratio” or “logit”) does exhibit a linear relationship between predicted response and explanatory variable. As the explanatory variable changes, the response metric does not change significantly - only linearly.\nFor visualization purposes, it is usually better to plot the odds ratio and apply a log transformation to the y-axis scale since the actual values of log odds ratio are less intuitive than (linear) odds ratio.\n\n\nCode\n# Update prediction data with log_odds_ratio\nprediction_data[\"log_odds_ratio\"] = np.log(prediction_data['odds_ratio'])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio  \\\n0                      -1.50     0.626448                  1.0    1.677003   \n1                      -1.15     0.596964                  1.0    1.481166   \n2                      -0.80     0.566762                  1.0    1.308199   \n3                      -0.45     0.536056                  1.0    1.155431   \n4                      -0.10     0.505074                  1.0    1.020502   \n\n   log_odds_ratio  \n0        0.517008  \n1        0.392830  \n2        0.268651  \n3        0.144473  \n4        0.020295  \n\n\n\n\nCode\n# Update prediction data with log_odds_ratio\nprediction_data[\"log_odds_ratio\"] = np.log(prediction_data[\"odds_ratio\"])\n\nfig = plt.figure()\n\n# Update the line plot: log_odds_ratio vs. time_since_first_purchase\nsns.lineplot(x=\"time_since_first_purchase\",\n             y=\"log_odds_ratio\",\n             data=prediction_data)\n\n# Add a dotted horizontal line at log_odds_ratio = 0\nplt.axhline(y=0, linestyle=\"dotted\")\n\nplt.show()\nprint(\"\\nThe linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about.\")\n\n\n\n\n\n\nThe linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#section",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#section",
    "title": "Simple Logistic Regression Modeling",
    "section": "",
    "text": "Quantifying logistic regression fit\nResid plot, QQplot & Scale location plot are less useful in the case of logistic regression. Instead, we can use confusion matrices to analyze the fit performance. With True/False positive & negative outcomes. We can also compute metrics based on various ratios.\nAccuracy : proportion of correct predictions. Higher better.\nTN+TP / (TN+FN+FP+TP)\nSensitivity : proportions of observations where the actual response was true and where the model also predicted it was true. Higher better.\nTP / (FN + TP)\nSpecificity : proportions of observations where the actual was false and where the model also predicted it was false. Higher better.\nTN / (TN + FP) Calculating the confusion matrix\nA confusion matrix (occasionally called a confusion table) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.\nTrue positive: The customer churned and the model predicted they would.\nFalse positive: The customer didn't churn, but the model predicted they would.\nTrue negative: The customer didn't churn and the model predicted they wouldn't.\nFalse negative: The customer churned, but the model predicted they wouldn't.\n\n\nCode\n# Get the actual responses\nactual_response = churn[\"has_churned\"]\n\n# Get the predicted responses\npredicted_response = np.round(mdl_churn_vs_relationship.predict())\n\n# Create outcomes as a DataFrame of both Series\noutcomes = pd.DataFrame({\"actual_response\": actual_response,\n                         \"predicted_response\": predicted_response})\n\n# Print the outcomes\nprint(outcomes.value_counts(sort = False))\n\n\nactual_response  predicted_response\n0                0.0                   112\n                 1.0                    88\n1                0.0                    76\n                 1.0                   124\ndtype: int64\n\n\n\n\nCode\nconf_matrix = pd.crosstab(outcomes['actual_response'], outcomes['predicted_response'], rownames=['Actual'], colnames=['Predicted'])\nprint(conf_matrix)\n\n\nPredicted  0.0  1.0\nActual             \n0          112   88\n1           76  124"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#drawing-a-mosaic-plot-of-the-confusion-matrix",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#drawing-a-mosaic-plot-of-the-confusion-matrix",
    "title": "Simple Logistic Regression Modeling",
    "section": "Drawing a mosaic plot of the confusion matrix",
    "text": "Drawing a mosaic plot of the confusion matrix\nWhile calculating the performance matrix might be fun, it would become tedious if you needed multiple confusion matrices of different models. Luckily, the .pred_table() method can calculate the confusion matrix for you.\nAdditionally, you can use the output from the .pred_table() method to visualize the confusion matrix, using the mosaic() function.\n\n\nCode\n# Import mosaic from statsmodels.graphics.mosaicplot\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# Calculate the confusion matrix conf_matrix\nconf_matrix = mdl_churn_vs_relationship.pred_table()\n\n# Print it\nprint(conf_matrix)\n\n# Draw a mosaic plot of conf_matrix\nmosaic(conf_matrix)\nplt.show()\n\n\n[[112.  88.]\n [ 76. 124.]]"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#measuring-logistic-model-performance",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#measuring-logistic-model-performance",
    "title": "Simple Logistic Regression Modeling",
    "section": "Measuring logistic model performance",
    "text": "Measuring logistic model performance\nAs you know by now, several metrics exist for measuring the performance of a logistic regression model. In this last exercise, you’ll manually calculate accuracy, sensitivity, and specificity.\nAccuracy is the proportion of predictions that are correct.\naccuracy = (TN + TP) / TN + FN + FP + TP\nSensitivity is the proportion of true observations that are correctly predicted by the model as being true\nsensitivity = TP / (TP + FN)\nspecificity is the proportion of false observations that are correctly predicted by the model as being false.\nspecificity = TN / (TN + FP)\n\n\nCode\n# Extract TN, TP, FN and FP from conf_matrix\nTN = conf_matrix[0,0]\nTP = conf_matrix[1,1]\nFN = conf_matrix[1,0]\nFP = conf_matrix[0,1]\n\n# Calculate and print the accuracy\naccuracy = (TN + TP) / (TN + FN + FP + TP)\nprint(\"accuracy: \", accuracy)\n\n# Calculate and print the sensitivity\nsensitivity = TP / (TP + FN)\nprint(\"sensitivity: \", sensitivity)\n\n# Calculate and print the specificity\nspecificity = TN / (TN + FP)\nprint(\"specificity: \", specificity)\n\nprint(\"\\n Using these metrics, it becomes much easier to interpret and compare logistic regression models.\")\n\n\naccuracy:  0.59\nsensitivity:  0.62\nspecificity:  0.56\n\n Using these metrics, it becomes much easier to interpret and compare logistic regression models."
  },
  {
    "objectID": "posts/Summary of statistics/Summary Of Statistics.html",
    "href": "posts/Summary of statistics/Summary Of Statistics.html",
    "title": "Summary Of Statistics",
    "section": "",
    "text": "Datacamp course: Introduction to Statistic in Python\nThe study of statistics involves collecting, analyzing, and interpreting data. You can use it to bring the future into focus and infer answers to tons of questions. How many calls will your support team receive, and how many jeans sizes should you manufacture to fit 95% of the population? Statistical skills are developed in this course, which teaches you how to calculate averages, plot relationships between numeric values, and calculate correlations. In addition, you’ll learn how to conduct a well-designed study using Python to draw your own conclusions.\nCourse Takeaways:\n\nSummary Statistics\nRandom Numbers & Probability\nMore Distributions and the Central Limit Theorem\nCorrelation and Experimental Design\n\n\n\nStatistics - what is it?\n\nStatistics is the practice and study of collecting and analyzing data\nA summary statistic is a fact about or a summary of some data\n\nHow can statistics be used?\n\nDoes a product have a high likelihood of being purchased? People are more likely to purchase the product if they are familiar with it\nIs there an alternative payment system available?\nCan you tell me how many occupants your hotel will have? In what ways can you optimize occupancy?\nTo meet the needs of 95% of the population, how many sizes of jeans should be manufactured?\nCan the same number of each size be produced?\nA/B tests: Which advertisement is more effective in motivating the purchase of a product?\n\n\n\n\nDescriptive: To describe & summarize data e.g. 25% ride bike, 35% take bus ride & 50% drive to work\nInferential : Use sample data to make inferences about a larger population e.g. what percent of people drive to work?\n\n\n\n\n\nNumeric (quantitative)\nContinuous (measured)\n\nairplance speed\ntime spent waiting\n\nDiscrete (counted)\n\nnumber of devices\nnumber of people\n\nCategorical (qualitative)\nNominal (unordered)\n\nsingle / married\ncountry of residence\n\nOrdinal (ordered) agree, disagree, strongly diagree\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\nCode\nfood_consumption=pd.read_csv('food_consumption.csv')\n\n\n\n\nCode\n# Filter for Belgium\nbe_consumption = food_consumption[food_consumption['country']=='Belgium']\n\n# Filter for USA\nusa_consumption = food_consumption[food_consumption['country']=='USA']\n\n# Calculate mean and median consumption in Belgium\nprint(np.mean(be_consumption['consumption']))\nprint(np.median(be_consumption['consumption']))\n\n# Calculate mean and median consumption in USA\nprint(np.mean(usa_consumption['consumption']))\nprint(np.median(usa_consumption['consumption']))\n\n\n42.13272727272727\n12.59\n44.650000000000006\n14.58\n\n\n\n\nCode\n# Subset for Belgium and USA only\nbe_and_usa = food_consumption[(food_consumption['country']=='Belgium') | (food_consumption['country']=='USA')]\n\n# Group by country, select consumption column, and compute mean and median\nprint(be_and_usa.groupby('country')['consumption'].agg([np.mean,np.median]))\n\n\n              mean  median\ncountry                   \nBelgium  42.132727   12.59\nUSA      44.650000   14.58\n\n\nMean vs Median\n\n\nCode\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Subset for food_category equals rice\nrice_consumption = food_consumption[food_consumption['food_category']=='rice']\n\n# Histogram of co2_emission for rice and show plot\nplt.hist(rice_consumption['co2_emission'])\nplt.show()\n\n\n\n\n\n\n\nCode\n# Calculate mean and median of co2_emission with .agg()\nprint(rice_consumption['co2_emission'].agg([np.mean,np.median]))\n\n\nmean      37.591615\nmedian    15.200000\nName: co2_emission, dtype: float64\n\n\n\n\n\n\nVariance: Average distance from each data point to the data’s mean\nStandard Deviation\n\n\n\nCode\n# Calculate the quartiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,5)))\n\n# Calculate the quintiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,6)))\n\n# Calculate the deciles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,11)))\n\n\n[   0.        5.21     16.53     62.5975 1712.    ]\n[   0.       3.54    11.026   25.59    99.978 1712.   ]\n[0.00000e+00 6.68000e-01 3.54000e+00 7.04000e+00 1.10260e+01 1.65300e+01\n 2.55900e+01 4.42710e+01 9.99780e+01 2.03629e+02 1.71200e+03]\n\n\n\n\n\nA variable’s variance and standard deviation are two of the most common ways to measure its spread, and you will practice calculating them in this exercise. Spread informs expectations. In other words, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10, they might sell 40 products one day, and one or two the next. Predictions require information like this.\n\n\nCode\n# Print variance and sd of co2_emission for each food_category\nprint(food_consumption.groupby('food_category')['co2_emission'].agg([np.var,np.std]))\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Create histogram of co2_emission for food_category 'beef'\nplt.hist(food_consumption[food_consumption['food_category']=='beef']['co2_emission'])\n# Show plot\nplt.show()\n\n# Create histogram of co2_emission for food_category 'eggs'\nplt.hist(food_consumption[food_consumption['food_category']=='eggs']['co2_emission'])\n# Show plot\nplt.show()\n\n\n                        var         std\nfood_category                          \nbeef           88748.408132  297.906710\ndairy          17671.891985  132.935669\neggs              21.371819    4.622966\nfish             921.637349   30.358481\nlamb_goat      16475.518363  128.356996\nnuts              35.639652    5.969895\npork            3094.963537   55.632396\npoultry          245.026801   15.653332\nrice            2281.376243   47.763754\nsoybeans           0.879882    0.938020\nwheat             71.023937    8.427570\n\n\n\n\n\n\n\n\nFinding outliers using IQR\nOutliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that’s less influenced by outliers. IQR is also often used to find outliers. If a value is less than Q1−1.5×IQRQ1−1.5×IQR or greater than Q3+1.5×IQRQ3+1.5×IQR, it’s considered an outlier.\n\n\nCode\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\nprint(emissions_by_country)\n\n\ncountry\nAlbania      1777.85\nAlgeria       707.88\nAngola        412.99\nArgentina    2172.40\nArmenia      1109.93\n              ...   \nUruguay      1634.91\nVenezuela    1104.10\nVietnam       641.51\nZambia        225.30\nZimbabwe      350.33\nName: co2_emission, Length: 130, dtype: float64\n\n\n\n\nCode\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quantiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n\n# Subset emissions_by_country to find outliers\noutliers = emissions_by_country[(emissions_by_country > upper) | (emissions_by_country < lower)]\nprint(outliers)\n\n\ncountry\nArgentina    2172.4\nName: co2_emission, dtype: float64\n\n\n\n\n\n\nIn this chapter, you’ll learn how to generate random samples and measure chance using probability. You’ll work with real-world sales data to calculate the probability of a salesperson being successful. Finally, you’ll use the binomial distribution to model events with binary outcomes"
  }
]