[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Muhammad Asad Kamran is passionate\nMuhammad Asad Kamran has more than 15 years of experience in Software Engineering profession. Having strong hands on for architect, development, consultation & team building/managing experience of enterprise application & their integration with business critical applications, which includes SharePoint/Project Server, Asp.Net MVC & Biztalk complex applications.\nBeing a certified PMP & Prince 2 practitioner, Asad has been managing & mentoring mission critical team which delivered successful & award wining projects worth millions for clients in Middle East, Europe including Telco Operators, Oil & Gas clients, ministries, Insurance giant & multinational Attorney giants. Having numerous success stories of working with key stakeholders to develop architectural framework that aligns strategy, processes, and IT assets with business goals.\nExcellent communication, presentation, and organizational skills. Involved in successful Digital Transformation & Integration projects which provides G2G, G2B, B2B, B2C & G2C e-commerce & Digital services. His core competencies are Collaboration, Messaging, Enterprise solution architecture, Digitization transformation, project management, Agile methodologies & Data analytics."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity Of Michigan | Michigan, USA Masters in applied data science (MADS) | May 2022 - April 2024\nComsats Institute Of Information Technology | Lahore, Pakistan Bachelors Of Computer Science (Software Engineering) | 2002 - 2005"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Fuzzy Regression Discontinuity Design: An Introduction\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nRegression Discontinuity in Causal Inference: An Introduction\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSharp Regression Discontinuity Design: An Introduction\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kakamana’s Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nIntroduction to networks\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nnetwork analysis\n\n\nmachine learning\n\n\nNetworkX\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nFuzzy Regression Discontinuity Design: An Introduction\n\n\n\n\n\n\n\npython\n\n\ncausal inference\n\n\nMADS\n\n\nUniversity Of Michigan\n\n\nstatistics\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nRegression Discontinuity in Causal Inference: An Introduction\n\n\n\n\n\n\n\npython\n\n\ncausal inference\n\n\nMADS\n\n\nUniversity Of Michigan\n\n\nstatistics\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSharp Regression Discontinuity Design: An Introduction\n\n\n\n\n\n\n\npython\n\n\ncausal inference\n\n\nMADS\n\n\nUniversity Of Michigan\n\n\nstatistics\n\n\nregression discontinuity\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Time Series Data\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nValidating and Inspecting Time Series Models\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nkakamana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTime Series and Machine Learning Primer\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nTime Series as Inputs to a Model\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nFeature Engineering\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nfeature engineering\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Data Preprocessing\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nPutting It All Together\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nfeature engineering\n\n\nPCA\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSelecting Features for Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nfeature engineering\n\n\nPCA\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nkakamana\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nStandardizing Data\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nExploring High Dimensional Data\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nfeature engineering\n\n\nmachine learning\n\n\ndimension\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nFeature Extraction\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nfeature engineering\n\n\nmachine learning\n\n\ndimension reduction\n\n\nPCA\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nkakamana\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nFeature Selection I - Selecting for Feature Information\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nfeature engineering\n\n\nmachine learning\n\n\ndimension reduction\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nFeature Selection II - Selecting for Model Accuracy\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nfeature engineering\n\n\nmachine learning\n\n\ndimension reduction\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nkakamana\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification with XGBoost\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nclassification\n\n\nmachine learning\n\n\nXGBoost\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nFine-tuning your XGBoost model\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nhyperparameters\n\n\nmachine learning\n\n\nXGBoost\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nRegression with XGBoost\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nregression\n\n\nmachine learning\n\n\nXGBoost\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nUsing XGBoost in pipelines\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nhyperparameters\n\n\nmachine learning\n\n\nXGBoost\n\n\npipeline\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nApplying logistic regression and SVM\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nlogistic regression\n\n\nmachine learning\n\n\nSVM\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Hypothesis Testing\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nhypothesis\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nLogistic regression\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nlinear classifier\n\n\nlogistic regression\n\n\nmachine learning\n\n\nSVM\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nkakamana\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nLoss function\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nlogistic regression\n\n\nlinear classifier\n\n\nmachine learning\n\n\nSVM\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nRegression\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nlogistic regression\n\n\nmachine learning\n\n\nsupervised\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSupport Vector Machines\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nlinear classifier\n\n\nmachine learning\n\n\nSVM\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSampling Distribution\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAssessing model fit\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nSampling Methods\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSimple Logistic Regression Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to sampling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nsampling\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPredictions and model objects in linear regression\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSimple Linear Regression Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation in a nutshell\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nclassification\n\n\nmachine learning\n\n\nsupervised\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation and Experimental Design\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nDistribution (pdf, cdf) of iris data\n\n\n\n\n\n\n\npython\n\n\nstatistics\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nFine Tunning Model\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nsupervised\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nPreprocessing and Pipelines\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nmachine learning\n\n\nsupervised\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nRandom Numbers and Probability\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSummary Of Statistics\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Applying logistic regression and SVM/Applying logistic regression and SVM.html",
    "href": "posts/Applying logistic regression and SVM/Applying logistic regression and SVM.html",
    "title": "Applying logistic regression and SVM",
    "section": "",
    "text": "Applying logistic regression and SVM\nWe will learn the basics of applying logistic regression and support vector machines (SVMs) to classification problems. You’ll use the scikit-learn library to fit classification models to real data.\nThis Applying logistic regression and SVM is part of Datacamp course: Linear Classifiers in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport matplotlib.pyplot as plt\n\n\nWe will explore subset of Large movie review dataset The X variables contain features based on the words in the movie reviews, and the y variables contain labels for whether the review sentiment is positive (+1) or negative (-1).\n\n\nCode\nX_train, y_train = load_svmlight_file('dataset/aclImdb_v1/aclImdb/train/labeledBow.feat')\nX_test, y_test = load_svmlight_file('dataset/aclImdb_v1/aclImdb/test/labeledBow.feat')\n\n\n\n\nCode\nX_train.shape\n\n\n(1347, 64)\n\n\n\n\nCode\nX_test.shape\n\n\n(450, 64)\n\n\n\n\nCode\ntype(X_train)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nX_train[0]\n\n\narray([ 0.,  0.,  0.,  3., 16.,  3.,  0.,  0.,  0.,  0.,  0., 12., 16.,\n        2.,  0.,  0.,  0.,  0.,  8., 16., 16.,  4.,  0.,  0.,  0.,  7.,\n       16., 15., 16., 12., 11.,  0.,  0.,  8., 16., 16., 16., 13.,  3.,\n        0.,  0.,  0.,  0.,  7., 14.,  1.,  0.,  0.,  0.,  0.,  0.,  6.,\n       16.,  0.,  0.,  0.,  0.,  0.,  0.,  4., 14.,  0.,  0.,  0.])\n\n\n\n\nCode\ny_train[y_train < 5] = -1.0\ny_train[y_train >=5] = 1.0\n\ny_test[y_test < 5] = -1.0\ny_test[y_test >= 5] = 1.0\n\n\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create and fit the model\nknn = KNeighborsClassifier()\nknn.fit(X_train[:, :89523], y_train)\n\n# Predict on the test features, print the results\npred = knn.predict(X_test)[0]\nprint(\"Prediction for test example 0:\", pred)\n\n\nPrediction for test example 0: 1.0\n\n\n\nRunning LogisticRegression and SVC\nIn this exercise, you’ll apply logistic regression and a support vector machine to classify images of handwritten digits.\n\n\nCode\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\ndigits = datasets.load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n\n# Apply logistic regression and print scores\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(lr.score(X_train, y_train))\nprint(lr.score(X_test, y_test))\n\n\n1.0\n0.9622222222222222\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\nCode\n# Apply SVM and print scores\nsvm = SVC()\nsvm.fit(X_train, y_train)\nprint(svm.score(X_train, y_train))\nprint(svm.score(X_test, y_test))\n\n\n0.994060876020787\n0.9911111111111112\n\n\n\n\nSentiment analysis for movie reviews\nIn this exercise you’ll explore the probabilities outputted by logistic regression on a subset of the Large Movie Review Dataset.\nThe variables X and y are already loaded into the environment. X contains features based on the number of times words appear in the movie reviews, and y contains labels for whether the review sentiment is positive (+1) or negative (-1).\n\n\nCode\n# Instantiate logistic regression and train\nlr = LogisticRegression()\nlr.fit(X, y)\n\n# Predict sentiment for a glowing review\nreview1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n# Review: LOVED IT! This movie was amazing. Top 10 this year.\n# Probability of positive review: 0.8079007873616059\n\n\n# Predict sentiment for a poor review\nreview2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\nreview2_features = get_features(review2)\nprint(\"Review:\", review2)\nprint(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])\n\n# Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n# Probability of positive review: 0.5855117402793947\n\n\n\n\nLinear classifiers\n\nClassification: learning to predict categories\ndecision boundary: the surface separating different predicted classes\nlinear classifier: a classifier that learn linear decision boundaries e.g. logistic regression, linear SVM\nlinearly separable: a dataset can be perfectly explained by a linear classifier\n\n\n\nVisualizing decision boundaries\n\n\nCode\ndef make_meshgrid(x, y, h=.02, lims=None):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n        x: data to base x-axis meshgrid on\n        y: data to base y-axis meshgrid on\n        h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n        xx, yy : ndarray\n    \"\"\"\n\n    if lims is None:\n        x_min, x_max = x.min() - 1, x.max() + 1\n        y_min, y_max = y.min() - 1, y.max() + 1\n    else:\n        x_min, x_max, y_min, y_max = lims\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\n\n\nCode\ndef plot_contours(ax, clf, xx, yy, proba=False, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n        ax: matplotlib axes object\n        clf: a classifier\n        xx: meshgrid ndarray\n        yy: meshgrid ndarray\n        params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    if proba:\n        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,-1]\n        Z = Z.reshape(xx.shape)\n        out = ax.imshow(Z,extent=(np.min(xx), np.max(xx), np.min(yy), np.max(yy)),\n                        origin='lower', vmin=0, vmax=1, **params)\n        ax.contour(xx, yy, Z, levels=[0.5])\n    else:\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\n\n\nCode\ndef plot_classifier(X, y, clf, ax=None, ticks=False, proba=False, lims=None):\n    # assumes classifier \"clf\" is already fit\n    X0, X1 = X[:, 0], X[:, 1]\n    xx, yy = make_meshgrid(X0, X1, lims=lims)\n\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n        show = True\n    else:\n        show = False\n\n    # can abstract some of this into a higher-level function for learners to call\n    cs = plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8, proba=proba)\n    if proba:\n        cbar = plt.colorbar(cs)\n        cbar.ax.set_ylabel('probability of red $\\Delta$ class', fontsize=20, rotation=270, labelpad=30)\n        cbar.ax.tick_params(labelsize=14)\n        #ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors=\\'k\\', linewidth=1)\n    labels = np.unique(y)\n    if len(labels) == 2:\n        ax.scatter(X0[y==labels[0]], X1[y==labels[0]], cmap=plt.cm.coolwarm,\n                   s=60, c='b', marker='o', edgecolors='k')\n        ax.scatter(X0[y==labels[1]], X1[y==labels[1]], cmap=plt.cm.coolwarm,\n                   s=60, c='r', marker='^', edgecolors='k')\n    else:\n        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=50, edgecolors='k', linewidth=1)\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    #     ax.set_xlabel(data.feature_names[0])\n    #     ax.set_ylabel(data.feature_names[1])\n    if ticks:\n        ax.set_xticks(())\n        ax.set_yticks(())\n        #     ax.set_title(title)\n    if show:\n        plt.show()\n    else:\n        return ax\n\n\n\n\nCode\ndef plot_4_classifiers(X, y, clfs):\n    # Set-up 2x2 grid for plotting.\n    fig, sub = plt.subplots(2, 2)\n    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n\n    for clf, ax, title in zip(clfs, sub.flatten(), (\"(1)\", \"(2)\", \"(3)\", \"(4)\")):\n        # clf.fit(X, y)\n        plot_classifier(X, y, clf, ax, ticks=True)\n        ax.set_title(title)\n\n\n\n\nCode\n#hide\nX = np.array([[11.45,  2.4 ],\n       [13.62,  4.95],\n       [13.88,  1.89],\n       [12.42,  2.55],\n       [12.81,  2.31],\n       [12.58,  1.29],\n       [13.83,  1.57],\n       [13.07,  1.5 ],\n       [12.7 ,  3.55],\n       [13.77,  1.9 ],\n       [12.84,  2.96],\n       [12.37,  1.63],\n       [13.51,  1.8 ],\n       [13.87,  1.9 ],\n       [12.08,  1.39],\n       [13.58,  1.66],\n       [13.08,  3.9 ],\n       [11.79,  2.13],\n       [12.45,  3.03],\n       [13.68,  1.83],\n       [13.52,  3.17],\n       [13.5 ,  3.12],\n       [12.87,  4.61],\n       [14.02,  1.68],\n       [12.29,  3.17],\n       [12.08,  1.13],\n       [12.7 ,  3.87],\n       [11.03,  1.51],\n       [13.32,  3.24],\n       [14.13,  4.1 ],\n       [13.49,  1.66],\n       [11.84,  2.89],\n       [13.05,  2.05],\n       [12.72,  1.81],\n       [12.82,  3.37],\n       [13.4 ,  4.6 ],\n       [14.22,  3.99],\n       [13.72,  1.43],\n       [12.93,  2.81],\n       [11.64,  2.06],\n       [12.29,  1.61],\n       [11.65,  1.67],\n       [13.28,  1.64],\n       [12.93,  3.8 ],\n       [13.86,  1.35],\n       [11.82,  1.72],\n       [12.37,  1.17],\n       [12.42,  1.61],\n       [13.9 ,  1.68],\n       [14.16,  2.51]])\n\ny = np.array([ True,  True, False,  True,  True,  True, False, False,  True,\n       False,  True,  True, False, False,  True, False,  True,  True,\n        True, False,  True,  True,  True, False,  True,  True,  True,\n        True,  True,  True,  True,  True, False,  True,  True,  True,\n       False, False,  True,  True,  True,  True, False, False, False,\n        True,  True,  True, False,  True])\n\n\n\n\nCode\nfrom sklearn.svm import LinearSVC\n\n# Define the classifiers\nclassifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n\n# Fit the classifiers\nfor c in classifiers:\n    c.fit(X, y)\n\n# Plot the classifiers\nplot_4_classifiers(X, y, classifiers)\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn("
  },
  {
    "objectID": "posts/Classification/Classification.html",
    "href": "posts/Classification/Classification.html",
    "title": "Classification",
    "section": "",
    "text": "We will explore how to solve classification problems using supervised learning techniques, which include splitting data into training and test sets, fitting a model, predicting outcomes, and evaluating accuracy. You’ll learn the relationship between model complexity and performance, applying what you learn to a churn dataset, in which you’ll classify customers’ churn status.\nThis Classification is part of Datacamp course: Supervised Learning with scikit-learn\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import the course packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport scipy.stats\nimport seaborn as sns\n\n# Import the course datasets as DataFrames\nauto = pd.read_csv(\"auto.csv\")\nboston = pd.read_csv(\"boston.csv\")\ndiabetes = pd.read_csv(\"diabetes.csv\")\ngapminder = pd.read_csv(\"gm_2008_region.csv\")\nvotes = pd.read_csv(\"votes.csv\")\nwhitewine = pd.read_csv(\"white-wine.csv\")\nredwine = pd.read_csv(\"winequality-red.csv\")\n\n# Preview the first DataFrame\nauto\n\n\n\n\n\n\n  \n    \n      \n      mpg\n      displ\n      hp\n      weight\n      accel\n      origin\n      size\n    \n  \n  \n    \n      0\n      18.0\n      250.0\n      88\n      3139\n      14.5\n      US\n      15.0\n    \n    \n      1\n      9.0\n      304.0\n      193\n      4732\n      18.5\n      US\n      20.0\n    \n    \n      2\n      36.1\n      91.0\n      60\n      1800\n      16.4\n      Asia\n      10.0\n    \n    \n      3\n      18.5\n      250.0\n      98\n      3525\n      19.0\n      US\n      15.0\n    \n    \n      4\n      34.3\n      97.0\n      78\n      2188\n      15.8\n      Europe\n      10.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      387\n      18.0\n      250.0\n      88\n      3021\n      16.5\n      US\n      15.0\n    \n    \n      388\n      27.0\n      151.0\n      90\n      2950\n      17.3\n      US\n      10.0\n    \n    \n      389\n      29.5\n      98.0\n      68\n      2135\n      16.6\n      Asia\n      10.0\n    \n    \n      390\n      17.5\n      250.0\n      110\n      3520\n      16.4\n      US\n      15.0\n    \n    \n      391\n      25.1\n      140.0\n      88\n      2720\n      15.4\n      US\n      10.0\n    \n  \n\n392 rows × 7 columns\n\n\n\n\nSupervised learning: Uses labeled data\nUnsupervised learning: uses unlabled data\n\n\n\n\nUncovering hidden patterns from unlabeled data\nExample:\n\ngrouping customers into distinct categories (clustering)\n\n\n\n\n\n\nPredictor variables / features and a target variable\nAim: predict target variable, given predictor variables\n\nClassification: Target variable consist of categories\nRegression: Target variable is continuous\n\n\n\n\n\n\nSoftware agents interact with an environment\n\nLearn how to optimize their behavior\nGiven a system of rewards and punishments\nDraws inspiration from behavioral psychology\n\nApplications\n\nEconomics\nGenetics\nGame playing\n\n\nEDA of IRIS dataset\n\n\nCode\nfrom sklearn import datasets\nplt.style.use('ggplot')\n\n\n\n\nCode\niris=datasets.load_iris()\n\n\n\n\nCode\nprint(type(iris))\nprint(iris.keys())\nprint(type(iris.data))\nprint(type(iris.keys))\nprint(iris.data.shape)\nprint(iris.target_names)\n\n\n<class 'sklearn.utils._bunch.Bunch'>\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n<class 'numpy.ndarray'>\n<class 'builtin_function_or_method'>\n(150, 4)\n['setosa' 'versicolor' 'virginica']\n\n\n\n\nCode\nx=iris.data\ny=iris.target\ndf=pd.DataFrame(x,columns=iris.feature_names)\nprint(df.head())\n\n\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2\n\n\n\n\nCode\n_ = pd.plotting.scatter_matrix(df, c = y, figsize = [8, 8], s=150, marker = 'D')\n\n\n\n\n\nBasic EDA of votes dataset\n\n\nCode\nvotes.head()\n\n\n\n\n\n\n  \n    \n      \n      party\n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      0\n      republican\n      n\n      y\n      n\n      y\n      y\n      y\n      n\n      n\n      n\n      y\n      ?\n      y\n      y\n      y\n      n\n      y\n    \n    \n      1\n      republican\n      n\n      y\n      n\n      y\n      y\n      y\n      n\n      n\n      n\n      n\n      n\n      y\n      y\n      y\n      n\n      ?\n    \n    \n      2\n      democrat\n      ?\n      y\n      y\n      ?\n      y\n      y\n      n\n      n\n      n\n      n\n      y\n      n\n      y\n      y\n      n\n      n\n    \n    \n      3\n      democrat\n      n\n      y\n      y\n      n\n      ?\n      y\n      n\n      n\n      n\n      n\n      y\n      n\n      y\n      n\n      n\n      y\n    \n    \n      4\n      democrat\n      y\n      y\n      y\n      n\n      y\n      y\n      n\n      n\n      n\n      n\n      y\n      ?\n      y\n      y\n      y\n      y\n    \n  \n\n\n\n\n\n\nCode\nvotes.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 435 entries, 0 to 434\nData columns (total 17 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   party              435 non-null    object\n 1   infants            435 non-null    object\n 2   water              435 non-null    object\n 3   budget             435 non-null    object\n 4   physician          435 non-null    object\n 5   salvador           435 non-null    object\n 6   religious          435 non-null    object\n 7   satellite          435 non-null    object\n 8   aid                435 non-null    object\n 9   missile            435 non-null    object\n 10  immigration        435 non-null    object\n 11  synfuels           435 non-null    object\n 12  education          435 non-null    object\n 13  superfund          435 non-null    object\n 14  crime              435 non-null    object\n 15  duty_free_exports  435 non-null    object\n 16  eaa_rsa            435 non-null    object\ndtypes: object(17)\nmemory usage: 57.9+ KB\n\n\n\n\nCode\nvotes.describe()\n\n\n\n\n\n\n  \n    \n      \n      party\n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      count\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n      435\n    \n    \n      unique\n      2\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n      3\n    \n    \n      top\n      democrat\n      n\n      y\n      y\n      n\n      y\n      y\n      y\n      y\n      y\n      y\n      n\n      n\n      y\n      y\n      n\n      y\n    \n    \n      freq\n      267\n      236\n      195\n      253\n      247\n      212\n      272\n      239\n      242\n      207\n      216\n      264\n      233\n      209\n      248\n      233\n      269\n    \n  \n\n\n\n\n\n\nCode\nvotes.shape\n\n\n(435, 17)\n\n\n\n\nCode\nplt.figure()\nsns.countplot(x='education', hue='party', data=votes, palette='RdBu')\nplt.xticks([0,1],['No','Yes'])\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.figure()\nsns.countplot(x='satellite', hue='party', data=votes, palette='RdBu')\nplt.xticks([0,1], ['No', 'Yes'])\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.figure()\nsns.countplot(x='missile', hue='party', data=votes, palette='RdBu')\nplt.xticks([0,1], ['No', 'Yes'])\nplt.show()\n\n\n\n\n\nUsing scikit-learn to fit classifier for IRIS data to fit a classifier\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=6)\nknn.fit(iris['data'],iris['target'])\n\n\nKNeighborsClassifier(n_neighbors=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=6)\n\n\n\n\nCode\nprint(iris['data'].shape)\nprint(iris['target'].shape)\n\n\n(150, 4)\n(150,)\n\n\nPredicting on unlabeled data\n\n\nCode\nX_new = np.array([[5.6, 2.8, 3.9, 1.1],\n[5.7, 2.6, 3.8, 1.3],\n[4.7, 3.2, 1.3, 0.2]])\n\nprediction = knn.predict(X_new)\nX_new.shape\n\n\n(3, 4)\n\n\n\n\nCode\nprint('Prediction: {}'.format(prediction))\n\n\nPrediction: [1 1 0]\n\n\nHere above it predicts one which relates to ‘versicolor’ for first two observation & 0 which relates to ‘setosa’ for the third observation as show below\n\n\nCode\nprint(iris.target_names)\n\n\n['setosa' 'versicolor' 'virginica']\n\n\nk-Nearest Neighbors: Fit\nNow that you have explored the Congressional voting records dataset, it is time to build your first classifier. You will fit a k-Nearest Neighbors classifier to the voting dataset, which has once again been loaded into a DataFrame.\nHugo discussed the importance of making sure your data conforms to the scikit-learn API format. It is necessary to place the features in an array where each column represents a feature and each row represents an observation or data point - in this case, a Congressman’s voting record. The target column must have the same number of observations as the feature data. This exercise has done this for you. Notice we named the feature array X and response variable y: This is in accordance with the common scikit-learn practice.\nBy specifying the n_neighbors parameter, you need to create a k-NN classifier with 6 neighbors and then fit it to the data\n\n\nCode\n#Here I need to load tranformed / cleaned data for votes which replace 'n' with 0 and 'y' with 1\ndf_votes = pd.read_csv(\"votes-ch1.csv\")\n# Create arrays for the features and the response variable\ny = df_votes['party'].values\nX = df_votes.drop('party', axis=1).values\n\n# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X,y)\n\n\nKNeighborsClassifier(n_neighbors=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=6)\n\n\nNow that as above shows our k-NN classifier with 6 neighbors has been fit to the data, it can be used to predict the labels of new data points.\nPredict k-nearest neighbors\nOnce we have fitted a k-NN classifier, we can use it to predict a new data point’s label. Due to the fact that all the data was used to fit the model, there are no unlabeled data available! In some cases, we can still use the .predict() method on the X that was used to fit the model, but it is not a good indicator of the model’s ability to generalize to new data sets.\nHugo will discuss a solution to this problem in the next video. For now, an unlabeled random data point has been generated and is available to you as X_new. As well as using the training data X, we will use your classifier to predict the label for this new data point. If you use .predict() on X_new, we will generate one prediction, but if you we it on X, we will generate 435 predictions: one for each sample.\nThe DataFrame has been pre-loaded as df. You will create the feature array X and target variable array Y yourself this time. DataFrame df has been pre-loaded with the data.\n\n\nCode\nX_new = pd.DataFrame((np.random.rand(1,16)))\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X)\n\n# Predict and print the label for the new data point X_new\nnew_prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(new_prediction))\n\n\nPrediction: ['democrat']\n\n\nOur model predict ‘democrat’ or ‘republican’? How sure can you be of its predictions? In other words, how can you measure its performance? This is what you will learn in below\nThe digits recognition dataset\nUp until now, you have been performing binary classification, since the target variable had two possible outcomes. Hugo, however, got to perform multi-class classification in the videos, where the target variable could take on three possible outcomes. Why does he get to have all the fun?! In the following exercises, you’ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn’s included datasets, and that is the one we will use in this exercise.\nEach sample in this scikit-learn dataset is an 8x8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. Recall that scikit-learn’s built-in datasets are of type Bunch, which are dictionary-like objects. Helpfully for the MNIST dataset, scikit-learn provides an ‘images’ key in addition to the ‘data’ and ‘target’ keys that you have seen with the Iris data. Because it is a 2D array of the images corresponding to each sample, this ‘images’ key is useful for visualizing the images, as you’ll see in this exercise (for more on plotting 2D arrays, see Chapter 2 of DataCamp’s course on Data Visualization with Python). On the other hand, the ‘data’ key contains the feature array - that is, the images as a flattened array of 64 pixels.\nNotice that you can access the keys of these Bunch objects in two different ways: By using the . notation, as in digits.images, or the [] notation, as in digits[‘images’].\nFor more on the MNIST data, check out this exercise in Part 1 of DataCamp’s Importing Data in Python course. There, the full version of the MNIST dataset is used, in which the images are 28x28. It is a famous dataset in machine learning and computer vision, and frequently used as a benchmark to evaluate the performance of a new model.\n\n\nCode\n# Import necessary modules\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\n# Load the digits dataset: digits\ndigits = datasets.load_digits()\n\n# Print the keys and DESCR of the dataset\nprint(digits.keys())\nprint(digits['DESCR'])\n\n# Print the shape of the images and data keys\nprint(digits.images.shape)\nprint(digits.data.shape)\n\n# Display digit 1010\nplt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()\n\n\ndict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n.. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 1797\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n\n(1797, 8, 8)\n(1797, 64)\n\n\nAbove It looks like the image in question corresponds to the digit ‘5’. Now, can we build a classifier that can make this prediction not only for this image, but for all the other ones in the dataset? we’ll do so in the next exercise!\nTrain/Test Split + Fit/Predict/Accuracy\nNow that we know why it’s important to separate your data into training and test sets, let’s practice on the digits dataset! Arrays for the features and target variable will be divided into training and test sets, and a k-NN classifier will be fitted to the training data and its accuracy will be calculated using .score().\n\n\nCode\n# Import necessary modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Create feature and target arrays\nX = digits.data\ny = digits.target\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n\n# Create a k-NN classifier with 7 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn.fit(X_train,y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))\n\n\nIncredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be\n\n\nCode\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n\n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train,y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n\n\nIt looks like the test accuracy is highest when using 3 and 5 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data"
  },
  {
    "objectID": "posts/Classification with XGBoost/Classification with XGBoost.html",
    "href": "posts/Classification with XGBoost/Classification with XGBoost.html",
    "title": "Classification with XGBoost",
    "section": "",
    "text": "The fundamental idea behind XGBoost—boosted learners—will be introduced to you in this module. After gaining an understanding of how XGBoost works, we’ll apply it to solving one of the most common classification problems in the industry: predicting when customers will stop being customers.\nThis Classification with XGBoost is part of Datacamp course: Extreme Gradient Boosting with XGBoost\nThis is my learning experience of data science through DataCamp\n\n\n\nSupervised Learning\n\nRelies on labeled data\nHave some understanding of past behavior\nExamples\n\ndoes a specific image contains person’s face?\nTraning data, vectors of pixel values\nLabels, 1 or 0\n\nClassification: binary or multi-class\n\nbinary: will a person purchase insurance package given some quote\nmulti-class: classifying species of a given bird\n\n\nAUC: Metric for binary classification models\n\nArea Under the ROC Curve (AUC)\n\nLarger area under the ROC curve = better model \n\n\nOther supervised learning considerations\n\nFeatures can be either numeric or categorical\nNumeric features should be scaled (Z-scored)\nCategorical features should be encoded (one-hot)\n\n\n\n\n\n\nWhat is XGBoost? (eXtreme Gradient Boosting)\n\nOptimized gradient-boosting machine learning library\nOriginally written in C++\nHas APIs in several languages:\n\nPython, R, Scala, Julia, Java\n\n\nWhat makes XGBoost so popular?\n\nSpeed and performance\nCore algorithm is parallelizable\nConsistently outperforms single-algorithm methods\nState-of-the-art performance in many ML tasks\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nplt.rcParams['figure.figsize'] = (10, 5)\n\n\n\n\nIt’s time to create our first XGBoost model! We can use the scikit-learn .fit() / .predict() paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API!\nHere, we’ll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up.\nOur goal is to use the first month’s worth of data to predict whether the app’s users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you’ll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy.\n\n\nCode\nchurn_data = pd.read_csv('dataset/churn_data.csv')\n\n\n\n\nCode\nchurn_data.head()\n\n\n\n\n\n\n  \n    \n      \n      avg_dist\n      avg_rating_by_driver\n      avg_rating_of_driver\n      avg_inc_price\n      inc_pct\n      weekday_pct\n      fancy_car_user\n      city_Carthag\n      city_Harko\n      phone_iPhone\n      first_month_cat_more_1_trip\n      first_month_cat_no_trips\n      month_5_still_here\n    \n  \n  \n    \n      0\n      3.67\n      5.0\n      4.7\n      1.10\n      15.4\n      46.2\n      True\n      0\n      1\n      1\n      1\n      0\n      1\n    \n    \n      1\n      8.26\n      5.0\n      5.0\n      1.00\n      0.0\n      50.0\n      False\n      1\n      0\n      0\n      0\n      1\n      0\n    \n    \n      2\n      0.77\n      5.0\n      4.3\n      1.00\n      0.0\n      100.0\n      False\n      1\n      0\n      1\n      1\n      0\n      0\n    \n    \n      3\n      2.36\n      4.9\n      4.6\n      1.14\n      20.0\n      80.0\n      True\n      0\n      1\n      1\n      1\n      0\n      1\n    \n    \n      4\n      3.13\n      4.9\n      4.4\n      1.19\n      11.8\n      82.4\n      False\n      0\n      0\n      0\n      1\n      0\n      0\n    \n  \n\n\n\n\n\n\nCode\nchurn_data.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 13 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   avg_dist                     50000 non-null  float64\n 1   avg_rating_by_driver         49799 non-null  float64\n 2   avg_rating_of_driver         41878 non-null  float64\n 3   avg_inc_price                50000 non-null  float64\n 4   inc_pct                      50000 non-null  float64\n 5   weekday_pct                  50000 non-null  float64\n 6   fancy_car_user               50000 non-null  bool   \n 7   city_Carthag                 50000 non-null  int64  \n 8   city_Harko                   50000 non-null  int64  \n 9   phone_iPhone                 50000 non-null  int64  \n 10  first_month_cat_more_1_trip  50000 non-null  int64  \n 11  first_month_cat_no_trips     50000 non-null  int64  \n 12  month_5_still_here           50000 non-null  int64  \ndtypes: bool(1), float64(6), int64(6)\nmemory usage: 4.6 MB\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n# Create arrays for the features and the target: X, y\nX, y = churn_data.iloc[:, :-1], churn_data.iloc[:, -1]\n\n# Create the training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Instantiate the XGBClassifier: xg_cl\nxg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n\n# Fit the classifier to the training set\nxg_cl.fit(X_train, y_train)\n\n# Predict the labels of the test set: preds\npreds = xg_cl.predict(X_test)\n\n# Compute the accuracy: accuracy\naccuracy = float(np.sum(preds == y_test)) / y_test.shape[0]\nprint(\"accuracy: %f\" % (accuracy))\nprint(\"\\nOur model has an accuracy of around 74%. Later we'll learn about ways to fine tune our XGBoost models\")\n\n\naccuracy: 0.758200\n\nOur model has an accuracy of around 74%. Later we'll learn about ways to fine tune our XGBoost models\n\n\n\n\n\n\n\nDecision trees as base learners\n\nBase learner : Individual learning algorithm in an ensemble algorithm\nComposed of a series of binary questions\nPredictions happen at the “leaves” of the tree\n\nCART: Classification And Regression Trees\n\nEach leaf always contains a real-valued score\nCan later be converted into categories\n\n\n\n\nYour task in this exercise is to make a simple decision tree using scikit-learn’s DecisionTreeClassifier on the breast cancer dataset.\nThis dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\nWe’ve preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You’ll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out here.\n\n\nCode\nX = pd.read_csv('dataset/breast_X.csv').to_numpy()\ny = pd.read_csv('dataset/breast_y.csv').to_numpy().ravel()\n\n\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create the training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Instantiate the classifier: dt_clf_4\ndt_clf_4 = DecisionTreeClassifier(max_depth=4)\n\n# Fit the classifier to the training set\ndt_clf_4.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred_4\ny_pred_4 = dt_clf_4.predict(X_test)\n\n# Compute the accuracy of the predictions: accuracy\naccuracy = float(np.sum(y_pred_4 == y_test)) / y_test.shape[0]\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.9736842105263158\n\n\n\n\n\n\n\nBoosting overview\n\nNot a specific machine learning algorithm\nConcept that can be applied to a set of machine learning models\n\n“Meta-algorithm”\n\nEnsemble meta-algorithm used to convert many weak learners into a strong learner\n\nWeak learners and strong learners\n\nWeak learner: ML algorithm that is slightly better than chance\nBoosting converts a collection of weak learners into a strong learner\nStrong learner: Any algorithm that can be tuned to achieve good performance.\n\nHow boosting is accomplished?\n\nIteratively learning a set of week models on subsets of the data\nWeighting each weak prediction according to each weak learner’s performance\nCombine the weighted predictions to obtain a single weighted prediction\nthat is much better than the individual predictions themselves!\n\nModel evaluation through cross-validation\n\nCross-validation: Robust method for estimating the performance of a model on unseen data\nGenerates many non-overlapping train/test splits on training data\nReports the average test set performance across all data splits\n\n\n\n\nWe’ll now practice using XGBoost’s learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\nIn the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that’s what you will do here before running cross-validation on churn_data.\n\n\nCode\nchurn_data = pd.read_csv('dataset/churn_data.csv')\n\n\n\n\nCode\n# Create arrays for the features and the target: X, y\nX, y = churn_data.iloc[:, :-1], churn_data.iloc[:, -1]\n\n# Create the DMatrix from X and y: churn_dmatrix\nchurn_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary: params\nparams = {'objective':\"reg:logistic\", \"max_depth\":3}\n\n# Perform cross-validation: cv_results\ncv_results = xgb.cv(dtrain=churn_dmatrix, params=params,\n                   nfold=3, num_boost_round=5,\n                   metrics=\"error\", as_pandas=True, seed=123)\n\n# Pint cv_results\nprint(cv_results)\n\n# Print the accuracy\nprint(((1 - cv_results['test-error-mean']).iloc[-1]))\nprint(\"\\ncv_results stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From cv_results, the final round 'test-error-mean' is extracted and converted into an accuracy, where accuracy is 1-error. The final accuracy of around 75% is an improvement from earlier!\")\n\n\n   train-error-mean  train-error-std  test-error-mean  test-error-std\n0           0.28232         0.002366          0.28378        0.001932\n1           0.26951         0.001855          0.27190        0.001932\n2           0.25605         0.003213          0.25798        0.003963\n3           0.25090         0.001844          0.25434        0.003827\n4           0.24654         0.001981          0.24852        0.000934\n0.751480015401492\n\ncv_results stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From cv_results, the final round 'test-error-mean' is extracted and converted into an accuracy, where accuracy is 1-error. The final accuracy of around 75% is an improvement from earlier!\n\n\n\n\n\nNow that you’ve used cross-validation to compute average out-of-sample accuracy (after converting from an error), it’s very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv().\nYour job in this exercise is to compute another common metric used in binary classification - the area under the curve (\"auc\").\n\n\nCode\n# Perform cross_validation: cv_results\ncv_results = xgb.cv(dtrain=churn_dmatrix, params=params,\n                    nfold=3, num_boost_round=5,\n                    metrics=\"auc\", as_pandas=True, seed=123)\n\n# Print cv_results\nprint(cv_results)\n\n# Print the AUC\nprint((cv_results[\"test-auc-mean\"]).iloc[-1])\nprint(\"\\nAn AUC of 0.84 is quite strong. As you have seen, XGBoost's learning API makes it very easy to compute any metric you may be interested in. Later, you'll learn about techniques to fine-tune your XGBoost models to improve their performance even further. For now, it's time to learn a little about exactly when to use XGBoost\")\n\n\n   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n0        0.768893       0.001544       0.767863      0.002819\n1        0.790864       0.006758       0.789156      0.006846\n2        0.815872       0.003900       0.814476      0.005997\n3        0.822959       0.002018       0.821682      0.003912\n4        0.827528       0.000769       0.826191      0.001937\n0.8261911413597645\n\nAn AUC of 0.84 is quite strong. As you have seen, XGBoost's learning API makes it very easy to compute any metric you may be interested in. Later, you'll learn about techniques to fine-tune your XGBoost models to improve their performance even further. For now, it's time to learn a little about exactly when to use XGBoost\n\n\n\n\n\n\n\nWhen to use XGBoost\n\nYou have a large number of training samples\n\nGreater than 1000 training samples and less 100 features\nThe number of features < number of training samples\n\nYou have a mixture of categorical and numeric features\n\nOr just numeric features\n\n\nWhen to NOT use XGBoost\n\nImage recognition\nComputer vision\nNatural language processing and understanding problems\nWhen the number of training samples is significantly smaller than the number of features"
  },
  {
    "objectID": "posts/Correlation and experimental design/Correlation and experimental design.html",
    "href": "posts/Correlation and experimental design/Correlation and experimental design.html",
    "title": "Correlation and Experimental Design",
    "section": "",
    "text": "We will explore how to quantify the strength of a linear relationship between two variables, and explore how confounding variables can affect the relationship between two other variables. we’ll also see how a study’s design can influence its results, change how the data should be analyzed, and potentially affect the reliability of your conclusions\nThis Correlation and Experimental Design is part of Datacamp course: Introduction to Statistic in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport numpy as np\n\n\n\n\n* Correlation coefficient\n    * Quantifies the linear relationship between two variables\n    * Number between -1 and 1\n    * Magnitude corresponds to strength of relationship\n    * Sign (+ or -) corresponds to direction of relationship\n\n* Pearson product-moment correlation(rr)\n\n\n\nHere we’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\nIn this exercise, you’ll examine the relationship between a country’s life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively.\n\n\nCode\nworld_happiness = pd.read_csv('world_happiness.csv', index_col=0)\nworld_happiness.head()\n\n\n\n\n\n\n  \n    \n      \n      country\n      social_support\n      freedom\n      corruption\n      generosity\n      gdp_per_cap\n      life_exp\n      happiness_score\n    \n  \n  \n    \n      1\n      Finland\n      2.0\n      5.0\n      4.0\n      47.0\n      42400\n      81.8\n      155\n    \n    \n      2\n      Denmark\n      4.0\n      6.0\n      3.0\n      22.0\n      48300\n      81.0\n      154\n    \n    \n      3\n      Norway\n      3.0\n      3.0\n      8.0\n      11.0\n      66300\n      82.6\n      153\n    \n    \n      4\n      Iceland\n      1.0\n      7.0\n      45.0\n      3.0\n      47900\n      83.0\n      152\n    \n    \n      5\n      Netherlands\n      15.0\n      19.0\n      12.0\n      7.0\n      50500\n      81.8\n      151\n    \n  \n\n\n\n\n\n\nCode\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x='life_exp', y='happiness_score',data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create scatterplot of happiness_score vs life_exp with trendline\nsns.lmplot(x='life_exp', y='happiness_score',data=world_happiness, ci=None)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Correlation between life_exp and happiness_score\ncor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n\nprint(cor)\n\n\n0.7802249053272062\n\n\n\n\n\n* Correlation only accounts for linear relationships\n* Transformation\n   *  Certain statistical methods rely on variables having a linear relationship\n        * Correlation coefficient\n        * Linear regression\n* Correlation does not imply causation\n    * x is correlated with yy does not mean xx causes y\n\n\n\nWhile the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it’s far from perfect. Here we’ll explore one of the caveats of the correlation coefficient by examining the relationship between a country’s GDP per capita (gdp_per_cap) and happiness score.\n\n\nCode\n# Scatterplot of gdp_per_cap and life_exp\nsns.scatterplot(x='gdp_per_cap',y='life_exp', data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Correlation between gdp_per_cap and life_exp\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n\nprint(cor)\n\n\n0.7019547642148015\n\n\n\n\n\nWhen variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. Here we’ll perform a transformation yourself\n\n\nCode\n# Scatterplot of happiness_score vs. gdp_per_cap\nsns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n\n\n\n0.7279733012222975\n\n\n\n\nCode\n# Create log_gdp_per_cap column\nworld_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n\n# Scatterplot of log_gdp_per_cap and happiness_score\nsns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness);\nplt.show()\n\n# Calculate correlation\ncor =  world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\nprint(\"\\n The relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP\")\n\n\n\n\n\n0.8043146004918288\n\n The relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP\n\n\n\n\n\nA new column has been added to world_happiness called grams_sugar_per_day, which contains the average amount of sugar eaten per person per day in each country. We’ll examine the effect of a country’s average sugar consumption on its happiness score.\n\n\nCode\nworld_happiness = pd.read_csv('world_happiness_add_sugar.csv', index_col=0)\nworld_happiness\n\n\n\n\n\n\n  \n    \n      \n      country\n      social_support\n      freedom\n      corruption\n      generosity\n      gdp_per_cap\n      life_exp\n      happiness_score\n      grams_sugar_per_day\n    \n    \n      Unnamed: 0\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      Finland\n      2\n      5\n      4.0\n      47\n      42400\n      81.8\n      155\n      86.8\n    \n    \n      2\n      Denmark\n      4\n      6\n      3.0\n      22\n      48300\n      81.0\n      154\n      152.0\n    \n    \n      3\n      Norway\n      3\n      3\n      8.0\n      11\n      66300\n      82.6\n      153\n      120.0\n    \n    \n      4\n      Iceland\n      1\n      7\n      45.0\n      3\n      47900\n      83.0\n      152\n      132.0\n    \n    \n      5\n      Netherlands\n      15\n      19\n      12.0\n      7\n      50500\n      81.8\n      151\n      122.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129\n      Yemen\n      100\n      147\n      83.0\n      155\n      2340\n      68.1\n      5\n      77.9\n    \n    \n      130\n      Rwanda\n      144\n      21\n      2.0\n      90\n      2110\n      69.1\n      4\n      14.1\n    \n    \n      131\n      Tanzania\n      131\n      78\n      34.0\n      49\n      2980\n      67.7\n      3\n      28.0\n    \n    \n      132\n      Afghanistan\n      151\n      155\n      136.0\n      137\n      1760\n      64.1\n      2\n      24.5\n    \n    \n      133\n      Central African Republic\n      155\n      133\n      122.0\n      113\n      794\n      52.9\n      1\n      22.4\n    \n  \n\n133 rows × 9 columns\n\n\n\n\n\nCode\n# Scatterplot of grams_sugar_per_day and happiness_score\nsns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor =  world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n\n\n\n0.6939100021829634\n\n\n\n\n\n\n* Vocabulary\n    * Experiment aims to answer: What is the effect of the treatment on the response?\n        * Treatment: explanatory / independent variable\n        * Response: response / dependent variable\n    * E.g.: What is the effect of an advertisement on the number of products purchased?\n        * Treatment: advertisement\n        * Response: number of products purchased\n            * Controlled experiments\n            * Participants are assigned by researchers to either treatment group or control group\n            * Treatment group sees advertisement\n            * Control group does not\n            * Group should be comparable so that causation can be inferred\n            * If groups are not comparable, this could lead to confounding (bias)\n* Gold standard of experiment\n    * Randomized controlled trial\n        * Participants are assigned to treatment/control randomly, not based on any other characteristics\n        C* hoosing randomly helps ensure that groups are comparable\n    * Placebo\n        * Resembles treatement, but has no effect\n        * Participants will not know which group they're in\n    * Double-blind trial\n        * Person administering the treatment/running the study doesn't know whether the treatment is real or a placebo\n        * Prevents bias in the response and/or analysis of results\n    * Fewopportunities for bias = more reliable conclusion about causation\n* Observational studies\n    * Participants are not assigned randomly to groups\n        * Participants assign themselves, usually based on pre-existing characteristics\n    * Many research questions are not conductive to a controlled experiment\n        * Cannot force someone to smoke or have a disease\n    * Establish association, not causation\n        * Effects can be confounded by factors that got certain people into the control or treatment group\n        * There are ways to control for confounders to get more reliable conclusions about association\n            * Longitudinal vs. cross-sectional studies\n    * Longitudinal study\n        * Participants are followed over a period of time to examine effect of treatment on response\n        * Effect of age on height is not confounded by generation\n        * More expensive, results take longer\n    * Cross-sectional study\n        * Data on participants is collected from a single snapshot in time\n        * Effect of age on height is confounded by generation\n        * Cheaper, fater, more convenient"
  },
  {
    "objectID": "posts/Correlation in a nutshell/Correlation in a nutshell.html",
    "href": "posts/Correlation in a nutshell/Correlation in a nutshell.html",
    "title": "Correlation in a nutshell",
    "section": "",
    "text": "In this article we will explore basically a linear relationship between two variables, its possible quantification (magnitude & direction). We will also touch high level of confounding & caveats of correlation. This article use exploration of study for mammals sleeping habits & world happiness\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nCode\ndf = pd.read_csv('mammals.csv')\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      species\n      body_wt\n      brain_wt\n      non_dreaming\n      dreaming\n      total_sleep\n      life_span\n      gestation\n      predation\n      exposure\n      danger\n    \n  \n  \n    \n      0\n      Africanelephant\n      6654.000\n      5712.0\n      NaN\n      NaN\n      3.3\n      38.6\n      645.0\n      3\n      5\n      3\n    \n    \n      1\n      Africangiantpouchedrat\n      1.000\n      6.6\n      6.3\n      2.0\n      8.3\n      4.5\n      42.0\n      3\n      1\n      3\n    \n    \n      2\n      ArcticFox\n      3.385\n      44.5\n      NaN\n      NaN\n      12.5\n      14.0\n      60.0\n      1\n      1\n      1\n    \n    \n      3\n      Arcticgroundsquirrel\n      0.920\n      5.7\n      NaN\n      NaN\n      16.5\n      NaN\n      25.0\n      5\n      2\n      3\n    \n    \n      4\n      Asianelephant\n      2547.000\n      4603.0\n      2.1\n      1.8\n      3.9\n      69.0\n      624.0\n      3\n      5\n      4\n    \n  \n\n\n\n\n\n\nCode\ndf.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 62 entries, 0 to 61\nData columns (total 11 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   species       62 non-null     object \n 1   body_wt       62 non-null     float64\n 2   brain_wt      62 non-null     float64\n 3   non_dreaming  48 non-null     float64\n 4   dreaming      50 non-null     float64\n 5   total_sleep   58 non-null     float64\n 6   life_span     58 non-null     float64\n 7   gestation     58 non-null     float64\n 8   predation     62 non-null     int64  \n 9   exposure      62 non-null     int64  \n 10  danger        62 non-null     int64  \ndtypes: float64(7), int64(3), object(1)\nmemory usage: 5.5+ KB\n\n\n\n\n\nThe sleep time of 39 species of mammals distributed over 13 orders is analyzed in regards to their distribution over the 13 orders. There are 62 observations across 11 variables.\nspecies : Mammal species\nbody_wt : Mammal’s total body weight (kg)\nbrain_wt : Mammal’s brain weight (kg)\nnon_dreaming : Sleep hours without dreaming\ndreaming : Sleep hours spent dreaming\ntotal_sleep : Total number of hours of sleep\nlife_span : Life span (in years)\ngestation : Days during gestation / pregnancy\nThe likelihood that a mammal will be preyed upon. 1 = least likely to be preyed on. 5 = most likely to be preyed upon.\nexposure : How exposed a mammal is during sleep. 1 = least exposed (e.g., sleeps in a well-protected den). 5 = most exposed.\nA measure of how much danger the mammal faces. This index is based upon Predation and Exposure. 1 = least danger from other animals. 5 = most danger from other animals.\n\n\nCode\ndf.isnull().sum()\n\n\nspecies          0\nbody_wt          0\nbrain_wt         0\nnon_dreaming    14\ndreaming        12\ntotal_sleep      4\nlife_span        4\ngestation        4\npredation        0\nexposure         0\ndanger           0\ndtype: int64\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Body Weight Distribution')\nsns.histplot(df['body_wt'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Body Weight Distribution'}, xlabel='body_wt', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Body Wight')\nsns.barplot(x='body_wt', y='species', data=df.sort_values('body_wt',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Body Wight'}, xlabel='body_wt', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"body_wt\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c10a9430>\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Brain Weight Distribution')\nsns.histplot(df['brain_wt'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Brain Weight Distribution'}, xlabel='brain_wt', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Brain Wight')\nsns.barplot(x='brain_wt', y='species', data=df.sort_values('brain_wt',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Brain Wight'}, xlabel='brain_wt', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"brain_wt\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c5b40bb0>\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Life Span Distribution')\nsns.histplot(df['life_span'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Life Span Distribution'}, xlabel='life_span', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Life Span')\nsns.barplot(x='life_span', y='species', data=df.sort_values('life_span',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Life Span'}, xlabel='life_span', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"life_span\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c6460d60>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Predation Total Sleep Visualization')\nsns.countplot(x='predation',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"predation\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"predation\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"predation\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3907231976.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"predation\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='predation', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Exposure Total Sleep Visualization')\nsns.countplot(x='exposure',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"exposure\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"exposure\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"exposure\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3542283944.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"exposure\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='exposure', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Danger Total Sleep Visualization')\nsns.countplot(x='danger',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"danger\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"danger\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"danger\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3554697531.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"danger\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='danger', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\nx = explanatory / independent variables y = response / dependent variable\n\n\nCode\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x='total_sleep', y='dreaming',data=df)\n\n# Show plot\nplt.title('Sleeping habits')\nplt.ylabel(\"rem sleep per day(hour\")\nplt.xlabel(\"total sleep per day(hour\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.lmplot(x=\"total_sleep\", y=\"dreaming\", data=df, ci=None)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\ndf['total_sleep'].corr(df['dreaming'])\n\n\n0.7270869571641637\n\n\n\n\nCode\ndf['dreaming'].corr(df['total_sleep'])\n\n\n0.7270869571641637\n\n\n\n\n\n\n\nCode\n# Create a scatterplot of gestation vs. total_sleep and show\nsns.scatterplot(x='gestation', y='total_sleep',data=df)\n\n# Show plot\nplt.title('High negative correlation')\nplt.ylabel(\"gestation\")\nplt.xlabel(\"total sleep per day(hour\")\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.scatterplot(x='total_sleep', y='dreaming',data=df)\nplt.title(\"low positive correlation\")\nplt.show()"
  },
  {
    "objectID": "posts/Distribution (pdf, cdf) of iris dataset/Distribution (pdf, cdf) of iris data set.html",
    "href": "posts/Distribution (pdf, cdf) of iris dataset/Distribution (pdf, cdf) of iris data set.html",
    "title": "Distribution (pdf, cdf) of iris data",
    "section": "",
    "text": "Lets explore distribution functions pdf and cdf using Iris data set\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings;\nwarnings.filterwarnings('ignore');\n\n\n\n\nCode\niris=pd.read_csv('iris.csv')\niris.head()\n\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      type\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n    \n  \n\n\n\n\n\n\nCode\niris.shape\n\n\n(150, 5)\n\n\n\n\nCode\niris.columns\n\n\nIndex(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'type'], dtype='object')\n\n\n\n\nCode\niris['type'].value_counts()\n\n\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\nName: type, dtype: int64\n\n\n\n\nCode\nsetosa=iris[iris['type']==\"Iris-setosa\"]\nsetosa['petal_length'].value_counts()\n\n\n1.5    14\n1.4    12\n1.3     7\n1.6     7\n1.7     4\n1.2     2\n1.9     2\n1.1     1\n1.0     1\nName: petal_length, dtype: int64\n\n\n\n\n\n\nCode\niris.plot(kind='scatter',x='sepal_length',y='sepal_width');\nplt.show()\n\n\n\n\n\n\n\nCode\n#here we plot the scatter diagram with colour coding\nsns.set_style('whitegrid')\nsns.FacetGrid(iris,hue=\"type\",aspect = 2).map(plt.scatter,\"sepal_length\",\"sepal_width\").add_legend()\nplt.show()\n\n\n\n\n\n\n\n\nFor cross-referencing\n\n\nCode\nplt.close()\nsns.set_style(\"whitegrid\")\nsns.pairplot(iris,hue=\"type\",size=3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsetosa=iris.loc[iris[\"type\"]==\"Iris-setosa\"]\nversicolor=iris.loc[iris[\"type\"]==\"Iris-versicolor\"]\nvirginica=iris.loc[iris[\"type\"]==\"Iris-virginica\"]\n\n\n\n\nCode\nplt.plot(setosa[\"petal_length\"],np.zeros_like(setosa['petal_length']), 'o')\nplt.plot(versicolor[\"petal_length\"],np.zeros_like(versicolor['petal_length']), 'o')\nplt.plot(virginica[\"petal_length\"],np.zeros_like(virginica['petal_length']), 'o')\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.distplot(iris[iris['type']== 'Iris-setosa']['petal_length'])\n\n\n<AxesSubplot:xlabel='petal_length', ylabel='Density'>\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\",aspect = 2).map(sns.distplot, \"petal_length\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect = 2).map(sns.distplot, \"petal_width\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect=2).map(sns.distplot, \"sepal_length\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect=2).map(sns.distplot, \"sepal_width\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = False)\n\nprint(\"histogram counts\\n\",counts)\npdf = counts/(sum(counts))\nprint(\"Sum of count is\\n\",sum(counts))\nprint(\"bin edges \\n\",bin_edges)\nprint(\"PDF below \\n\",pdf);\n\nplt.gca().legend(('Pdf'))\nplt.title('PDF and PDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.plot(bin_edges[1:],pdf)\n\n\nhistogram counts\n [ 1  1  2  7 12 14  7  4  0  2]\nSum of count is\n 50\nbin edges \n [1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\nPDF below \n [0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n\n\n[<matplotlib.lines.Line2D at 0x1a5ae102100>]\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = False)\n\nprint(\"histogram counts\\n\",counts)\npdf = counts/(sum(counts))\nprint(\"Sum of count is\\n\",sum(counts))\nprint(\"bin edges \\n\",bin_edges)\nprint(\"PDF is below  \\n\",pdf);\n\ncdf = np.cumsum(pdf)\nprint(\"CDF is below\\n\",cdf)\nplt.gca().legend(('Cdf'))\nplt.title('CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\n\nplt.plot(bin_edges[1:],cdf)\n\n\nhistogram counts\n [ 1  1  2  7 12 14  7  4  0  2]\nSum of count is\n 50\nbin edges \n [1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\nPDF is below  \n [0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\nCDF is below\n [0.02 0.04 0.08 0.22 0.46 0.74 0.88 0.96 0.96 1.  ]\n\n\n[<matplotlib.lines.Line2D at 0x1a5ae1721c0>]\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\n\nprint(counts)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\n\n#compute CDF\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\nplt.gca().legend(('Pdf','Cdf'))\nplt.title('PDF and CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.show();\n\n\n[0.22222222 0.22222222 0.44444444 1.55555556 2.66666667 3.11111111\n 1.55555556 0.88888889 0.         0.44444444]\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\n\n\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=20,\n                                 density = True)\npdf = counts/(sum(counts))\nplt.plot(bin_edges[1:],pdf);\nplt.gca().legend(('Pdf','Cdf','bin edges'))\nplt.title('PDF and CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.show();\n\n\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\n\n# virginica\ncounts, bin_edges = np.histogram(virginica['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\n\n#versicolor\ncounts, bin_edges = np.histogram(versicolor['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\nplt.title('PDF and CDF For iris_versicolor')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\n\nplt.show();\n\n\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n[0.02 0.1  0.24 0.08 0.18 0.16 0.1  0.04 0.02 0.06]\n[4.5  4.74 4.98 5.22 5.46 5.7  5.94 6.18 6.42 6.66 6.9 ]\n[0.02 0.04 0.06 0.04 0.16 0.14 0.12 0.2  0.14 0.08]\n[3.   3.21 3.42 3.63 3.84 4.05 4.26 4.47 4.68 4.89 5.1 ]\n\n\n\n\n\n\n\n\n\n\nCode\n#Mean, Variance, Std-deviation,\nprint(\"Means:\")\nprint(np.mean(setosa[\"petal_length\"]))\n#Mean with an outlier.\nprint(np.mean(np.append(setosa[\"petal_length\"],50)));\nprint(np.mean(virginica[\"petal_length\"]))\nprint(np.mean(versicolor[\"petal_length\"]))\n\nprint(\"\\nStd-dev:\");\nprint(np.std(setosa[\"petal_length\"]))\nprint(np.std(virginica[\"petal_length\"]))\nprint(np.std(versicolor[\"petal_length\"]))\n\n\nMeans:\n1.464\n2.4156862745098038\n5.5520000000000005\n4.26\n\nStd-dev:\n0.17176728442867112\n0.546347874526844\n0.4651881339845203\n\n\n\n\n\n\n\nCode\n#Median, Quantiles, Percentiles, IQR.\nprint(\"\\nMedians:\")\nprint(np.median(setosa[\"petal_length\"]))\n#Median with an outlier\nprint(np.median(np.append(setosa[\"petal_length\"],50)));\nprint(np.median(virginica[\"petal_length\"]))\nprint(np.median(versicolor[\"petal_length\"]))\n\n\nprint(\"\\nQuantiles:\")\nprint(np.percentile(setosa[\"petal_length\"],np.arange(0, 100, 25)))\nprint(np.percentile(virginica[\"petal_length\"],np.arange(0, 100, 25)))\nprint(np.percentile(versicolor[\"petal_length\"], np.arange(0, 100, 25)))\n\nprint(\"\\n90th Percentiles:\")\nprint(np.percentile(setosa[\"petal_length\"],90))\nprint(np.percentile(virginica[\"petal_length\"],90))\nprint(np.percentile(versicolor[\"petal_length\"], 90))\n\nfrom statsmodels import robust\nprint (\"\\nMedian Absolute Deviation\")\nprint(robust.mad(setosa[\"petal_length\"]))\nprint(robust.mad(virginica[\"petal_length\"]))\nprint(robust.mad(versicolor[\"petal_length\"]))\n\n\n\nMedians:\n1.5\n1.5\n5.55\n4.35\n\nQuantiles:\n[1.    1.4   1.5   1.575]\n[4.5   5.1   5.55  5.875]\n[3.   4.   4.35 4.6 ]\n\n90th Percentiles:\n1.7\n6.31\n4.8\n\nMedian Absolute Deviation\n0.14826022185056031\n0.6671709983275211\n0.5189107764769602\n\n\n\n\n\n\n\nCode\nsns.boxplot(x='type',y='petal_length', data=iris)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.violinplot(x=\"type\", y=\"petal_length\", data=iris, size=8)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.jointplot(x=\"petal_length\", y=\"petal_width\", data=setosa, kind=\"kde\");\nplt.show();"
  },
  {
    "objectID": "posts/Exploring high dimensional data/Exploring High Dimensional data.html",
    "href": "posts/Exploring high dimensional data/Exploring High Dimensional data.html",
    "title": "Exploring High Dimensional Data",
    "section": "",
    "text": "It will introduce us to dimensionality reduction and explain why and when it is important. It will also teach us the difference between feature selection and feature extraction, and we’ll learn how to apply both techniques for data exploration. Finally, the chapter covers t-SNE, a powerful feature extraction method for analyzing high-dimensional data.\nThis Exploring High Dimensional Data is part of Datacamp course: Hypothesis Testing in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (10, 5)\n\n\n\n\nCode\npokemon_df = pd.read_csv('dataset/pokemon_gen1.csv')\npokemon_df.head()\n\n\n\n\n\n\n  \n    \n      \n      HP\n      Attack\n      Defense\n      Generation\n      Name\n      Type\n      Legendary\n    \n  \n  \n    \n      0\n      45\n      49\n      49\n      1\n      Bulbasaur\n      Grass\n      False\n    \n    \n      1\n      60\n      62\n      63\n      1\n      Ivysaur\n      Grass\n      False\n    \n    \n      2\n      80\n      82\n      83\n      1\n      Venusaur\n      Grass\n      False\n    \n    \n      3\n      80\n      100\n      123\n      1\n      VenusaurMega Venusaur\n      Grass\n      False\n    \n    \n      4\n      39\n      52\n      43\n      1\n      Charmander\n      Fire\n      False\n    \n  \n\n\n\n\n\n\nCode\npokemon_df.describe()\n\n\n\n\n\n\n  \n    \n      \n      HP\n      Attack\n      Defense\n      Generation\n    \n  \n  \n    \n      count\n      160.00000\n      160.00000\n      160.000000\n      160.0\n    \n    \n      mean\n      64.61250\n      74.98125\n      70.175000\n      1.0\n    \n    \n      std\n      27.92127\n      29.18009\n      28.883533\n      0.0\n    \n    \n      min\n      10.00000\n      5.00000\n      5.000000\n      1.0\n    \n    \n      25%\n      45.00000\n      52.00000\n      50.000000\n      1.0\n    \n    \n      50%\n      60.00000\n      71.00000\n      65.000000\n      1.0\n    \n    \n      75%\n      80.00000\n      95.00000\n      85.000000\n      1.0\n    \n    \n      max\n      250.00000\n      155.00000\n      180.000000\n      1.0\n    \n  \n\n\n\n\n\n\nCode\npokemon_df.describe(exclude='number')\n\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n      Legendary\n    \n  \n  \n    \n      count\n      160\n      160\n      160\n    \n    \n      unique\n      160\n      15\n      1\n    \n    \n      top\n      Bulbasaur\n      Water\n      False\n    \n    \n      freq\n      1\n      31\n      160\n    \n  \n\n\n\n\n\n\n\n\nA sample of the Pokemon dataset has been loaded as pokemon_df. To get an idea of which features have little variance to calculate summary as above statistics on this sample. Then adjust the code to create a smaller, easier to understand, dataset.\n\n\nCode\n# Remove the feature without variance from this list\nnumber_cols = ['HP', 'Attack', 'Defense']\n\n# Leave this list as is for now\nnon_number_cols = ['Name', 'Type', 'Legendary']\n\n# Sub-select by combining the lists with chosen features\ndf_selected = pokemon_df[number_cols + non_number_cols]\n\n# Prints the first 5 lines of the new DataFrame\nprint(df_selected.head())\n\n\n   HP  Attack  Defense                   Name   Type  Legendary\n0  45      49       49              Bulbasaur  Grass      False\n1  60      62       63                Ivysaur  Grass      False\n2  80      82       83               Venusaur  Grass      False\n3  80     100      123  VenusaurMega Venusaur  Grass      False\n4  39      52       43             Charmander   Fire      False\n\n\n\n\nCode\n# Leave this list as is\nnumber_cols = ['HP', 'Attack', 'Defense']\n\n# Remove the feature without variance from this list\nnon_number_cols = ['Name', 'Type' ]\n\n# Create a new dataframe by subselecting the chosen features\ndf_selected = pokemon_df[number_cols + non_number_cols]\n\n# Prints the first 5 lines of the new dataframe\nprint(df_selected.head())\n\n\n   HP  Attack  Defense                   Name   Type\n0  45      49       49              Bulbasaur  Grass\n1  60      62       63                Ivysaur  Grass\n2  80      82       83               Venusaur  Grass\n3  80     100      123  VenusaurMega Venusaur  Grass\n4  39      52       43             Charmander   Fire\n\n\nAll Pokemon in this dataset are non-legendary and from generation one so you could choose to drop those two features.\n\n\n\n\n\nWhy reduce dimensionality?\n\nYour dataset will:\n\nbe less complex\nrequire less disk space\nrequire less computation time\nhave lower chance of model overfitting\n\n\n\n\n\n\nFigure 1: feature\n\n\n\n\nData visualization is a crucial step in any data exploration. Let’s use Seaborn to explore some samples of the US Army ANSUR body measurement dataset.\n\n\nCode\nansur_df_1 = pd.read_csv('dataset/ansur_df_1.csv')\nansur_df_2 = pd.read_csv('dataset/ansur_df_2.csv')\n\n\n\n\nCode\n# Create a pairplot and color the points using the 'Gender' feature\nsns.pairplot(ansur_df_1, hue='Gender', diag_kind='hist');\n\n\n\n\n\n\n\nCode\n# Remove one of the redundant features\nreduced_df = ansur_df_1.drop('body_height', axis=1)\n\n# Creat a pairplot and color the points using the 'Gender' feature\nsns.pairplot(reduced_df, hue='Gender');\n\n\n\n\n\n\n\nCode\n# Create a pairplot and color the points using the 'Gender' feature\nsns.pairplot(ansur_df_2, hue='Gender', diag_kind='hist');\n\n\n\n\n\n\n\nCode\n# Remove the redundant feature\nreduced_df = ansur_df_2.drop(['n_legs'], axis=1)\n\n# Create a pairplot and color the points using the 'Gender' feature\nsns.pairplot(reduced_df, hue='Gender', diag_kind='hist');\n\n\n\n\n\nthe body height (inches) and stature (meters) hold the same information in a different unit + all the individuals in the second sample have two legs.\n\n\n\n\n  \n\n\nt-SNE is a great technique for visual exploration of high dimensional datasets. In this exercise, you’ll apply it to the ANSUR dataset. You’ll remove non-numeric columns from the pre-loaded dataset df and fit TSNE to his numeric dataset.\n\n\nCode\nansur_male = pd.read_csv('dataset/ANSUR_II_MALE.csv')\nansur_female = pd.read_csv('dataset/ANSUR_II_FEMALE.csv')\n\ndf = pd.concat([ansur_male, ansur_female])\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Branch\n      Component\n      Gender\n      abdominalextensiondepthsitting\n      acromialheight\n      acromionradialelength\n      anklecircumference\n      axillaheight\n      balloffootcircumference\n      balloffootlength\n      ...\n      waistdepth\n      waistfrontlengthsitting\n      waistheightomphalion\n      wristcircumference\n      wristheight\n      weight_kg\n      stature_m\n      BMI\n      BMI_class\n      Height_class\n    \n  \n  \n    \n      0\n      Combat Arms\n      Regular Army\n      Male\n      266\n      1467\n      337\n      222\n      1347\n      253\n      202\n      ...\n      240\n      440\n      1054\n      175\n      853\n      81.5\n      1.776\n      25.838761\n      Overweight\n      Tall\n    \n    \n      1\n      Combat Support\n      Regular Army\n      Male\n      233\n      1395\n      326\n      220\n      1293\n      245\n      193\n      ...\n      225\n      371\n      1054\n      167\n      815\n      72.6\n      1.702\n      25.062103\n      Overweight\n      Normal\n    \n    \n      2\n      Combat Support\n      Regular Army\n      Male\n      287\n      1430\n      341\n      230\n      1327\n      256\n      196\n      ...\n      255\n      411\n      1041\n      180\n      831\n      92.9\n      1.735\n      30.861480\n      Overweight\n      Normal\n    \n    \n      3\n      Combat Service Support\n      Regular Army\n      Male\n      234\n      1347\n      310\n      230\n      1239\n      262\n      199\n      ...\n      205\n      399\n      968\n      176\n      793\n      79.4\n      1.655\n      28.988417\n      Overweight\n      Normal\n    \n    \n      4\n      Combat Service Support\n      Regular Army\n      Male\n      250\n      1585\n      372\n      247\n      1478\n      267\n      224\n      ...\n      214\n      379\n      1245\n      188\n      954\n      94.6\n      1.914\n      25.823034\n      Overweight\n      Tall\n    \n  \n\n5 rows × 99 columns\n\n\n\n\n\nCode\nfrom sklearn.manifold import TSNE\n\n# Non-numeric columns in the dataset\nnon_numeric = ['Branch', 'Gender', 'Component', 'BMI_class', 'Height_class']\n\n# Drop the non-numeric columns from df\ndf_numeric = df.drop(non_numeric, axis=1)\n\n# Create a t-SNE model with learning rate 50\nm = TSNE(learning_rate=50)\n\n# fit and transform the t-SNE model on the numeric dataset\ntsne_features = m.fit_transform(df_numeric)\nprint(tsne_features.shape)\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\n\n\n(6068, 2)\n\n\nt-SNE reduced the more than 90 features in the dataset to just 2 which you can now plot.\n\n\n\nTime to look at the results of your hard work. In this exercise, you will visualize the output of t-SNE dimensionality reduction on the combined male and female Ansur dataset. You’ll create 3 scatterplots of the 2 t-SNE features ('x' and 'y') which were added to the dataset df. In each scatterplot you’ll color the points according to a different categorical variable.\n\n\nCode\ndf['x'] = tsne_features[:, 0]\ndf['y'] = tsne_features[:, 1]\n\n\n\n\nCode\n# Color the points according to Army Component\nsns.scatterplot(x='x', y='y', hue='Component', data=df)\n\n\n<AxesSubplot:xlabel='x', ylabel='y'>\n\n\n\n\n\n\n\nCode\n# Color the points by Army Branch\nsns.scatterplot(x='x', y='y', hue='Branch', data=df);\n\n\n\n\n\n\n\nCode\n# Color the points by Gender\nsns.scatterplot(x='x', y='y', hue='Gender', data=df);\n\n\n\n\n\nThere is a Male and a Female cluster. t-SNE found these gender differences in body shape without being told about them explicitly! From the second plot you learned there are more males in the Combat Arms Branch."
  },
  {
    "objectID": "posts/Feature Engineering - Preprocessing/Feature Engineering.html",
    "href": "posts/Feature Engineering - Preprocessing/Feature Engineering.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "We will cover a variety of aspects of feature engineering in this section, including how to use the features already present in a dataset to create new, more useful, features, while also showing you how to encode, aggregate, and extract information from both numerical and textual features within a dataset.\nThis Feature Engineering is part of Datacamp course: Preprocessing for Machine Learning in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\nCreation of new features based on existing features\nInsight into relationships between features\nExtract and expand data\nDataset-dependent\n\n\n\nTake an exploratory look at the volunteer dataset, using the variable of that name. Which of the following columns would you want to perform a feature engineering task on?\n\n\nCode\nvolunteer = pd.read_csv('dataset/volunteer_opportunities.csv')\nvolunteer.head()\n\n\n\n\n\n\n  \n    \n      \n      opportunity_id\n      content_id\n      vol_requests\n      event_time\n      title\n      hits\n      summary\n      is_priority\n      category_id\n      category_desc\n      ...\n      end_date_date\n      status\n      Latitude\n      Longitude\n      Community Board\n      Community Council\n      Census Tract\n      BIN\n      BBL\n      NTA\n    \n  \n  \n    \n      0\n      4996\n      37004\n      50\n      0\n      Volunteers Needed For Rise Up & Stay Put! Home...\n      737\n      Building on successful events last summer and ...\n      NaN\n      NaN\n      NaN\n      ...\n      July 30 2011\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      5008\n      37036\n      2\n      0\n      Web designer\n      22\n      Build a website for an Afghan business\n      NaN\n      1.0\n      Strengthening Communities\n      ...\n      February 01 2011\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      5016\n      37143\n      20\n      0\n      Urban Adventures - Ice Skating at Lasker Rink\n      62\n      Please join us and the students from Mott Hall...\n      NaN\n      1.0\n      Strengthening Communities\n      ...\n      January 29 2011\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      5022\n      37237\n      500\n      0\n      Fight global hunger and support women farmers ...\n      14\n      The Oxfam Action Corps is a group of dedicated...\n      NaN\n      1.0\n      Strengthening Communities\n      ...\n      March 31 2012\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      5055\n      37425\n      15\n      0\n      Stop 'N' Swap\n      31\n      Stop 'N' Swap reduces NYC's waste by finding n...\n      NaN\n      4.0\n      Environment\n      ...\n      February 05 2011\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 35 columns\n\n\n\n\n\n\n\n\n\nTake a look at the hiking dataset. There are several columns here that need encoding, one of which is the Accessible column, which needs to be encoded in order to be modeled. Accessible is a binary feature, so it has two values - either Y or N - so it needs to be encoded into 1s and 0s. Use scikit-learn’s LabelEncoder method to do that transformation.\n\n\nCode\nhiking = pd.read_json('dataset/hiking.json')\nhiking.head()\n\n\n\n\n\n\n  \n    \n      \n      Prop_ID\n      Name\n      Location\n      Park_Name\n      Length\n      Difficulty\n      Other_Details\n      Accessible\n      Limited_Access\n      lat\n      lon\n    \n  \n  \n    \n      0\n      B057\n      Salt Marsh Nature Trail\n      Enter behind the Salt Marsh Nature Center, loc...\n      Marine Park\n      0.8 miles\n      None\n      <p>The first half of this mile-long trail foll...\n      Y\n      N\n      NaN\n      NaN\n    \n    \n      1\n      B073\n      Lullwater\n      Enter Park at Lincoln Road and Ocean Avenue en...\n      Prospect Park\n      1.0 mile\n      Easy\n      Explore the Lullwater to see how nature thrive...\n      N\n      N\n      NaN\n      NaN\n    \n    \n      2\n      B073\n      Midwood\n      Enter Park at Lincoln Road and Ocean Avenue en...\n      Prospect Park\n      0.75 miles\n      Easy\n      Step back in time with a walk through Brooklyn...\n      N\n      N\n      NaN\n      NaN\n    \n    \n      3\n      B073\n      Peninsula\n      Enter Park at Lincoln Road and Ocean Avenue en...\n      Prospect Park\n      0.5 miles\n      Easy\n      Discover how the Peninsula has changed over th...\n      N\n      N\n      NaN\n      NaN\n    \n    \n      4\n      B073\n      Waterfall\n      Enter Park at Lincoln Road and Ocean Avenue en...\n      Prospect Park\n      0.5 miles\n      Easy\n      Trace the source of the Lake on the Waterfall ...\n      N\n      N\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\n\n# Set up the LabelEncoder object\nenc = LabelEncoder()\n\n# Apply the encoding to the \"Accessible\" column\nhiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n\n# Compare the two columns\nhiking[['Accessible', 'Accessible_enc']].head()\n\n\n\n\n\n\n  \n    \n      \n      Accessible\n      Accessible_enc\n    \n  \n  \n    \n      0\n      Y\n      1\n    \n    \n      1\n      N\n      0\n    \n    \n      2\n      N\n      0\n    \n    \n      3\n      N\n      0\n    \n    \n      4\n      N\n      0\n    \n  \n\n\n\n\n\n\n\nOne of the columns in the volunteer dataset, category_desc, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use Pandas’ get_dummies() function to do so.\n\n\nCode\n# Transform the category_desc column\ncategory_enc = pd.get_dummies(volunteer['category_desc'])\n\n# Take a look at the encoded columns\nprint(category_enc.head())\n\n\n   Education  Emergency Preparedness  Environment  Health  \\\n0          0                       0            0       0   \n1          0                       0            0       0   \n2          0                       0            0       0   \n3          0                       0            0       0   \n4          0                       0            1       0   \n\n   Helping Neighbors in Need  Strengthening Communities  \n0                          0                          0  \n1                          0                          1  \n2                          0                          1  \n3                          0                          1  \n4                          0                          0  \n\n\n\n\n\n\n\n\nA good use case for taking an aggregate statistic to create a new feature is to take the mean of columns. Here, you have a DataFrame of running times named running_times_5k. For each name in the dataset, take the mean of their 5 run times.\n\n\nCode\nrunning_times_5k = pd.read_csv('dataset/running_times_5k.csv')\nrunning_times_5k\n\n\n\n\n\n\n  \n    \n      \n      name\n      run1\n      run2\n      run3\n      run4\n      run5\n    \n  \n  \n    \n      0\n      Sue\n      20.1\n      18.5\n      19.6\n      20.3\n      18.3\n    \n    \n      1\n      Mark\n      16.5\n      17.1\n      16.9\n      17.6\n      17.3\n    \n    \n      2\n      Sean\n      23.5\n      25.1\n      25.2\n      24.6\n      23.9\n    \n    \n      3\n      Erin\n      21.7\n      21.1\n      20.9\n      22.1\n      22.2\n    \n    \n      4\n      Jenny\n      25.8\n      27.1\n      26.1\n      26.7\n      26.9\n    \n    \n      5\n      Russell\n      30.9\n      29.6\n      31.4\n      30.4\n      29.9\n    \n  \n\n\n\n\n\n\nCode\n# Create a list of the columns to average\nrun_columns = ['run1', 'run2', 'run3', 'run4', 'run5']\n\n# Use apply to create a mean column\nrunning_times_5k['mean'] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n\n# Take a look at the results\nprint(running_times_5k)\n\n\n      name  run1  run2  run3  run4  run5   mean\n0      Sue  20.1  18.5  19.6  20.3  18.3  19.36\n1     Mark  16.5  17.1  16.9  17.6  17.3  17.08\n2     Sean  23.5  25.1  25.2  24.6  23.9  24.46\n3     Erin  21.7  21.1  20.9  22.1  22.2  21.60\n4    Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n5  Russell  30.9  29.6  31.4  30.4  29.9  30.44\n\n\n\n\n\nThere are several columns in the volunteer dataset comprised of datetimes. Let’s take a look at the start_date_date column and extract just the month to use as a feature for modeling.\n\n\nCode\n# First, convert string column to date column\nvolunteer['start_date_converted'] = pd.to_datetime(volunteer['start_date_date'])\n\n# Extract just the month from the converted column\nvolunteer['start_date_month'] = volunteer['start_date_converted'].apply(lambda row: row.month)\n\n# Take a look at the converted and new month columns\nvolunteer[['start_date_converted', 'start_date_month']].head()\n\n\n\n\n\n\n  \n    \n      \n      start_date_converted\n      start_date_month\n    \n  \n  \n    \n      0\n      2011-07-30\n      7\n    \n    \n      1\n      2011-02-01\n      2\n    \n    \n      2\n      2011-01-29\n      1\n    \n    \n      3\n      2011-02-14\n      2\n    \n    \n      4\n      2011-02-05\n      2\n    \n  \n\n\n\n\n\n\n\n\n\n\nThe Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We’re going to extract this mileage using regular expressions, and then use a lambda in Pandas to apply the extraction to the DataFrame.\n\n\nCode\nimport re\n\n# Write a pattern to extract numbers and decimals\ndef return_mileage(length):\n    pattern = re.compile(r'\\d+\\.\\d+')\n\n    if length == None:\n        return\n\n    # Search the text for matches\n    mile = re.match(pattern, length)\n\n    # If a value is returned, use group(0) to return the found value\n    if mile is not None:\n        return float(mile.group(0))\n\n# Apply the function to the Length column and take a look at both columns\nhiking['Length_num'] = hiking['Length'].apply(lambda row: return_mileage(row))\nhiking[['Length', 'Length_num']].head()\n\n\n\n\n\n\n  \n    \n      \n      Length\n      Length_num\n    \n  \n  \n    \n      0\n      0.8 miles\n      0.80\n    \n    \n      1\n      1.0 mile\n      1.00\n    \n    \n      2\n      0.75 miles\n      0.75\n    \n    \n      3\n      0.5 miles\n      0.50\n    \n    \n      4\n      0.5 miles\n      0.50\n    \n  \n\n\n\n\n\n\n\nLet’s transform the volunteer dataset’s title column into a text vector, to use in a prediction task in the next exercise.\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Need to drop NaN for train_test_split\nvolunteer = pd.read_csv('dataset/volunteer_opportunities.csv')\nvolunteer = volunteer.dropna(subset=['category_desc'], axis=0)\n\n# Take the title text\ntitle_text = volunteer['title']\n\n# Create the vectorizer method\ntfidf_vec = TfidfVectorizer()\n\n# Transform the text into tf-idf vectors\ntext_tfidf = tfidf_vec.fit_transform(title_text)\n\n\n\n\n\nNow that we’ve encoded the volunteer dataset’s title column into tf/idf vectors, let’s use those vectors to try to predict the category_desc column.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\n# Split the dataset according to the class distribution of category_desc\ny = volunteer['category_desc']\nX_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n\n# Fit the model to the training data\nnb.fit(X_train, y_train)\n\n# Print out the model's accuracy\nprint(nb.score(X_test, y_test))\n\n\n0.5032258064516129"
  },
  {
    "objectID": "posts/Feature extraction/Feature Extraction.html",
    "href": "posts/Feature extraction/Feature Extraction.html",
    "title": "Feature Extraction",
    "section": "",
    "text": "Principal Component Analysis (PCA), the most widely used dimensionality reduction algorithm, is covered briefly here. We will learn how and why this algorithm is so powerful, as well as how to apply it for data exploration and preprocessing as part of a modeling pipeline. We will conclude with an example of image compression.\nThis Feature Extraction is part of Datacamp course: Dimensionality Reduction in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (8, 8)\n\n\n\n\n\n\n\nfeature\n\n\n\n\nYou want to compare prices for specific products between stores. The features in the pre-loaded dataset sales_df are: storeID, product, quantity and revenue. The quantity and revenue features tell you how many items of a particular product were sold in a store and what the total revenue was. For the purpose of your analysis it’s more interesting to know the average price per product.\n\n\nCode\nsales_df = pd.read_csv('dataset/grocery_sales.csv')\nsales_df.head()\n\n\n\n\n\n\n  \n    \n      \n      storeID\n      product\n      quantity\n      revenue\n    \n  \n  \n    \n      0\n      A\n      Apples\n      1811\n      9300.6\n    \n    \n      1\n      A\n      Bananas\n      1003\n      3375.2\n    \n    \n      2\n      A\n      Oranges\n      1604\n      8528.5\n    \n    \n      3\n      B\n      Apples\n      1785\n      9181.0\n    \n    \n      4\n      B\n      Bananas\n      944\n      3680.2\n    \n  \n\n\n\n\n\n\nCode\n# Calculate the price from the quantity sold and revenue\nsales_df['price'] = sales_df['revenue'] / sales_df['quantity']\n\n# Drop the quantity and revenue features\nreduced_df = sales_df.drop(['revenue', 'quantity'], axis=1)\n\nreduced_df.head()\n\n\n\n\n\n\n  \n    \n      \n      storeID\n      product\n      price\n    \n  \n  \n    \n      0\n      A\n      Apples\n      5.135616\n    \n    \n      1\n      A\n      Bananas\n      3.365105\n    \n    \n      2\n      A\n      Oranges\n      5.317020\n    \n    \n      3\n      B\n      Apples\n      5.143417\n    \n    \n      4\n      B\n      Bananas\n      3.898517\n    \n  \n\n\n\n\n\n\n\nYou’re working on a variant of the ANSUR dataset, height_df, where a person’s height was measured 3 times. Add a feature with the mean height to the dataset, then drop the 3 original features.\n\n\nCode\nheight_df = pd.read_csv('dataset/height_df.csv')\nheight_df.head()\n\n\n\n\n\n\n  \n    \n      \n      weight_kg\n      height_1\n      height_2\n      height_3\n    \n  \n  \n    \n      0\n      81.5\n      1.78\n      1.80\n      1.80\n    \n    \n      1\n      72.6\n      1.70\n      1.70\n      1.69\n    \n    \n      2\n      92.9\n      1.74\n      1.75\n      1.73\n    \n    \n      3\n      79.4\n      1.66\n      1.68\n      1.67\n    \n    \n      4\n      94.6\n      1.91\n      1.93\n      1.90\n    \n  \n\n\n\n\n\n\nCode\n# Calculate the mean height\nheight_df['height'] = height_df[['height_1', 'height_2', 'height_3']].mean(axis=1)\n\n# Drop the 3 original height features\nreduced_df = height_df.drop(['height_1', 'height_2', 'height_3'], axis=1)\n\nreduced_df.head()\n\n\n\n\n\n\n  \n    \n      \n      weight_kg\n      height\n    \n  \n  \n    \n      0\n      81.5\n      1.793333\n    \n    \n      1\n      72.6\n      1.696667\n    \n    \n      2\n      92.9\n      1.740000\n    \n    \n      3\n      79.4\n      1.670000\n    \n    \n      4\n      94.6\n      1.913333\n    \n  \n\n\n\n\n\n\n\n\n\nPCA concept \n\n\n\nYou’ll visually inspect a 4 feature sample of the ANSUR dataset before and after PCA using Seaborn’s pairplot(). This will allow you to inspect the pairwise correlations between the features.\n\n\nCode\nansur_df = pd.read_csv('dataset/ansur_sample.csv')\n\n\n\n\nCode\n# Create a pairplot to inspect ansur_df\nsns.pairplot(ansur_df);\n\n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Create the scaler and standardize the data\nscaler = StandardScaler()\nansur_std = scaler.fit_transform(ansur_df)\n\n# Create the PCA instance and fit and transform the data with pca\npca = PCA()\npc = pca.fit_transform(ansur_std)\n\n# This changes the numpy array output back to a dataframe\npc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n\n# Create a pairplot of the pricipal component dataframe\nsns.pairplot(pc_df);\nprint(\"\\nNotice how, in contrast to the input features, none of the principal components are correlated to one another.\")\n\n\n\nNotice how, in contrast to the input features, none of the principal components are correlated to one another.\n\n\n\n\n\n\n\n\nYou’ll now apply PCA on a somewhat larger ANSUR datasample with 13 dimensions. The fitted model will be used in the next exercise. Since we are not using the principal components themselves there is no need to transform the data, instead, it is sufficient to fit pca to the data.\n\n\nCode\ndf = pd.read_csv('./dataset/ANSUR_II_MALE.csv')\nansur_df = df[['stature_m', 'buttockheight', 'waistdepth', 'span',\n               'waistcircumference', 'shouldercircumference', 'footlength',\n               'handlength', 'functionalleglength', 'chestheight',\n               'chestcircumference', 'cervicaleheight', 'sittingheight']]\n\n\n\n\nCode\n# Scale the data\nscaler = StandardScaler()\nansur_std = scaler.fit_transform(ansur_df)\n\n# Apply PCA\npca = PCA()\npca.fit(ansur_std)\n\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA()\n\n\nYou’ve fitted PCA on our 13 feature datasample. Now let’s see how the components explain the variance\n\n\n\nYou’ll be inspecting the variance explained by the different principal components of the pca instance you created in the previous exercise.\n\n\nCode\n# Inspect the explained variance ratio per component\nprint(pca.explained_variance_ratio_)\n\n\n[0.57832831 0.2230137  0.06404218 0.04252456 0.0278581  0.01761021\n 0.01681037 0.01014147 0.00706488 0.00607973 0.00344643 0.00228095\n 0.00079911]\n\n\n\n\nCode\n# Print the cumulative sum of the explained variance ratio\nprint(pca.explained_variance_ratio_.cumsum())\n\n\n[0.57832831 0.801342   0.86538419 0.90790875 0.93576684 0.95337706\n 0.97018743 0.9803289  0.98739378 0.99347351 0.99691994 0.99920089\n 1.        ]\n\n\nBased on the data, we can use 4 principal components if we don’t want to lose more than 10% of explained variance during dimensionality reduction. Using just 4 principal components we can explain more than 90% of the variance in the 13 feature dataset.\n\n\n\n\n\n\nYou’ll apply PCA to the numeric features of the Pokemon dataset, poke_df, using a pipeline to combine the feature scaling and PCA in one go. You’ll then interpret the meanings of the first two components.\n\n\nCode\ndf = pd.read_csv('dataset/pokemon.csv')\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      #\n      Name\n      Type 1\n      Type 2\n      Total\n      HP\n      Attack\n      Defense\n      Sp. Atk\n      Sp. Def\n      Speed\n      Generation\n      Legendary\n    \n  \n  \n    \n      0\n      1\n      Bulbasaur\n      Grass\n      Poison\n      318\n      45\n      49\n      49\n      65\n      65\n      45\n      1\n      False\n    \n    \n      1\n      2\n      Ivysaur\n      Grass\n      Poison\n      405\n      60\n      62\n      63\n      80\n      80\n      60\n      1\n      False\n    \n    \n      2\n      3\n      Venusaur\n      Grass\n      Poison\n      525\n      80\n      82\n      83\n      100\n      100\n      80\n      1\n      False\n    \n    \n      3\n      3\n      VenusaurMega Venusaur\n      Grass\n      Poison\n      625\n      80\n      100\n      123\n      122\n      120\n      80\n      1\n      False\n    \n    \n      4\n      4\n      Charmander\n      Fire\n      NaN\n      309\n      39\n      52\n      43\n      60\n      50\n      65\n      1\n      False\n    \n  \n\n\n\n\n\n\nCode\npoke_df = df[['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']]\npoke_df.head()\n\n\n\n\n\n\n  \n    \n      \n      HP\n      Attack\n      Defense\n      Sp. Atk\n      Sp. Def\n      Speed\n    \n  \n  \n    \n      0\n      45\n      49\n      49\n      65\n      65\n      45\n    \n    \n      1\n      60\n      62\n      63\n      80\n      80\n      60\n    \n    \n      2\n      80\n      82\n      83\n      100\n      100\n      80\n    \n    \n      3\n      80\n      100\n      123\n      122\n      120\n      80\n    \n    \n      4\n      39\n      52\n      43\n      60\n      50\n      65\n    \n  \n\n\n\n\n\n\nCode\nfrom sklearn.pipeline import Pipeline\n\n# Build the pipeline\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=2))\n])\n\n# Fit it to the dataset and extract the component vectors\npipe.fit(poke_df)\nvectors = pipe.steps[1][1].components_.round(2)\n\n# Print feature effects\nprint('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\nprint('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n\n\nPC 1 effects = {'HP': 0.39, 'Attack': 0.44, 'Defense': 0.36, 'Sp. Atk': 0.46, 'Sp. Def': 0.45, 'Speed': 0.34}\nPC 2 effects = {'HP': 0.08, 'Attack': -0.01, 'Defense': 0.63, 'Sp. Atk': -0.31, 'Sp. Def': 0.24, 'Speed': -0.67}\n\n\nIn PC1, All features have a similar positive effect. PC 1 can be interpreted as a measure of overall quality (high stats). In contrast, PC2’s defense has a strong positive effect on the second component and speed a strong negative one. This component quantifies an agility vs. armor & protection trade-off.\n\n\n\nYou’ll use the PCA pipeline you’ve built in the previous exercise to visually explore how some categorical features relate to the variance in poke_df. These categorical features (Type & Legendary) can be found in a separate dataframe poke_cat_df.\n\n\nCode\npoke_cat_df = df[['Type 1', 'Legendary']]\n\n\n\n\nCode\n# Build the pipeline\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=2))\n])\n\n# Fit the pipeline to poke_df and transform the data\npc = pipe.fit_transform(poke_df)\n\nprint(pc)\n\n\n[[-1.5563747  -0.02148212]\n [-0.36286656 -0.05026854]\n [ 1.28015158 -0.06272022]\n ...\n [ 2.45821626 -0.51588158]\n [ 3.5303971  -0.95106516]\n [ 2.23378629  0.53762985]]\n\n\n\n\nCode\n# Add the 2 components to poke_cat_df\npoke_cat_df.loc[:, 'PC 1'] = pc[:, 0]\npoke_cat_df.loc[:, 'PC 2'] = pc[:, 1]\n\npoke_cat_df.head()\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_21512\\3952925675.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  poke_cat_df.loc[:, 'PC 1'] = pc[:, 0]\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_21512\\3952925675.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  poke_cat_df.loc[:, 'PC 2'] = pc[:, 1]\n\n\n\n\n\n\n  \n    \n      \n      Type 1\n      Legendary\n      PC 1\n      PC 2\n    \n  \n  \n    \n      0\n      Grass\n      False\n      -1.556375\n      -0.021482\n    \n    \n      1\n      Grass\n      False\n      -0.362867\n      -0.050269\n    \n    \n      2\n      Grass\n      False\n      1.280152\n      -0.062720\n    \n    \n      3\n      Grass\n      False\n      2.620916\n      0.704263\n    \n    \n      4\n      Fire\n      False\n      -1.758284\n      -0.706179\n    \n  \n\n\n\n\n\n\nCode\n# Use the Type feature to color the PC 1 vs. PC 2 scatterplot\nsns.scatterplot(data=poke_cat_df, x='PC 1', y='PC 2', hue='Type 1');\n\n\n\n\n\n\n\nCode\n# Use the Legendary feature to color the PC 1 vs. PC 2 scatterplot\nsns.scatterplot(data=poke_cat_df, x='PC 1', y='PC 2', hue='Legendary');\n\n\n\n\n\nLooks like the different types are scattered all over the place while the legendary Pokemon always score high for PC 1 meaning they have high stats overall. Their spread along the PC 2 axis tells us they aren’t consistently fast and vulnerable or slow and armored.\n\n\n\nWe just saw that legendary Pokemon tend to have higher stats overall. Let’s see if we can add a classifier to our pipeline that detects legendary versus non-legendary Pokemon based on the principal components.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX = poke_df\ny = df['Legendary']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\n\n\nCode\n# Build the pipeline\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=2)),\n    ('classifier', RandomForestClassifier(random_state=0))\n])\n\n# Fit the pipeline to the training data\npipe.fit(X_train, y_train)\n\n# Prints the explained variance ratio\nprint(pipe.steps[1][1].explained_variance_ratio_)\n\n# Score the acuracy on the test set\naccuracy = pipe.score(X_test, y_test)\n\n# Prints the model accuracy\nprint('{0:.1%} test set accuracy'.format(accuracy))\n\n\n[0.45673596 0.18599109]\n92.1% test set accuracy\n\n\nRepeat the process with 3 extracted components.\n\n\nCode\n# Build the pipeline\npipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('reducer', PCA(n_components=3)),\n        ('classifier', RandomForestClassifier(random_state=0))])\n\n# Fit the pipeline to the training data\npipe.fit(X_train, y_train)\n\n# Score the accuracy on the test set\naccuracy = pipe.score(X_test, y_test)\n\n# Prints the explained variance ratio and accuracy\nprint(pipe.steps[1][1].explained_variance_ratio_)\nprint('{0:.1%} test set accuracy'.format(accuracy))\n\n\n[0.45673596 0.18599109 0.12852181]\n93.8% test set accuracy\n\n\n\n\nCode\n# Build the pipeline\npipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('reducer', PCA(n_components=4)),\n        ('classifier', RandomForestClassifier(random_state=0))])\n\n# Fit the pipeline to the training data\npipe.fit(X_train, y_train)\n\n# Score the accuracy on the test set\naccuracy = pipe.score(X_test, y_test)\n\n# Prints the explained variance ratio and accuracy\nprint(pipe.steps[1][1].explained_variance_ratio_)\nprint('{0:.1%} test set accuracy'.format(accuracy))\n\n\n[0.45673596 0.18599109 0.12852181 0.11442161]\n95.0% test set accuracy\n\n\n\n\n\n\n\nPCA operations \n\n\n\nYou’ll let PCA determine the number of components to calculate based on an explained variance threshold that you decide.\n\n\nCode\nansur_df = pd.read_csv('dataset/ANSUR_II_FEMALE.csv')\nansur_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Branch\n      Component\n      Gender\n      abdominalextensiondepthsitting\n      acromialheight\n      acromionradialelength\n      anklecircumference\n      axillaheight\n      balloffootcircumference\n      balloffootlength\n      ...\n      waistdepth\n      waistfrontlengthsitting\n      waistheightomphalion\n      wristcircumference\n      wristheight\n      weight_kg\n      stature_m\n      BMI\n      BMI_class\n      Height_class\n    \n  \n  \n    \n      0\n      Combat Support\n      Regular Army\n      Female\n      231\n      1282\n      301\n      204\n      1180\n      222\n      177\n      ...\n      217\n      345\n      942\n      152\n      756\n      65.7\n      1.560\n      26.997041\n      Overweight\n      Normal\n    \n    \n      1\n      Combat Service Support\n      Regular Army\n      Female\n      194\n      1379\n      320\n      207\n      1292\n      225\n      178\n      ...\n      168\n      329\n      1032\n      155\n      815\n      53.4\n      1.665\n      19.262506\n      Normal\n      Normal\n    \n    \n      2\n      Combat Service Support\n      Regular Army\n      Female\n      183\n      1369\n      329\n      233\n      1271\n      237\n      196\n      ...\n      159\n      367\n      1035\n      162\n      799\n      66.3\n      1.711\n      22.647148\n      Normal\n      Tall\n    \n    \n      3\n      Combat Service Support\n      Regular Army\n      Female\n      261\n      1356\n      306\n      214\n      1250\n      240\n      188\n      ...\n      235\n      371\n      999\n      173\n      818\n      78.2\n      1.660\n      28.378575\n      Overweight\n      Normal\n    \n    \n      4\n      Combat Arms\n      Regular Army\n      Female\n      309\n      1303\n      308\n      214\n      1210\n      217\n      182\n      ...\n      300\n      380\n      911\n      152\n      762\n      88.6\n      1.572\n      35.853259\n      Overweight\n      Normal\n    \n  \n\n5 rows × 99 columns\n\n\n\n\n\nCode\nansur_df.drop(['Gender', 'Branch', 'Component', 'BMI_class', 'Height_class'],\n              axis=1, inplace=True)\nansur_df.shape\n\n\n(1986, 94)\n\n\n\n\nCode\n# Pipe a scaler to PCA selecting 80% of the variance\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=0.8))\n])\n\n# Fit the pipe to the data\npipe.fit(ansur_df)\n\nprint('{} components selected'.format(len(pipe.steps[1][1].components_)))\n\n\n11 components selected\n\n\n\n\nCode\n# Pipe a scaler to PCA selecting 90% of the variance\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=0.9))\n])\n\n# Fit the pipe to the data\npipe.fit(ansur_df)\n\nprint('{} components selected'.format(len(pipe.steps[1][1].components_)))\n\n\n23 components selected\n\n\nFrom the result, we need more than 12 components to go from 80% to 90% explained variance.\n\n\n\nYou’ll now make a more informed decision on the number of principal components to reduce your data to using the “elbow in the plot” technique.\n\n\nCode\n# Pipeline a scaler and PCA selecting 10 components\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=10))\n])\n\n# Fit the pipe to the data\npipe.fit(ansur_df)\n\n# Plot the explained variance ratio\nplt.plot(pipe.steps[1][1].explained_variance_ratio_);\nplt.xlabel('Principal component index');\nplt.ylabel('Explained variance ratio');\nplt.title('Elbow plot of Explained variance ratio');\nplt.grid(True);\n\n\n\n\n\n\n\n\nYou’ll reduce the size of 16 images with hand written digits (MNIST dataset) using PCA.\nThe samples are 28 by 28 pixel gray scale images that have been flattened to arrays with 784 elements each (28 x 28 = 784) and added to the 2D numpy array X_test. Each of the 784 pixels has a value between 0 and 255 and can be regarded as a feature.\nA pipeline with a scaler and PCA model to select 78 components has been pre-loaded for you as pipe. This pipeline has already been fitted to the entire MNIST dataset except for the 16 samples in X_test.\n\n\nCode\ndef plot_digits(data):\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6),\n                             subplot_kw={'xticks':[], 'yticks':[]},\n                             gridspec_kw=dict(hspace=0.05, wspace=0.05))\n    for i, ax in enumerate(axes.flat):\n        ax.imshow(data[i].reshape(28, 28),\n                  cmap='binary',\n                  clim=(0, 300))\n\n\n\n\nCode\nfrom sklearn.datasets import fetch_openml\n\nX, y = X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nX_sample = X_test[:1600:100]\n\n\n\n\nCode\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=78))\n])\n\npipe.fit(X_train)\n\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('reducer', PCA(n_components=78))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('reducer', PCA(n_components=78))])StandardScalerStandardScaler()PCAPCA(n_components=78)\n\n\n\n\nCode\nX_sample.shape\n\n\n(16, 784)\n\n\n\n\nCode\n# Plot the MNIST sample data\nplot_digits(X_sample)\n\n\nKeyError: 0\n\n\n\n\n\n\n\nCode\n# Transform the input data to principal components\npc = pipe.transform(X_sample)\n\n# Prints the number of features per dataset\nprint(\"X_test has {} features\".format(X_sample.shape[1]))\nprint(\"pc has {} features\".format(pc.shape[1]))\n\n\nX_test has 784 features\npc has 78 features\n\n\n\n\nCode\n# Inverse transform the components to original feature space\nX_rebuilt = pipe.inverse_transform(pc)\n\n# Prints the number of features\nprint(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n\n\nX_rebuilt has 784 features\n\n\n\n\nCode\n# Plot the reconstructed data\nplot_digits(X_rebuilt)\n\n\n\n\n\nYou’ve reduced the size of the data 10 fold but were able to reconstruct images with reasonable quality."
  },
  {
    "objectID": "posts/Feature selection I - selection for feature information/Feature selection I - selection for feature information.html",
    "href": "posts/Feature selection I - selection for feature information/Feature selection I - selection for feature information.html",
    "title": "Feature Selection I - Selecting for Feature Information",
    "section": "",
    "text": "As we progress through feature selection, we’ll learn how dimensionality reduction can help us overcome its curse. We’ll be introduced to a variety of techniques for identifying and removing features that don’t add much value to your data. Either because they have little variance, too many missing values, or because they are strongly correlated with other features.\nThis Feature Selection I - Selecting for Feature Information is part of Datacamp course: Dimensionality Reduction in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (7, 7)\n\n\n\n\n\n\nWe will keep working with the ANSUR dataset. Before we can build a model on our dataset, we should first decide on which feature you want to predict. In this case, we’re trying to predict gender.\nwe need to extract the column holding this feature from the dataset and then split the data into a training and test set. The training set will be used to train the model and the test set will be used to check its performance on unseen data.\n\n\nCode\nansur_male = pd.read_csv('dataset/ANSUR_II_MALE.csv')\nansur_female = pd.read_csv('dataset/ANSUR_II_FEMALE.csv')\n\nansur_df = pd.concat([ansur_male, ansur_female])\n# unused columns in the dataset\nunused = ['Branch', 'Component', 'BMI_class', 'Height_class', 'BMI', 'weight_kg', 'stature_m']\n\n# Drop the non-numeric columns from df\nansur_df.drop(unused, axis=1, inplace=True)\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n# Select the Gender column as the feature to be predict (y)\ny = ansur_df['Gender']\n\n# Remove the Gender column to create the training data\nX = ansur_df.drop('Gender', axis=1)\n\n# Perform a 70% train and 30% test data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(\n    X_test.shape[0], X_train.shape[0], X_test.shape[1]\n))\n\n\n1821 rows in test set vs. 4247 in training set. 91 Features.\n\n\n\n\n\nAbove we split the dataset into X_train, X_test, y_train, and y_test. These datasets have been pre-loaded for you. We’ll now create a support vector machine classifier model (SVC()) and fit that to the training data. You’ll then calculate the accuracy on both the test and training set to detect overfitting.\n\n\nCode\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Create an instance of the Support Vector Classification class\nsvc = SVC()\n\n# Fit the model to the training data\nsvc.fit(X_train, y_train)\n\n# Calculate accuracy scores on both train and test data\naccuracy_train = accuracy_score(y_train, svc.predict(X_train))\naccuracy_test = accuracy_score(y_test, svc.predict(X_test))\n\nprint(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test,\n                                                                       accuracy_train))\nprint(\"\\nCurrent data doesn't show overfitting. But example in datacamp shows overfitting from dataset.\")\n\n\n98.9% accuracy on test set vs. 99.0% on training set\n\nCurrent data doesn't show overfitting. But example in datacamp shows overfitting from dataset.\n\n\n\n\nCode\nansur_df_overfit = pd.read_csv('dataset/ansur_overfit.csv')\n\n# Select the Gender column as the feature to be predict (y)\ny = ansur_df_overfit['Gender']\n\n# Remove the Gender column to create the training data\nX = ansur_df_overfit.drop('Gender', axis=1)\n\n# Perform a 70% train and 30% test data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(\n    X_test.shape[0], X_train.shape[0], X_test.shape[1]\n))\n\n# Create an instance of the Support Vector Classification class\nsvc = SVC()\n\n# Fit the model to the training data\nsvc.fit(X_train, y_train)\n\n# Calculate accuracy scores on both train and test data\naccuracy_train = accuracy_score(y_train, svc.predict(X_train))\naccuracy_test = accuracy_score(y_test, svc.predict(X_test))\n\nprint(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test,\n                                                                       accuracy_train))\n\n\n300 rows in test set vs. 700 in training set. 91 Features.\n91.0% accuracy on test set vs. 94.9% on training set\n\n\n\n\n\nYou’ll reduce the overfit with the help of dimensionality reduction. In this case, you’ll apply a rather drastic form of dimensionality reduction by only selecting a single column that has some good information to distinguish between genders. You’ll repeat the train-test split, model fit and prediction steps to compare the accuracy on test vs. training data.\n\n\nCode\n# Assign just the 'neckcircumferencebase' column from ansur_df to X\nX = ansur_df_overfit[['neckcircumferencebase']]\n\n# SPlit the data, instantiate a classifier and fit the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nsvc = SVC()\nsvc.fit(X_train, y_train)\n\n# Calculate accuracy scores on both train and test data\naccuracy_train = accuracy_score(y_train, svc.predict(X_train))\naccuracy_test = accuracy_score(y_test, svc.predict(X_test))\n\nprint(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test,\n                                                                       accuracy_train))\nprint(\"\\nOn the full dataset the model is overfitted but with a single feature we can make good predictions? This is an example of the curse of dimensionality! The model badly overfits when we feed it too many features. It overlooks that neck circumference by itself is pretty different for males and females.\")\n\n\n93.7% accuracy on test set vs. 94.6% on training set\n\nOn the full dataset the model is overfitted but with a single feature we can make good predictions? This is an example of the curse of dimensionality! The model badly overfits when we feed it too many features. It overlooks that neck circumference by itself is pretty different for males and females.\n\n\n\n\n\n\n    \n\n\nYou’ll be working on a slightly modified subsample of the ANSUR dataset with just head measurements\n\n\nCode\nhead_df = pd.read_csv('dataset/head_df.csv')\nhead_df.head()\n\n\n\n\n\n\n  \n    \n      \n      headbreadth\n      headcircumference\n      headlength\n      tragiontopofhead\n      n_hairs\n      measurement_error\n    \n  \n  \n    \n      0\n      150\n      583\n      206\n      140\n      100016.243454\n      0.1\n    \n    \n      1\n      146\n      568\n      201\n      120\n      99993.882436\n      0.1\n    \n    \n      2\n      148\n      573\n      202\n      125\n      99994.718282\n      0.1\n    \n    \n      3\n      158\n      576\n      199\n      127\n      99989.270314\n      0.1\n    \n    \n      4\n      153\n      566\n      197\n      122\n      100008.654076\n      0.1\n    \n  \n\n\n\n\n\n\nCode\n# Create the boxplot\nfig, ax = plt.subplots(figsize=(10, 5));\nhead_df.boxplot(ax=ax);\n\n\n\n\n\n\n\nCode\n# Normalize the data\nnormalized_df = head_df / head_df.mean()\n\n# Print the variances of the normalized data\nprint(normalized_df.var())\n\nfig, ax = plt.subplots(figsize=(10, 10));\nnormalized_df.boxplot(ax=ax);\n\n\nheadbreadth          1.678952e-03\nheadcircumference    1.029623e-03\nheadlength           1.867872e-03\ntragiontopofhead     2.639840e-03\nn_hairs              1.002552e-08\nmeasurement_error    0.000000e+00\ndtype: float64\n\n\n\n\n\n\n\n\nEarlier we established that 0.001 is a good threshold to filter out low variance features in head_df after normalization. Now use the VarianceThreshold feature selector to remove these features.\n\n\nCode\nfrom sklearn.feature_selection import VarianceThreshold\n\n# Create a VarianceThreshold feature selector\nsel = VarianceThreshold(threshold=0.001)\n\n# Fit the selector to normalized head_df\nsel.fit(head_df / head_df.mean())\n\n# Create a boolean mask\nmask = sel.get_support()\n\n# Apply the mask to create a reduced dataframe\nreduced_df = head_df.loc[:, mask]\n\nprint(\"Dimensionality reduced from {} to {}\".format(head_df.shape[1], reduced_df.shape[1]))\n\n\nDimensionality reduced from 6 to 4\n\n\n\n\n\nWe will apply feature selection on the Boston Public Schools dataset which has been pre-loaded as school_df. Calculate the missing value ratio per feature and then create a mask to remove features with many missing values.\n\n\nCode\nschool_df = pd.read_csv('dataset/Public_Schools2.csv')\n\n\n\n\nCode\n# Create a boolean mask on whether each feature less than 50% missing values\nmask = school_df.isna().sum() / len(school_df) < 0.5\n\n# Create a reduced dataset by applying the mask\nreduced_df = school_df.loc[:, mask]\n\nprint(school_df.shape)\nprint(reduced_df.shape)\n\n\n(131, 21)\n(131, 19)\n\n\n\n\n\n\n\nCorrelation coefficient (r) \n\n\n\nReading the correlation matrix of ansur_df in its raw, numeric format doesn’t allow us to get a quick overview. Let’s improve this by removing redundant values and visualizing the matrix using seaborn.\n\n\nCode\nansur_df_sample = ansur_df[['elbowrestheight', 'wristcircumference', 'anklecircumference',\n                            'buttockheight', 'crotchheight']]\nansur_df_sample.columns = ['Elbow rest height', 'Wrist circumference',\n                           'Ankle circumference', 'Buttock height', 'Crotch height']\nansur_df_sample.head()\n\n\n\n\n\n\n  \n    \n      \n      Elbow rest height\n      Wrist circumference\n      Ankle circumference\n      Buttock height\n      Crotch height\n    \n  \n  \n    \n      0\n      247\n      175\n      222\n      882\n      877\n    \n    \n      1\n      232\n      167\n      220\n      870\n      851\n    \n    \n      2\n      237\n      180\n      230\n      901\n      854\n    \n    \n      3\n      272\n      176\n      230\n      821\n      769\n    \n    \n      4\n      188\n      188\n      247\n      1080\n      1014\n    \n  \n\n\n\n\n\n\nCode\n# Create the correlation matrix\ncorr = ansur_df_sample.corr()\n\ncmap = sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\");\n\n\n\n\n\n\n\nCode\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Add the mask to the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt='.2f');\n\n\n\n\n\n\n\n\n\n\n\nWe’re going to automate the removal of highly correlated features in the numeric ANSUR dataset. We’ll calculate the correlation matrix and filter out columns that have a correlation coefficient of more than 0.95 or less than -0.95.\nSince each correlation coefficient occurs twice in the matrix (correlation of A to B equals correlation of B to A) you’ll want to ignore half of the correlation matrix so that only one of the two correlated features is removed. Use a mask trick for this purpose.\n\n\nCode\nansur_male = pd.read_csv('dataset/ANSUR_II_MALE.csv')\nansur_df = ansur_male\n\n\n\n\nCode\n# Calculate the correlation matrix and take the absolute value\ncorr_matrix = ansur_df.corr().abs()\n\n# Create a True/False mask and apply it\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ntri_df = corr_matrix.mask(mask)\n\n# List column names of highly correlated features (r > 0.95)\nto_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n\n# Drop the features in the to_drop list\nreduced_df = ansur_df.drop(to_drop, axis=1)\n\nprint(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))\n\n\nThe reduced dataframe has 88 columns.\n\n\n\n\n\nThe dataset that has been pre-loaded for you as weird_df contains actual data provided by the US Centers for Disease Control & Prevention and Department of Energy.\nLet’s see if we can find a pattern.\n\n\nCode\nweird_df = pd.read_csv('dataset/weird_df.csv')\n\n\n\n\nCode\n# Print the first five lines of weird_df\nprint(weird_df.head())\n\n\n   pool_drownings  nuclear_energy\n0             421           728.3\n1             465           753.9\n2             494           768.8\n3             538           780.1\n4             430           763.7\n\n\n\n\nCode\n# Put nuclear energy production on the x-axis and the number of pool drownings on the y-axis\nsns.scatterplot(x='nuclear_energy', y='pool_drownings', data=weird_df);\n\n\n\n\n\n\n\nCode\n# Print out the correlation matrix of weird_df\nprint(weird_df.corr())\n\n\n                pool_drownings  nuclear_energy\npool_drownings        1.000000        0.901179\nnuclear_energy        0.901179        1.000000"
  },
  {
    "objectID": "posts/Feature selection II - selecting for model accuracy/Feature Selection II - Selecting for Model Accuracy.html",
    "href": "posts/Feature selection II - selecting for model accuracy/Feature Selection II - Selecting for Model Accuracy.html",
    "title": "Feature Selection II - Selecting for Model Accuracy",
    "section": "",
    "text": "We’ll explore how to use models to identify the most important features in a dataset in order to predict certain targets. We then concludes with a lesson in which we will decide which features to keep based on advice from multiple, different models.\nThis Feature Selection II - Selecting for Model Accuracy is part of Datacamp course: Dimensionality Reduction in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (8, 8)\n\n\n\n\n\n\nYou’ll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset.\n\n\nCode\ndiabetes_df = pd.read_csv('dataset/PimaIndians.csv')\ndiabetes_df.head()\n\n\n\n\n\n\n  \n    \n      \n      pregnant\n      glucose\n      diastolic\n      triceps\n      insulin\n      bmi\n      family\n      age\n      test\n    \n  \n  \n    \n      0\n      1\n      89\n      66\n      23\n      94\n      28.1\n      0.167\n      21\n      negative\n    \n    \n      1\n      0\n      137\n      40\n      35\n      168\n      43.1\n      2.288\n      33\n      positive\n    \n    \n      2\n      3\n      78\n      50\n      32\n      88\n      31.0\n      0.248\n      26\n      positive\n    \n    \n      3\n      2\n      197\n      70\n      45\n      543\n      30.5\n      0.158\n      53\n      positive\n    \n    \n      4\n      1\n      189\n      60\n      23\n      846\n      30.1\n      0.398\n      59\n      positive\n    \n  \n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom pprint import pprint\n\nX, y = diabetes_df.iloc[:, :-1], diabetes_df.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nscaler = StandardScaler()\nlr = LogisticRegression()\n\n\n\n\nCode\n# Fit the scaler on the training features and transform these in one go\nX_train_std = scaler.fit_transform(X_train)\n\n# Fit the logistic regression model on the scaled training data\nlr.fit(X_train_std, y_train)\n\n# Scaler the test features\nX_test_std = scaler.transform(X_test)\n\n# Predict diabetes presence on the scaled test set\ny_pred = lr.predict(X_test_std)\n\n# Print accuracy metrics and feature coefficients\nprint(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred)))\npprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\nprint(\"\\nWe get almost 80% accuracy on the test set. Take a look at the differences in model coefficients for the different features.\")\n\n\n75.4% accuracy on test set.\n{'age': 0.4,\n 'bmi': 0.61,\n 'diastolic': 0.1,\n 'family': 0.61,\n 'glucose': 1.03,\n 'insulin': 0.19,\n 'pregnant': 0.26,\n 'triceps': 0.07}\n\nWe get almost 80% accuracy on the test set. Take a look at the differences in model coefficients for the different features.\n\n\n\n\n\nNow that we’ve created a diabetes classifier, let’s see if we can reduce the number of features without hurting the model accuracy too much.\nOn the second line of code the features are selected from the original dataframe. Adjust this selection.\n\n\nCode\n# Remove the feature with the lowest model coefficient\nX = diabetes_df[['pregnant', 'glucose', 'triceps',\n                 'insulin', 'bmi', 'family', 'age']]\n\n# Performs a 25-75% train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Scales features and fits the logistic regression model\nlr.fit(scaler.fit_transform(X_train), y_train)\n\n# Calculate the accuracy on the test set and prints coefficients\nacc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\nprint(\"{0: .1%} accuracy on test set.\".format(acc))\npprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n\n\n 80.6% accuracy on test set.\n{'age': 0.35,\n 'bmi': 0.39,\n 'family': 0.34,\n 'glucose': 1.24,\n 'insulin': 0.2,\n 'pregnant': 0.05,\n 'triceps': 0.24}\n\n\n\n\nCode\n# Remove the 2 features with the lowest model coefficients\nX = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n\n# Performs a 25-75% train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Scales features and fits the logistic regression model\nlr.fit(scaler.fit_transform(X_train), y_train)\n\n# Calculates the accuracy on the test set and prints coefficients\nacc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\nprint(\"{0:.1%} accuracy on test set.\".format(acc))\npprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n\n\n79.6% accuracy on test set.\n{'age': 0.37, 'bmi': 0.34, 'family': 0.34, 'glucose': 1.13, 'triceps': 0.25}\n\n\n\n\nCode\n# Only keep the feature with the highest coefficient\nX = diabetes_df[['glucose']]\n\n# Performs a 25-75% train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Scales features and fits the logistic regression model to the data\nlr.fit(scaler.fit_transform(X_train), y_train)\n\n# Calculates the accuracy on the test set and prints coefficients\nacc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\nprint(\"{0:.1%} accuracy on test set.\".format(acc))\nprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n\n\n75.5% accuracy on test set.\n{'glucose': 1.28}\n\n\n\n\n\nNow let’s automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features.\n\n\nCode\nX, y = diabetes_df.iloc[:, :-1], diabetes_df.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nlr = LogisticRegression()\n\n# Fit the scaler on the training features and transform these in one go\nX_train_std = scaler.fit_transform(X_train)\n\n# Fit the logistic regression model on the scaled training data\nlr.fit(X_train_std, y_train)\n\n# Scaler the test features\nX_test_std = scaler.transform(X_test)\n\n\n\n\nCode\nfrom sklearn.feature_selection import RFE\n\n# Create the RFE a LogisticRegression estimator and 3 features to select\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n\n# Fits the eliminator to the data\nrfe.fit(X_train_std, y_train)\n\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X.columns, rfe.ranking_)))\n\n# Print the features that are not elimiated\nprint(X.columns[rfe.support_])\n\n# CAlculates the test set accuracy\nacc = accuracy_score(y_test, rfe.predict(X_test_std))\nprint(\"{0:.1%} accuracy on test set.\".format(acc))\n\n\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\nFitting estimator with 4 features.\n{'pregnant': 2, 'glucose': 1, 'diastolic': 6, 'triceps': 4, 'insulin': 5, 'bmi': 1, 'family': 3, 'age': 1}\nIndex(['glucose', 'bmi', 'age'], dtype='object')\n74.6% accuracy on test set.\n\n\n\n\n\n\n\nRandom forest classifier \n\n\n\nYou’ll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You’ll fit the model on the training data after performing the train-test split and consult the feature importance values.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Perform a 75% training and 25% test data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Fit the random forest model to the training data\nrf = RandomForestClassifier(random_state=0)\nrf.fit(X_train, y_train)\n\n# Calculate the accuracy\nacc = accuracy_score(y_test, rf.predict(X_test))\n\n# Print the importances per feature\npprint(dict(zip(X.columns, rf.feature_importances_.round(2))))\n\n# Print accuracy\nprint(\"{0:.1%} accuracy on test set.\".format(acc))\n\n\n{'age': 0.13,\n 'bmi': 0.12,\n 'diastolic': 0.09,\n 'family': 0.12,\n 'glucose': 0.25,\n 'insulin': 0.14,\n 'pregnant': 0.07,\n 'triceps': 0.09}\n79.6% accuracy on test set.\n\n\n\n\n\nNow lets use the fitted random model to select the most important features from our input dataset X.\n\n\nCode\n# Create a mask for features importances above the threshold\nmask = rf.feature_importances_ > 0.15\n\n# Prints out the mask\nprint(mask)\n\n# Apply the mask to the feature dataset X\nreduced_X = X.loc[:, mask]\n\n# Prints out the selected column names\nprint(reduced_X.columns)\n\n\n[False  True False False False False False False]\nIndex(['glucose'], dtype='object')\n\n\n\n\n\nYou’ll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others.\n\n\nCode\n# Wrap the feature eliminator around the random forest model\nrfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n\n# Fit the model to the training data\nrfe.fit(X_train, y_train)\n\n# Create a mask using an attribute of rfe\nmask = rfe.support_\n\n# Apply the mask to the feature dataset X and print the result\nreduced_X = X.loc[:, mask]\nprint(reduced_X.columns)\n\n\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\nFitting estimator with 4 features.\nFitting estimator with 3 features.\nIndex(['glucose', 'bmi'], dtype='object')\n\n\n\n\nCode\n# Wrap the feature eliminator around the random forest model\nrfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n\n# Fit the model to the training data\nrfe.fit(X_train, y_train)\n\n# Create a mask using an attribute of rfe\nmask = rfe.support_\n\n# Apply the mask to the feature dataset X and print the result\nreduced_X = X.loc[:, mask]\nprint(reduced_X.columns)\nprint(\"\\nCompared to the quick and dirty single threshold method from the previous exercise one of the selected features is different.\")\n\n\nFitting estimator with 8 features.\nFitting estimator with 6 features.\nFitting estimator with 4 features.\nIndex(['glucose', 'bmi'], dtype='object')\n\nCompared to the quick and dirty single threshold method from the previous exercise one of the selected features is different.\n\n\n\n\n\n\n\nLoss function: Mean Squared Error \nAdding regularization \\[ \\text{MSE} + \\overbrace{\\alpha(\\vert \\beta_1 \\vert + \\vert \\beta_2 \\vert + \\vert \\beta_3 \\vert)}^{\\text{Regularization term}} \\]\n\nMSE tries to make model accurate\nRegularization term tries to make model simple\n\\(\\alpha\\), when it’s too low, the model might overfit. when it’s too high, the model might become too simple and inaccurate. One linear model that includes this type of regularization is called Lasso, for Least Absolute Shrinkage and Selection.\n\n\n\n\nYou’ll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the Lasso() regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge.\nYou’ll standardize the data first using the StandardScaler() that has been instantiated for you as scaler to make sure all coefficients face a comparable regularizing force trying to bring them down.\n\n\nCode\nansur_male = pd.read_csv('dataset/ANSUR_II_MALE.csv')\n\nansur_df = ansur_male\n# unused columns in the dataset\nunused = ['Gender', 'Branch', 'Component', 'BMI_class', 'Height_class', 'weight_kg', 'stature_m']\n\n# Drop the non-numeric columns from df\nansur_df.drop(unused, axis=1, inplace=True)\n\nX = ansur_df.drop('BMI', axis=1)\ny = ansur_df['BMI']\n\nscaler = StandardScaler()\n\n\n\n\nCode\nfrom sklearn.linear_model import Lasso\n\n# Set the test size to 30% to get a 70-30% train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Fit the scaler on the training features and transform these in one go\nX_train_std = scaler.fit_transform(X_train)\n\n# Create the Lasso model\nla = Lasso()\n\n# Fit it to the standardized training data\nla.fit(X_train_std, y_train)\n\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\n\n\nNow that you’ve trained the Lasso model, you’ll score its predictive capacity (\\(R^2\\)) on the test set and count how many features are ignored because their coefficient is reduced to zero.\n\n\nCode\n# Transform the test set with the pre-fitted scaler\nX_test_std = scaler.transform(X_test)\n\n# Calculate the coefficient of determination (R squared) on X_test_std\nr_squared = la.score(X_test_std, y_test)\nprint(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n\n# Create a list that has True values when coefficients equal 0\nzero_coef = la.coef_ == 0\n\n# Calculate how many features have a zero coefficient\nn_ignored = sum(zero_coef)\nprint(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\nprint(\"\\nWe can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The $R^2$ could be higher though.\")\n\n\nThe model can predict 84.7% of the variance in the test set.\nThe model has ignored 82 out of 91 features.\n\nWe can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The $R^2$ could be higher though.\n\n\n\n\n\nYour current Lasso model has an \\(R^2\\) score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power.\nLet’s improve the balance between predictive power and model simplicity by tweaking the alpha parameter.\n\n\nCode\nalpha_list = [1, 0.5, 0.1, 0.01]\nmax_r = 0\nmax_alpha = 0\nfor alpha in alpha_list:\n    # Find the highest alpha value with R-squared above 98%\n    la = Lasso(alpha=alpha, random_state=0)\n\n    # Fits the model and calculates performance stats\n    la.fit(X_train_std, y_train)\n    r_squared = la.score(X_test_std, y_test)\n    n_ignored_features = sum(la.coef_ == 0)\n\n    # Print peformance stats\n    print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n    print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))\n    if r_squared > 0.98:\n        max_r = r_squared\n        max_alpha = alpha\n        break\nprint(\"Max R-squared: {}, alpha: {}\".format(max_r, max_alpha))\n\n\nThe model can predict 84.7% of the variance in the test set.\n82 out of 91 features were ignored.\nThe model can predict 93.8% of the variance in the test set.\n79 out of 91 features were ignored.\nThe model can predict 98.3% of the variance in the test set.\n64 out of 91 features were ignored.\nMax R-squared: 0.9828190248586458, alpha: 0.1\n\n\n\n\n\n\n\n\nYou’ll be predicting biceps circumference on a subsample of the male ANSUR dataset using the LassoCV() regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation.\n\n\nCode\nX = ansur_df[['acromialheight', 'axillaheight', 'bideltoidbreadth', 'buttockcircumference', 'buttockkneelength', 'buttockpopliteallength', 'cervicaleheight', 'chestcircumference', 'chestheight',\n       'earprotrusion', 'footbreadthhorizontal', 'forearmcircumferenceflexed', 'handlength', 'headbreadth', 'heelbreadth', 'hipbreadth', 'iliocristaleheight', 'interscyeii',\n       'lateralfemoralepicondyleheight', 'lateralmalleolusheight', 'neckcircumferencebase', 'radialestylionlength', 'shouldercircumference', 'shoulderelbowlength', 'sleeveoutseam',\n       'thighcircumference', 'thighclearance', 'verticaltrunkcircumferenceusa', 'waistcircumference', 'waistdepth', 'wristheight', 'BMI']]\ny = ansur_df['bicepscircumferenceflexed']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n\n\nCode\nfrom sklearn.linear_model import LassoCV\n\n# Create and fit the LassoCV model on the training set\nlcv = LassoCV()\nlcv.fit(X_train, y_train)\nprint('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n\n# Calculate R squared on the test set\nr_squared = lcv.score(X_test, y_test)\nprint('The model explains {0:.1%} of the test set variance'.format(r_squared))\n\n# Create a mask for coefficients not equal to zero\nlcv_mask = lcv.coef_ != 0\nprint('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n\n\nOptimal alpha = 0.035\nThe model explains 85.6% of the test set variance\n24 features out of 32 selected\n\n\n\n\n\nThe LassoCV() model selected 24 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let’s use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE).\n\n\nCode\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\nrfe_gb = RFE(estimator=GradientBoostingRegressor(),\n            n_features_to_select=10, step=3, verbose=1)\nrfe_gb.fit(X_train, y_train)\n\n# Calculate the R squared on the test set\nr_squared = rfe_gb.score(X_test, y_test)\nprint('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n\n# Assign the support array to gb_mask\ngb_mask = rfe_gb.support_\n\n\nFitting estimator with 32 features.\nFitting estimator with 29 features.\nFitting estimator with 26 features.\nFitting estimator with 23 features.\nFitting estimator with 20 features.\nFitting estimator with 17 features.\nFitting estimator with 14 features.\nFitting estimator with 11 features.\nThe model can explain 83.3% of the variance in the test set\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\nrfe_rf = RFE(estimator=RandomForestRegressor(),\n            n_features_to_select=10, step=3, verbose=1)\nrfe_rf.fit(X_train, y_train)\n\n# Calculate the R squared on the test set\nr_squared = rfe_rf.score(X_test, y_test)\nprint('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n\n# Assign the support array to rf_mask\nrf_mask = rfe_rf.support_\nprint(\"\\nInluding the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important.\")\n\n\nFitting estimator with 32 features.\nFitting estimator with 29 features.\nFitting estimator with 26 features.\nFitting estimator with 23 features.\nFitting estimator with 20 features.\nFitting estimator with 17 features.\nFitting estimator with 14 features.\nFitting estimator with 11 features.\nThe model can explain 82.6% of the variance in the test set\n\nInluding the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important.\n\n\n\n\n\nWe’ll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We’ll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset.\n\n\nCode\n# Sum the votes of the three models\nvotes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\nprint(votes)\n\n# Create a mask for features selected by all 3 models\nmeta_mask = votes == 3\nprint(meta_mask)\n\n# Apply the dimensionality reduction on X\nX_reduced = X.loc[:, meta_mask]\nprint(X_reduced.columns)\n\n\n[0 1 3 3 0 1 1 3 1 1 1 3 1 1 1 0 0 1 0 1 1 1 3 3 0 3 2 1 1 3 0 3]\n[False False  True  True False False False  True False False False  True\n False False False False False False False False False False  True  True\n False  True False False False  True False  True]\nIndex(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference',\n       'forearmcircumferenceflexed', 'shouldercircumference',\n       'shoulderelbowlength', 'thighcircumference', 'waistdepth', 'BMI'],\n      dtype='object')\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\n\n# Plug the reduced data into a linear regression pipeline\nX_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\nlm.fit(scaler.fit_transform(X_train), y_train)\nr_squared = lm.score(scaler.transform(X_test), y_test)\nprint('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))\nprint(\"\\nUsing the votes from 3 models you were able to select just 7 features that allowed a simple linear model to get a high accuracy!\")\n\n\nThe model can explain 84.4% of the variance in the test set using 9 features.\n\nUsing the votes from 3 models you were able to select just 7 features that allowed a simple linear model to get a high accuracy!"
  },
  {
    "objectID": "posts/Fine Tuning Model/Fine Tuning Model.html",
    "href": "posts/Fine Tuning Model/Fine Tuning Model.html",
    "title": "Fine Tunning Model",
    "section": "",
    "text": "Fine Tunning Model\nAfter training models, you’ll learn how to assess them in this chapter. You’ll learn how to analyze classification model performance using scikit-learn by using several metrics and a visualization technique. Using hyperparameter tuning, you will also be able to optimize classification and regression models.\nThis Fine Tunning Model is part of Datacamp course: Supervised Learning with scikit-learn\nThis is my learning experience of data science through DataCamp\n\n\nOptimizing your model\nAfter training your model, we must evaluate its performance. In this section, we will explore some of the other metrics available in scikit-learn for assessing our model’s performance. Using hyperparameter tuning, you can optimize your classification and regression models.\n\nClassification metrics\nChapter 1 evaluated the accuracy of your k-NN classifier. As Andy discussed, accuracy is not always an informative metric. By computing a confusion matrix and generating a classification report, you will evaluate the performance of binary classifiers.\nThe classification report consisted of three rows and an additional support column, as shown in the video. In the video example, the support was the number of Republicans or Democrats in the test set used to compute the classification report. These columns gave the precision, recall, and f1-score metrics for that particular class.\nThis tutorial uses the PIMA Indians dataset available at the UCI Machine Learning Repository. Based on factors such as BMI, age, and number of pregnancies, we can predict whether or not a given female patient will develop diabetes. As a result, it is a binary classification problem. Diabetes is not present in a patient with a target value of 0, whereas diabetes is present in a patient with a target value of 1. To deal with missing values, the dataset has been preprocessed in earlier excercises.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n\n\n\nCode\ndf=pd.read_csv('diabetes.csv')\n\ndf.insulin.replace(0, np.nan, inplace=True)\ndf.triceps.replace(0, np.nan, inplace=True)\ndf.bmi.replace(0, np.nan, inplace=True)\n\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.fillna(x.mean()))\ny = df['diabetes']\nX = df.drop('diabetes', axis=1)\n\n# Create training and test set\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n\n# Instantiate a k-NN classifier: knn\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train,y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n[[176  30]\n [ 52  50]]\n              precision    recall  f1-score   support\n\n           0       0.77      0.85      0.81       206\n           1       0.62      0.49      0.55       102\n\n    accuracy                           0.73       308\n   macro avg       0.70      0.67      0.68       308\nweighted avg       0.72      0.73      0.72       308\n\n\n\nBy analyzing the confusion matrix and classification report, you can get a much better understanding of your classifier’s performance.\nAn ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate\n\nBuilding a logistic regression model\nTime to build your first logistic regression model! As Hugo showed in the video, scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as ‘estimators’. You’ll see this now for yourself as you train a logistic regression model on exactly the same data as in the previous exercise. Will it outperform k-NN? There’s only one way to find out!\n\n\nCode\n# Import the necessary modules\nfrom sklearn.linear_model import LogisticRegression\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n# Create the classifier: logreg\nlogreg = LogisticRegression(solver=\"liblinear\")\n\n# Fit the classifier to the training data\nlogreg.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n[[176  30]\n [ 35  67]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.85      0.84       206\n           1       0.69      0.66      0.67       102\n\n    accuracy                           0.79       308\n   macro avg       0.76      0.76      0.76       308\nweighted avg       0.79      0.79      0.79       308\n\n\n\nROC curve plotting\nThe previous exercise was a great success - you now have a new classifier in your toolbox!\nModel performance can be evaluated quantitatively using classification reports and confusion matrices, while visually using ROC curves. Most scikit-learn classifiers have a .predict_proba() method that returns the probability of a given sample being in a particular class, as Hugo demonstrated in the video. After building a logistic regression model, you will plot an ROC curve to evaluate its performance. As a result, you will become familiar with the .predict_proba() method.\nYou’ll continue working with the PIMA Indians diabetes dataset here\n\n\nCode\n# Import necessary modules\nfrom sklearn.metrics import roc_curve\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\n\n\n\n\nCurve of precision-recall\nWe may have noticed that your ROC curve’s y-axis (True positive rate) is also known as recall. ROC curves are not the only way to evaluate model performance visually. Precision-recall curves are generated by plotting precision and recall at different thresholds. Recall that precision and recall are defined as follows:\n\nThe precision-recall curve for the diabetes dataset can be seen below. IPython Shell displays the classification report and confusion matrix.\nTake a look at the precision-recall curve and then consider the following statements. Pick the statement that is not true. If the individual has diabetes, the class is positive (1).\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n#\n\ndf = pd.read_csv('diabetes.csv')\n\ndf.insulin.replace(0, np.nan, inplace=True)\ndf.triceps.replace(0, np.nan, inplace=True)\ndf.bmi.replace(0, np.nan, inplace=True)\n\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.fillna(x.mean()))\n\n#\ny = df['diabetes']\nX = df.drop('diabetes', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n#\nclf = LogisticRegression(solver=\"liblinear\")#add solver by Jinny\nclf.fit(X_train, y_train)\n\n#\nfrom sklearn.metrics import precision_recall_curve\n\n#\ny_pred_prob = clf.predict_proba(X_test)[:,1]\n\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n#\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n#\nplt.figure(figsize=(6, 6), dpi=None)\nplt.plot(recall, precision)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='best')\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n              precision    recall  f1-score   support\n\n           0       0.83      0.85      0.84       206\n           1       0.69      0.66      0.67       102\n\n    accuracy                           0.79       308\n   macro avg       0.76      0.76      0.76       308\nweighted avg       0.79      0.79      0.79       308\n\n[[176  30]\n [ 35  67]]\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#\n\n\ndf = pd.read_csv('diabetes.csv')\n\ndf.insulin.replace(0, np.nan, inplace=True)\ndf.triceps.replace(0, np.nan, inplace=True)\ndf.bmi.replace(0, np.nan, inplace=True)\n\ndf.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x.fillna(x.mean()))\ny = df['diabetes']\nX = df.drop('diabetes', axis=1)\n# Import necessary modules\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n# Instantiate a k-NN classifier: knn\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n[[176  30]\n [ 52  50]]\n              precision    recall  f1-score   support\n\n           0       0.77      0.85      0.81       206\n           1       0.62      0.49      0.55       102\n\n    accuracy                           0.73       308\n   macro avg       0.70      0.67      0.68       308\nweighted avg       0.72      0.73      0.72       308\n\n\n\nCompute the AUC\nSuppose we have a binary classifier that makes guesses at random. It would be correct approximately 50% of the time, and the ROC curve would be a diagonal line where the True Positive Rate and False Positive Rate are always equal. This ROC curve has an Area under it of 0.5. Hugo discussed the AUC in the video as an informative metric for evaluating models. The model is better than random guessing if the AUC is greater than 0.5. Always a good sign!\nWe will calculate AUC scores on the diabetes dataset using the roc_auc_score() function from sklearn.metrics.\n\n\nCode\n# Import necessary modules\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc = cross_val_score(logreg,X,y,cv=5,scoring='roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n\n\nAUC: 0.8255282695602512\nAUC scores computed using 5-fold cross-validation: [0.80203704 0.80648148 0.81481481 0.86264151 0.8554717 ]\n\n\nHyperparameter tuning with GridSearchCV\nUsing GridSearchCV on the voting dataset, we tune the n_neighbors parameter of KNeighborsClassifier(). Now we will practice this thoroughly using logistic regression on the diabetes dataset instead!\nWe saw earlier that logistic regression also has a regularization parameter: C. This parameter controls the inverse of regularization strength. A large number can result in an overfit model, while a small number can result in an underfit model.\nYou have been provided with the hyperparameter space for C. We will use GridSearchCV and logistic regression to find the optimal C. X represents the feature array and Y represents the target variable array.\nThis is why we haven’t separated the data into training and test sets. I agree with your observation! We will focus on setting up the hyperparameter grid and performing grid-search cross-validation. In practice, we will indeed want to hold out a portion of your data for evaluation purposes, and you will learn all about this in the next video!\n\n\nCode\n# Import necessary modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Setup the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\n# Instantiate a logistic regression classifier: logreg\nlogreg = LogisticRegression(solver=\"liblinear\")\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X,y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Best score is {}\".format(logreg_cv.best_score_))\n\n\nTuned Logistic Regression Parameters: {'C': 3.727593720314938}\nBest score is 0.7708768355827178\n\n\nHyperparameter tuning with RandomizedSearchCV\nGridSearchCV is computationally expensive, especially when dealing with multiple hyperparameters and large hyperparameter spaces. To solve this problem, RandomizedSearchCV can be used, in which not all hyperparameter values are tested. A fixed number of hyperparameter settings is instead sampled from specified probability distributions. In this exercise, you will practice using RandomizedSearchCV.\nYou will also be introduced to a new model: the Decision Tree. You don’t need to worry about the specifics of how this model works. In scikit-learn, decision trees also have .fit() and .predict() methods, just like k-NN, linear regression, and logistic regression. In RandomizedSearchCV, decision trees are ideal because they have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf.\nThe diabetes dataset has been preloaded with the feature array X and target variable array Y. You have been given the hyperparameter settings. To determine the optimal hyperparameters, you will use RandomizedSearchCV\n\n\nCode\n# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X,y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\n\n\nTuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 5, 'min_samples_leaf': 5}\nBest score is 0.7422629657923776\n\n\nHold-out set in practice I: Classification\nWe will now practice evaluating a model with tuned hyperparameters on a hold-out set. X and Y have been preloaded from the diabetes dataset as feature arrays and target variable arrays, respectively.\nIn addition to CC, logistic regression also has a ‘penalty’ hyperparameter that specifies whether ‘l1’ or ‘l2’ regularization should be used. This exercise requires you to create a hold-out set, tune the ‘C’ and ‘penalty’ hyperparameters of a logistic regression classifier using GridSearchCV.\n\n\nCode\n# Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Create the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the logistic regression classifier: logreg\nlogreg = LogisticRegression(solver='liblinear')\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train,y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n\n\nTuned Logistic Regression Parameter: {'C': 0.4393970560760795, 'penalty': 'l1'}\nTuned Logistic Regression Accuracy: 0.7673913043478262\n\n\nHold-out set in practice II: Regression\nRemember lasso and ridge regression from the previous chapter? Lasso used the penalty to regularize, while ridge used the penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties:\n\nIn scikit-learn, this term is represented by the ‘l1_ratio’parameter: An ’l1_ratio’ of 1 corresponds to an L1L1penalty, and anything lower is a combination of L1 and L2.\nIn this exercise, you will GridSearchCV to tune the ‘l1_ratio’ of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model’s performance.\n\n\nCode\ndf = pd.read_csv('gapminder-clean.csv')\ny = df['life'].values\nX = df.drop('life', axis=1).values\n\n\n\n\nImport necessary modules\nfrom sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV from sklearn.model_selection import train_test_split\n\n\nCreate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n\n\nCreate the hyperparameter grid\nl1_space = np.linspace(0, 1, 30) param_grid = {‘l1_ratio’: l1_space}\n\n\nInstantiate the ElasticNet regressor: elastic_net\nelastic_net = ElasticNet()\n\n\nSetup the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n\n\nFit it to the training data\ngm_cv.fit(X_train,y_train)\n\n\nPredict on the test set and compute metrics\ny_pred = gm_cv.predict(X_test) r2 = gm_cv.score(X_test, y_test) mse = mean_squared_error(y_test, y_pred) print(“Tuned ElasticNet l1 ratio: {}”.format(gm_cv.best_params_)) print(“Tuned ElasticNet R squared: {}”.format(r2)) print(“Tuned ElasticNet MSE: {}”.format(mse))\nNow that we have basic understanding how to fine-tune your models, it’s time to learn about preprocessing techniques and how to piece together all the different stages of the machine learning process into a pipeline!"
  },
  {
    "objectID": "posts/Fine Tuning XGBoost Model/Fine tunning your XGBoost model.html",
    "href": "posts/Fine Tuning XGBoost Model/Fine tunning your XGBoost model.html",
    "title": "Fine-tuning your XGBoost model",
    "section": "",
    "text": "You will learn how to adjust XGBoost’s parameters and how to tune them efficiently so that you can supercharge the performance of your models\nThis Fine-tuning your XGBoost model  is part of Datacamp course: Extreme Gradient Boosting with XGBoost\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nplt.rcParams['figure.figsize'] = (10, 10)\n\n\n\n\nNow that you’ve seen the effect that tuning has on the overall performance of your XGBoost model, let’s turn the question on its head and see if you can figure out when tuning your model might not be the best idea.\n\n\nLet’s start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You’ll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\nHere, you’ll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y.\n\n\nCode\ndf = pd.read_csv('dataset/ames_housing_trimmed_processed.csv')\nX, y = df.iloc[:, :-1], df.iloc[:, -1]\n\n\n\n\nCode\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Creata the parameter dictionary for each tree: params\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n\n# Create list of number of boosting rounds\nnum_rounds = [5, 10, 15]\n\n# Empty list to store final round rmse per XGBoost model\nfinal_rmse_per_round = []\n\n# Interate over num_rounds and build one model per num_boost_round parameter\nfor curr_num_rounds in num_rounds:\n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n                        num_boost_round=curr_num_rounds, metrics='rmse',\n                        as_pandas=True, seed=123)\n\n    # Append final round RMSE\n    final_rmse_per_round.append(cv_results['test-rmse-mean'].tail().values[-1])\n\n# Print the result DataFrame\nnum_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\nprint(pd.DataFrame(num_rounds_rmses, columns=['num_boosting_rounds', 'rmse']))\nprint(\"\\nAs you can see, increasing the number of boosting rounds decreases the RMSE.\")\n\n\n   num_boosting_rounds          rmse\n0                    5  50903.299752\n1                   10  34774.194090\n2                   15  32895.099185\n\nAs you can see, increasing the number of boosting rounds decreases the RMSE.\n\n\n\n\n\nNow, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\nEarly stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur.\n\n\nCode\n# Create your housing DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary for each tree: params\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n\n# Perform cross-validation with early-stopping: cv_results\ncv_results = xgb.cv(dtrain=housing_dmatrix, nfold=3, params=params, metrics=\"rmse\",\n                    early_stopping_rounds=10, num_boost_round=50, as_pandas=True, seed=123)\n\n# Print cv_results\nprint(cv_results)\n\n\n    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n0     141871.635216      403.633062   142640.653507     705.559723\n1     103057.033818       73.768079   104907.664683     111.117033\n2      75975.967655      253.727043    79262.056654     563.766693\n3      57420.530642      521.658273    61620.137859    1087.693428\n4      44552.956483      544.170426    50437.560906    1846.446643\n5      35763.948865      681.796675    43035.659539    2034.471115\n6      29861.464164      769.571418    38600.880800    2169.796804\n7      25994.675122      756.520639    36071.817710    2109.795408\n8      23306.836299      759.237848    34383.186387    1934.547433\n9      21459.770256      745.624640    33509.140338    1887.375358\n10     20148.721060      749.612186    32916.806725    1850.893437\n11     19215.382607      641.387200    32197.833474    1734.456654\n12     18627.388962      716.256240    31770.852340    1802.154296\n13     17960.695080      557.043324    31482.782172    1779.124406\n14     17559.736640      631.413137    31389.990252    1892.320326\n15     17205.713357      590.171774    31302.883291    1955.165882\n16     16876.571801      703.631953    31234.058914    1880.706205\n17     16597.662170      703.677363    31318.347820    1828.860754\n18     16330.460661      607.274258    31323.634893    1775.909992\n19     16005.972387      520.470815    31204.135450    1739.076237\n20     15814.300847      518.604822    31089.863868    1756.022175\n21     15493.405856      505.616461    31047.997697    1624.673447\n22     15270.734205      502.018639    31056.916210    1668.043691\n23     15086.381896      503.913078    31024.984403    1548.985086\n24     14917.608289      486.206137    30983.685376    1663.131135\n25     14709.589477      449.668262    30989.476981    1686.667218\n26     14457.286251      376.787759    30952.113767    1613.172390\n27     14185.567149      383.102597    31066.901381    1648.534545\n28     13934.066721      473.465580    31095.641882    1709.225578\n29     13749.644941      473.670743    31103.886799    1778.879849\n30     13549.836644      454.898742    30976.084872    1744.514518\n31     13413.484678      399.603422    30938.469354    1746.053330\n32     13275.915700      415.408595    30931.000055    1772.469405\n33     13085.878211      493.792795    30929.056846    1765.541040\n34     12947.181279      517.790033    30890.629160    1786.510472\n35     12846.027264      547.732747    30884.493051    1769.728787\n36     12702.378727      505.523140    30833.542124    1691.002007\n37     12532.244170      508.298300    30856.688154    1771.445485\n38     12384.055037      536.224929    30818.016568    1782.785175\n39     12198.443769      545.165604    30839.393263    1847.326671\n40     12054.583621      508.841802    30776.965294    1912.780332\n41     11897.036784      477.177932    30794.702627    1919.675130\n42     11756.221708      502.992363    30780.956160    1906.820178\n43     11618.846752      519.837483    30783.754746    1951.260120\n44     11484.080227      578.428500    30776.731276    1953.447810\n45     11356.552654      565.368946    30758.543732    1947.454939\n46     11193.557745      552.298986    30729.971937    1985.699239\n47     11071.315547      604.090125    30732.663173    1966.997252\n48     10950.778492      574.862853    30712.241251    1957.750615\n49     10824.865446      576.665678    30720.853939    1950.511037\n\n\n\n\n\n\n\nCommon tree tunable parameters\n\nlearning rate: learning rate/eta\ngamma: min loss reduction to create new tree split\nlambda: L2 regularization on leaf weights\nalpha: L1 regularization on leaf weights\nmax_depth: max depth per tree\nsubsample: % samples used per tree\ncolsample_bytree: % features used per tree\n\nLinear tunable parameters\n\nlambda: L2 reg on weights\nalpha: L1 reg on weights\nlambda_bias: L2 reg term on bias\n\nYou can also tune the number of estimators used for both base model types!\n\n\n\nIt’s time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You’ll begin by tuning the \"eta\", also known as the learning rate.\nThe learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization.\n\n\nCode\n# Create your housing DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary for each tree (boosting round)\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n\n# Create list of eta values and empty list to store final round rmse per xgboost model\neta_vals = [0.001, 0.01, 0.1]\nbest_rmse = []\n\n# Systematicallyvary the eta\nfor curr_val in eta_vals:\n    params['eta'] = curr_val\n\n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n                        early_stopping_rounds=5, num_boost_round=10, metrics='rmse', seed=123,\n                       as_pandas=True)\n\n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results['test-rmse-mean'].tail().values[-1])\n\n# Print the result DataFrame\nprint(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=['eta', 'best_rmse']))\n\n\n     eta      best_rmse\n0  0.001  195736.402543\n1  0.010  179932.183986\n2  0.100   79759.411808\n\n\n\n\n\nIn this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees.\n\n\nCode\n# Create your housing DMatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary\nparams = {\"objective\":\"reg:squarederror\"}\n\n# Create list of max_depth values\nmax_depths = [2, 5, 10, 20]\nbest_rmse = []\n\nfor curr_val in max_depths:\n    params['max_depth'] = curr_val\n\n    # Perform cross-validation\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n                       early_stopping_rounds=5, num_boost_round=10, metrics='rmse', seed=123,\n                        as_pandas=True)\n\n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results['test-rmse-mean'].tail().values[-1])\n\n# Print the result DataFrame\nprint(pd.DataFrame(list(zip(max_depths, best_rmse)), columns=['max_depth', 'best_rmse']))\n\n\n   max_depth     best_rmse\n0          2  37957.469464\n1          5  35596.599504\n2         10  36065.547345\n3         20  36739.576068\n\n\n\n\n\nNow, it’s time to tune \"colsample_bytree\". You’ve already seen this if you’ve ever worked with scikit-learn’s RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1.\n\n\nCode\n# Create your housing DMatrix\nhousing_dmatrix = xgb.DMatrix(data=X,label=y)\n\n# Create the parameter dictionary\nparams={\"objective\":\"reg:squarederror\", \"max_depth\":3}\n\n# Create list of hyperparameter values: colsample_bytree_vals\ncolsample_bytree_vals = [0.1, 0.5, 0.8, 1]\nbest_rmse = []\n\n# Systematically vary the hyperparameter value\nfor curr_val in colsample_bytree_vals:\n    params['colsample_bytree'] = curr_val\n\n    # Perform cross-validation\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n                 num_boost_round=10, early_stopping_rounds=5,\n                 metrics=\"rmse\", as_pandas=True, seed=123)\n\n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)),\n                   columns=[\"colsample_bytree\",\"best_rmse\"]))\nprint(\"\\nThere are several other individual parameters that you can tune, such as `'subsample'`, which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!\")\n\n\n   colsample_bytree     best_rmse\n0               0.1  40918.116895\n1               0.5  35813.904168\n2               0.8  35995.678734\n3               1.0  35836.044343\n\nThere are several other individual parameters that you can tune, such as `'subsample'`, which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!\n\n\n\n\n\n\n\nGrid search: review\n\nSearch exhaustively over a given set of hyperparameters, once per set of hyperparameters\nNumber of models = number of distinct values per hyperparameter multiplied across each hyperparameter\nPick final model hyperparameter values that give best cross-validated evaluation metric value\n\nRandom search: review\n\nCreate a (possibly infinte) range of hyperparameter values per hyperparameter that you would like to search over\nSet the number of iterations you would like for the random search to continue\nDuring each iteration, randomly draw a value in the range of specified values for each hyperparameter searched over and train/evaluate a model with those hyperparameters\nAfter you’ve reached the maximum number of iterations, select the hyperparameter configuration with the best evaluated score\n\n\n\n\nNow that you’ve learned how to tune parameters individually with XGBoost, let’s take your parameter tuning to the next level by using scikit-learn’s GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let’s get to work, starting with GridSearchCV!\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid: gbm_param_grid\ngbm_param_grid = {\n    'colsample_bytree': [0.3, 0.7],\n    'n_estimators': [50],\n    'max_depth': [2, 5]\n}\n\n# Instantiate the regressor: gbm\ngbm = xgb.XGBRegressor()\n\n# Perform grid search: grid_mse\ngrid_mse = GridSearchCV(param_grid=gbm_param_grid, estimator=gbm,\n                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n\n# Fit grid_mse to the data\ngrid_mse.fit(X, y)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n\n\nFitting 4 folds for each of 4 candidates, totalling 16 fits\nBest parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\nLowest RMSE found:  28986.18703093561\n\n\n\n\n\nOften, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter.\n\n\nCode\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Create the parameter grid: gbm_param_grid\ngbm_param_grid = {\n    'n_estimators': [25],\n    'max_depth': range(2, 12)\n}\n\n# Instantiate the regressor: gbm\ngbm = xgb.XGBRegressor(n_estimators=10)\n\n# Perform random search: randomized_mse\nrandomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, estimator=gbm,\n                                    scoring='neg_mean_squared_error', n_iter=5, cv=4,\n                                   verbose=1)\n\n# Fit randomized_mse to the data\nrandomized_mse.fit(X, y)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", randomized_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))\n\n\nFitting 4 folds for each of 5 candidates, totalling 20 fits\nBest parameters found:  {'n_estimators': 25, 'max_depth': 4}\nLowest RMSE found:  29998.4522530019\n\n\n\n\n\n\n\nlimitations\n\nGrid Search\n\nNumber of models you must build with every additionary new parameter grows very quickly\n\nRandom Search\n\nParameter space to explore can be massive\nRandomly jumping throughtout the space looking for a “best” results becomes a waiting game"
  },
  {
    "objectID": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html",
    "href": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html",
    "title": "Fuzzy Regression Discontinuity Design: An Introduction",
    "section": "",
    "text": "This article includes:"
  },
  {
    "objectID": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#what-is-regression-discontinuity-design",
    "href": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#what-is-regression-discontinuity-design",
    "title": "Fuzzy Regression Discontinuity Design: An Introduction",
    "section": "What is regression discontinuity design:",
    "text": "What is regression discontinuity design:\nAs part of causal inference, statistical techniques are utilized to identify causal relationships between variables. Regression analysis is an important method for causal inference. However, sometimes the relationship between the independent and dependent variables is not linear, and there is a discontinuity in the relationship. This is where regression discontinuity occurs."
  },
  {
    "objectID": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#a-brief-introduction-about-fuzzy-discontinuity-design",
    "href": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#a-brief-introduction-about-fuzzy-discontinuity-design",
    "title": "Fuzzy Regression Discontinuity Design: An Introduction",
    "section": "A brief introduction about fuzzy discontinuity design:",
    "text": "A brief introduction about fuzzy discontinuity design:\nA fuzzy regression discontinuity design (FRDD) is a research design that estimates causal effects based upon regression discontinuity. FRDD is used when there is a discontinuity in the outcome variable at a specific threshold, but the assignment of individuals to treatment and control groups is not entirely determined by the threshold. When the treatment or intervention is assigned based upon a continuous score that is correlated with the outcome variable but not perfectly, this can occur."
  },
  {
    "objectID": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#elaboration-of-fuzzy-regression-discontinuities",
    "href": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#elaboration-of-fuzzy-regression-discontinuities",
    "title": "Fuzzy Regression Discontinuity Design: An Introduction",
    "section": "Elaboration of fuzzy regression discontinuities:",
    "text": "Elaboration of fuzzy regression discontinuities:\nAn important component of FRDD is estimating the causal effect of the treatment or intervention on the outcome variable based on variations in the assignment to a treatment or control group. Individuals are assigned to treatment or control groups based on their distance from the threshold, but some degree of noise or randomness is allowed.\nAccording to FRDD, the outcome variable undergoes a sharp discontinuity at the threshold, but the assignment to a treatment or control group is not perfectly determined by this discontinuity. Using a fuzzy score that reflects the degree of uncertainty associated with group assignment, it is possible to quantify the degree of noise in group assignment.\nLet’s illustrate the concept with an example. Suppose a school district is considering a policy change that would provide extra resources to schools with low test scores. To qualify for extra resources, a school must score 60 on the standardized test. However, the assignment of schools to the treatment or control group is not solely determined by the test score, but also by factors such as school size and location. The purpose of this study is to estimate the causal effect of receiving the extra resources on test scores.\nWe can use FRDD to estimate this effect. First, we collect data on test scores, school size, location, and other relevant variables for all schools within the district. Based on the distance each school has from the threshold of 60, we assign each school to the treatment or control group, but with some degree of randomness. The relationship between test scores and resources received can be estimated through regression modeling, adjusting for other relevant variables in the process."
  },
  {
    "objectID": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#fuzzy-regression-discontinuity-design-with-instrument-variable",
    "href": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#fuzzy-regression-discontinuity-design-with-instrument-variable",
    "title": "Fuzzy Regression Discontinuity Design: An Introduction",
    "section": "Fuzzy Regression Discontinuity Design with Instrument Variable:",
    "text": "Fuzzy Regression Discontinuity Design with Instrument Variable:\nAs a result of fuzzy RD designs, the Local Average Treatment Effect (LATE) is calculated for the units just above and just below the fuzzy threshold, indicating the average causal effect. An instrument variable must be correlated with the independent variable around the fuzzy threshold, but not with the error term in fuzzy RD designs. The instrument variable serves as a proxy for the independent variable and can be used to identify the causal effect by accounting for endogeneity.\nAssuming constant effects and linearity in the independent variable 𝑋𝑖: 𝑌𝑖0 = 𝛼 + 𝛽𝑋𝑖 + 𝜂𝑖 𝑌𝑖1 = 𝑌𝑖0 + 𝜏\nUsing the switching equation 𝑌𝑖 = 𝑌𝑖0 + 𝑌𝑖1 - 𝑌𝑖0 𝐷𝑖, we get:\n𝑌𝑖 = 𝛼 + 𝛽𝑋𝑖 + 𝜏𝐷𝑖 + 𝜂𝑖\nwhere 𝑌𝑖 is the outcome variable for unit 𝑖, 𝑋𝑖 is the independent variable for unit 𝑖, 𝐷𝑖 is the treatment assignment for unit 𝑖 (equal to 1 if 𝑋𝑖 > 𝑐 and 0 otherwise), 𝜏 is the LATE at the fuzzy threshold, and 𝜂𝑖 is the error term.\nFuzzy RD designs estimate the LATE at the fuzzy threshold using the first stage and second stage regression equations:\n𝑋𝑖 = 𝛼1 + 𝛽1𝑍𝑖 + 𝜃1𝐷𝑖 + 𝜖1𝑖\n𝐷𝑖 = 𝛼2 + 𝛽2𝑍𝑖 + 𝜃2𝑋𝑖 + 𝜖2𝑖\nwhere 𝑍𝑖 is the instrument variable for unit 𝑖, and 𝜖1𝑖 and 𝜖2𝑖 are the error terms. The first stage equation estimates the relationship between the instrument variable and the independent variable, and the second stage equation estimates the relationship between the treatment assignment and the outcome variable, controlling for the endogeneity."
  },
  {
    "objectID": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#python-code-example",
    "href": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#python-code-example",
    "title": "Fuzzy Regression Discontinuity Design: An Introduction",
    "section": "Python code example:",
    "text": "Python code example:\nHere’s an example Python code to illustrate the process:\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\ndata = pd.read_csv('/data.csv')\n\n# Define the fuzzy threshold\nc = 50\n\n# Define the instrument variable\ndata['instrument'] = data['distance']\n# Fit the first stage regression model\nmodel_fs = sm.OLS(data['test_scores'], sm.add_constant(data[['instrument', 'treatment']]))\nresults_fs = model_fs.fit()\n\n# Calculate the predicted values of the independent variable\ndata['predicted_x'] = results_fs.predict()\n\n# Fit the second stage regression model\nmodel_ss = sm.OLS(data['test_scores'], sm.add_constant(data[['predicted_x', 'treatment']]))\nresults_ss = model_ss.fit()\n\n# Calculate the LATE at the fuzzy threshold\ntau = results_ss.params['treatment']\n\n# Print the results\nprint(\"First stage regression results:\")\nprint(results_fs.summary())\nprint(\"Second stage regression results:\")\nprint(results_ss.summary())\nprint(\"LATE at the fuzzy threshold: {:.2f}\".format(tau))\n \nIn this code, we fit the first stage regression model, which relates the instrument variable and the treatment variable to the independent variable, using Ordinary Least Squares (OLS) estimator. We then calculate the predicted values of the independent variable using the fitted regression line. In the second stage, we fit a regression model with the predicted values of the independent variable and the treatment variable as independent variables, and the test scores as the dependent variable, using OLS estimator.\nWe can print the regression results to see the estimated coefficients for the independent variables, and the LATE at the fuzzy threshold. We can also calculate the standard errors and t-statistics to test for the statistical significance of the LATE.\nNow let’s use following Python code to generate a plot to discuss\n# Plot the results\nfig, ax = plt.subplots()\nsns.regplot(x=data['predicted_x'], y=data['test_scores'], scatter_kws={'alpha':0.3}, line_kws={'color':'red', 'linestyle':'--'}, ci=None, ax=ax)\nsns.regplot(x=data['predicted_x'], y=data['test_scores'], scatter_kws={'alpha':0.3}, line_kws={'color':'blue'}, ci=None, ax=ax)\nsns.scatterplot(x=data['predicted_x'], y=data['test_scores'], hue=data['treatment'], alpha=0.5, ax=ax)\nax.axvline(x=data['predicted_x'].quantile(0.5), color='red', linestyle='--')\nax.set_xlabel('Predicted test scores')\nax.set_ylabel('Test scores')\nplt.show()\n\nThis code uses Seaborn’s library to plot the relationship between the predicted values of the independent variable and test scores for both treatment and control groups, with the red dashed line representing the fitted regression line for the control group and the blue line representing the fitted regression line for the treatment group. Additionally, the plot shows the treatment assignment for each unit, as well as the vertical red dashed line representing the median value of the independent variable.\nIn the plot, we can clearly see that the test scores are discontinuous around the median of the predicted values of the independent variable, as well as a significant difference in test scores between the treatment and control groups. For the units that are just above and just below the median of the predicted values of the independent variable, the LATE at the fuzzy threshold can be interpreted as the causal effect of the treatment on the test scores."
  },
  {
    "objectID": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#sharp-vs-fuzzy-discontinuities",
    "href": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#sharp-vs-fuzzy-discontinuities",
    "title": "Fuzzy Regression Discontinuity Design: An Introduction",
    "section": "Sharp vs Fuzzy discontinuities:",
    "text": "Sharp vs Fuzzy discontinuities:"
  },
  {
    "objectID": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#conclusion",
    "href": "posts/Fuzzy Regression Discontinuity Designs/Fuzzy Regression Discontinuity Design.html#conclusion",
    "title": "Fuzzy Regression Discontinuity Design: An Introduction",
    "section": "Conclusion:",
    "text": "Conclusion:\nTo conclude, Fuzzy Regression Discontinuity Designs (FRDD) are an extremely useful tool when the relationship between the independent and dependent variables discontinues, but the assignment to treatment or control groups is not entirely determined by the threshold. Using a regression analysis, we can estimate the causal effect of the independent variable on the dependent variable by introducing some degree of randomness or noise into the assignment."
  },
  {
    "objectID": "posts/Intoducting to sampling/Introduction to Sampling.html",
    "href": "posts/Intoducting to sampling/Introduction to Sampling.html",
    "title": "Introduction to sampling",
    "section": "",
    "text": "Get a better understanding of what sampling is and why it is so powerful. Additionally, We will learn about the problems associated with convenience sampling and what the difference between true randomness and pseudo-randomness is.\nThis Introduction to sampling is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\n\nPopulation: It is complete dataset\nSample: It is subset of data you calculate on\n\nPopulation parameter: It is a calculation on population dataset Points vs. flavor: population pts_vs_flavor_pop = coffee_ratings[[“total_cup_points”, “flavor”]] np.mean(pts_vs_flavor_pop[‘total_cup_points’])\nPoint estimate: Or sample statistic is a calculation made on sample dataset Points vs. flavor: 10 row sample pts_vs_flavor_samp = pts_vs_flavor_pop.sample(n=10) cup_points_samp = coffee_ratings[‘total_cup_points’].sample(n=10) np.mean(cup_points_samp)\n\n\n\nThe purpose of this exercise is to explore Spotify song data. There are over 40,000 rows in this population dataset, each representing a song. These columns include the title of the song, the artists who performed it, the release year, and attributes of the song, such as its duration, tempo, and danceability. To begin, you should examine the durations.\nThe Spotify dataset will be sampled and the mean duration of the sample will be compared with the mean duration of the population.\n\n\nCode\nspotify_population=pd.read_feather(\"dataset/spotify_2000_2020.feather\")\nspotify_population.head()\n\n\n\n\n\n\n  \n    \n      \n      acousticness\n      artists\n      danceability\n      duration_ms\n      duration_minutes\n      energy\n      explicit\n      id\n      instrumentalness\n      key\n      liveness\n      loudness\n      mode\n      name\n      popularity\n      release_date\n      speechiness\n      tempo\n      valence\n      year\n    \n  \n  \n    \n      0\n      0.97200\n      ['David Bauer']\n      0.567\n      313293.0\n      5.221550\n      0.227\n      0.0\n      0w0D8H1ubRerCXHWYJkinO\n      0.601000\n      10.0\n      0.110\n      -13.441\n      1.0\n      Shout to the Lord\n      47.0\n      2000\n      0.0290\n      136.123\n      0.0396\n      2000.0\n    \n    \n      1\n      0.32100\n      ['Etta James']\n      0.821\n      360240.0\n      6.004000\n      0.418\n      0.0\n      4JVeqfE2tpi7Pv63LJZtPh\n      0.000372\n      9.0\n      0.222\n      -9.841\n      0.0\n      Miss You\n      51.0\n      2000-12-12\n      0.0407\n      117.382\n      0.8030\n      2000.0\n    \n    \n      2\n      0.00659\n      ['Quasimoto']\n      0.706\n      202507.0\n      3.375117\n      0.602\n      1.0\n      5pxtdhLAi0RTh1gNqhGMNA\n      0.000138\n      11.0\n      0.400\n      -8.306\n      0.0\n      Real Eyes\n      44.0\n      2000-06-13\n      0.3420\n      89.692\n      0.4790\n      2000.0\n    \n    \n      3\n      0.00390\n      ['Millencolin']\n      0.368\n      173360.0\n      2.889333\n      0.977\n      0.0\n      3jRsoe4Vkxa4BMYqGHX8L0\n      0.000000\n      11.0\n      0.350\n      -2.757\n      0.0\n      Penguins & Polarbears\n      52.0\n      2000-02-22\n      0.1270\n      165.889\n      0.5480\n      2000.0\n    \n    \n      4\n      0.12200\n      ['Steve Chou']\n      0.501\n      344200.0\n      5.736667\n      0.511\n      0.0\n      4mronxcllhfyhBRqyZi8kU\n      0.000000\n      7.0\n      0.279\n      -9.836\n      0.0\n      黃昏\n      53.0\n      2000-12-25\n      0.0291\n      78.045\n      0.1130\n      2000.0\n    \n  \n\n\n\n\n\n\nCode\n# Sample 1000 rows from spotify_population\nspotify_sample = spotify_population.sample(n=1000)\n\n# Print the sample\nprint(spotify_sample)\n\n\n       acousticness                                            artists  \\\n7874        0.66400                                    ['The Walters']   \n2664        0.01020                    ['Beastie Boys', 'Fatboy Slim']   \n1683        0.00241                                          ['batta']   \n14491       0.11400            ['AJ Mitchell', 'Ava Max', 'Sam Feldt']   \n34495       0.96800                                         ['Yiruma']   \n...             ...                                                ...   \n25541       0.85400  ['Andrew Lloyd Webber', 'Patrick Wilson', 'Emm...   \n904         0.71900                                   ['Carl Carlton']   \n26932       0.02910                                    ['Cory Asbury']   \n30144       0.14900                              ['Twenty One Pilots']   \n12676       0.35000                                  ['Grupo Intenso']   \n\n       danceability  duration_ms  duration_minutes  energy  explicit  \\\n7874          0.747     151683.0          2.528050   0.422       0.0   \n2664          0.650     248507.0          4.141783   0.942       0.0   \n1683          0.389     145400.0          2.423333   0.988       0.0   \n14491         0.732     193548.0          3.225800   0.850       0.0   \n34495         0.287     218293.0          3.638217   0.292       0.0   \n...             ...          ...               ...     ...       ...   \n25541         0.194     294160.0          4.902667   0.119       0.0   \n904           0.546     153947.0          2.565783   0.828       0.0   \n26932         0.572     333386.0          5.556433   0.685       0.0   \n30144         0.550     277013.0          4.616883   0.625       0.0   \n12676         0.718     219960.0          3.666000   0.529       0.0   \n\n                           id  instrumentalness  key  liveness  loudness  \\\n7874   70QqoQ3krRFUHfEzit7vjT          0.002770  7.0    0.3920   -10.008   \n2664   2WGGxhsc2WtPNkhsXWVcYb          0.000000  1.0    0.1220    -6.609   \n1683   5V5akuBxKpIlTUPaueNpyy          0.000615  6.0    0.3460    -1.949   \n14491  2wenGTypSYHXl1sN1pNC7X          0.000002  1.0    0.0388    -5.999   \n34495  3xr8COed4nPPn6XWZ0iCGr          0.978000  9.0    0.0900   -19.285   \n...                       ...               ...  ...       ...       ...   \n25541  5klrh466oGToybceGHPGAX          0.000737  1.0    0.1090   -20.926   \n904    5i7rT8lbGzjj1n7TTXR5U8          0.030000  4.0    0.3720    -4.771   \n26932  0rH0mprtecH3grD9HFM5AD          0.000000  6.0    0.0963    -7.290   \n30144  4IN3imzEuTsiHO6tOwDQu5          0.000000  1.0    0.1610    -8.213   \n12676  0l7q7H1zYiJ9XHVqim2Uwc          0.000000  7.0    0.1510    -8.769   \n\n       mode                                           name  popularity  \\\n7874    1.0                                   Goodbye Baby        55.0   \n2664    1.0  Body Movin' - Fatboy Slim Remix/2005 Remaster        48.0   \n1683    0.0                                          chase        61.0   \n14491   1.0   Slow Dance (feat. Ava Max) - Sam Feldt Remix        72.0   \n34495   1.0                             River Flows in You        62.0   \n...     ...                                            ...         ...   \n25541   1.0                               All I Ask Of You        57.0   \n904     1.0                               Everlasting Love        48.0   \n26932   1.0                                  Reckless Love        71.0   \n30144   0.0                                       Trapdoor        56.0   \n12676   1.0                                         Y Volo        45.0   \n\n      release_date  speechiness    tempo  valence    year  \n7874    2015-10-20       0.0294  111.141   0.5950  2015.0  \n2664    2005-01-01       0.0754  101.786   0.7850  2005.0  \n1683    2016-07-27       0.1070  111.975   0.0996  2016.0  \n14491   2019-10-25       0.0444  124.024   0.3720  2019.0  \n34495   2011-12-09       0.0541  145.703   0.3460  2011.0  \n...            ...          ...      ...      ...     ...  \n25541   2004-12-10       0.0398   85.698   0.1400  2004.0  \n904     2009-01-01       0.0394  121.418   0.4010  2009.0  \n26932   2018-01-26       0.0356  110.698   0.2320  2018.0  \n30144   2009-12-29       0.0399  149.927   0.3170  2009.0  \n12676   2001-10-16       0.0325  142.069   0.9440  2001.0  \n\n[1000 rows x 20 columns]\n\n\n\n\nCode\n# Calculate the mean duration in mins from spotify_population\nmean_dur_pop = spotify_population['duration_minutes'].mean()\n\n# Calculate the mean duration in mins from spotify_sample\nmean_dur_samp = spotify_sample['duration_minutes'].mean()\n\n# Print the means\nprint(mean_dur_pop)\nprint(mean_dur_samp)\nprint(\"\\n Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.\")\n\n\n3.8521519140900073\n3.8048647333333334\n\n Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.\n\n\n\n\n\n\n\nCode\n# Subset the loudness column of spotify_population\nloudness_pop = spotify_population['loudness']\n\n# Sample 100 values of loudness_pop\nloudness_samp = loudness_pop.sample(n=100)\n\n# Print the sample\nprint(loudness_samp)\n\n\n28889   -6.697\n41542   -3.166\n24096   -8.327\n38021   -5.596\n29360   -3.779\n         ...  \n34958   -7.206\n41513   -7.560\n28413   -7.113\n41429   -6.073\n38857   -4.433\nName: loudness, Length: 100, dtype: float64\n\n\n\n\nCode\n# Calculate the mean of loudness_pop\nmean_loudness_pop = np.mean(loudness_pop)\n\n# Calculate the mean of loudness_samp\nmean_loudness_samp = np.mean(loudness_samp)\n\n# Print the means\nprint(mean_loudness_pop)\nprint(mean_loudness_samp)\nprint(\"\\n Again, notice that the calculated value (the mean) is close but not identical in each case\")\n\n\n-7.366856851353947\n-7.385839999999999\n\n Again, notice that the calculated value (the mean) is close but not identical in each case\n\n\n\n\n\nCollecting data by easiest method is convenience sampling\nSample bias: sample not true representation of population Selection bias\n\n\n\nIn your previous example, you saw that convenience sampling, which is the collection of data using the simplest method, can lead to samples that are not representative of the population. In other words, the findings of the sample cannot be generalized to the entire population. It is possible to determine whether or not a sample is representative of the population by examining the distributions of the population and the sample\n\n\nCode\n# Visualize the distribution of acousticness with a histogram\nwidth = 0.01\nspotify_population['acousticness'].hist(bins=np.arange(0,1.01,width))\nplt.show()\n\n\n\n\n\n\n\nCode\nspotify_mysterious_sample=spotify_population.sample(n=1107)\n# Update the histogram to use spotify_mysterious_sample\nspotify_mysterious_sample['acousticness'].hist(bins=np.arange(0, 1.01, 0.01))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Visualize the distribution of duration_minutes as a histogram\nspotify_population['duration_minutes'].hist(bins=np.arange(0,15.5,0.5))\nplt.show()\n\n\n\n\n\n\n\nCode\nspotify_mysterious_sample2=spotify_population.sample(n=50)\n# Update the histogram to use spotify_mysterious_sample2\nspotify_mysterious_sample2['duration_minutes'].hist(bins=np.arange(0, 15.5, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate random numbers from a Uniform(-3, 3)\nuniforms = np.random.uniform(low=-3, high=3, size=5000)\n\n# Print uniforms\nprint(uniforms)\n\n# Plot a histogram of uniform values, binwidth 0.25\nplt.hist(uniforms, bins=np.arange(-3,3.25,0.25))\nplt.show()\n\n\n[ 0.46978238 -1.66176314  1.31080161 ...  0.27384823  0.57683707\n  1.94834767]\n\n\n\n\n\n\n\nCode\n# Generate random numbers from a Normal(5, 2)\nnormals = np.random.normal(loc=5,scale=2,size=5000)\n\n# Print normals\nprint(normals)\n\n# Plot a histogram of normal values, binwidth 0.5\nplt.hist(normals,np.arange(-2,13.5,0.5))\nplt.show()\n\n\n[0.37088348 5.46510043 5.6804744  ... 5.06268094 7.1989964  4.26078515]"
  },
  {
    "objectID": "posts/Introduction to data preprocessing/Introduction to Data Preprocessing.html",
    "href": "posts/Introduction to data preprocessing/Introduction to Data Preprocessing.html",
    "title": "Introduction to Data Preprocessing",
    "section": "",
    "text": "In order to understand exactly what data preprocessing is all about, you will need to take the first steps in your preprocessing journey, which includes exploring data types and dealing with missing data.\nThis Introduction to Data Preprocessing is part of Datacamp course: Preprocessing for Machine Learning in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\n\n\n\n\n\nData Preprocessing\n\nBeyond cleaning and exploratory data analysis\nPrepping data for modeling\nModeling in python requires numerical input\n\n\n\n\nWe have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values.\n\n\nCode\nvolunteer = pd.read_csv('dataset/volunteer_opportunities.csv')\nvolunteer.head()\n\n\n\n\n\n\n  \n    \n      \n      opportunity_id\n      content_id\n      vol_requests\n      event_time\n      title\n      hits\n      summary\n      is_priority\n      category_id\n      category_desc\n      ...\n      end_date_date\n      status\n      Latitude\n      Longitude\n      Community Board\n      Community Council\n      Census Tract\n      BIN\n      BBL\n      NTA\n    \n  \n  \n    \n      0\n      4996\n      37004\n      50\n      0\n      Volunteers Needed For Rise Up & Stay Put! Home...\n      737\n      Building on successful events last summer and ...\n      NaN\n      NaN\n      NaN\n      ...\n      July 30 2011\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      5008\n      37036\n      2\n      0\n      Web designer\n      22\n      Build a website for an Afghan business\n      NaN\n      1.0\n      Strengthening Communities\n      ...\n      February 01 2011\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      5016\n      37143\n      20\n      0\n      Urban Adventures - Ice Skating at Lasker Rink\n      62\n      Please join us and the students from Mott Hall...\n      NaN\n      1.0\n      Strengthening Communities\n      ...\n      January 29 2011\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      5022\n      37237\n      500\n      0\n      Fight global hunger and support women farmers ...\n      14\n      The Oxfam Action Corps is a group of dedicated...\n      NaN\n      1.0\n      Strengthening Communities\n      ...\n      March 31 2012\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      5055\n      37425\n      15\n      0\n      Stop 'N' Swap\n      31\n      Stop 'N' Swap reduces NYC's waste by finding n...\n      NaN\n      4.0\n      Environment\n      ...\n      February 05 2011\n      approved\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 35 columns\n\n\n\n\n\nCode\nvolunteer.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 665 entries, 0 to 664\nData columns (total 35 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   opportunity_id      665 non-null    int64  \n 1   content_id          665 non-null    int64  \n 2   vol_requests        665 non-null    int64  \n 3   event_time          665 non-null    int64  \n 4   title               665 non-null    object \n 5   hits                665 non-null    int64  \n 6   summary             665 non-null    object \n 7   is_priority         62 non-null     object \n 8   category_id         617 non-null    float64\n 9   category_desc       617 non-null    object \n 10  amsl                0 non-null      float64\n 11  amsl_unit           0 non-null      float64\n 12  org_title           665 non-null    object \n 13  org_content_id      665 non-null    int64  \n 14  addresses_count     665 non-null    int64  \n 15  locality            595 non-null    object \n 16  region              665 non-null    object \n 17  postalcode          659 non-null    float64\n 18  primary_loc         0 non-null      float64\n 19  display_url         665 non-null    object \n 20  recurrence_type     665 non-null    object \n 21  hours               665 non-null    int64  \n 22  created_date        665 non-null    object \n 23  last_modified_date  665 non-null    object \n 24  start_date_date     665 non-null    object \n 25  end_date_date       665 non-null    object \n 26  status              665 non-null    object \n 27  Latitude            0 non-null      float64\n 28  Longitude           0 non-null      float64\n 29  Community Board     0 non-null      float64\n 30  Community Council   0 non-null      float64\n 31  Census Tract        0 non-null      float64\n 32  BIN                 0 non-null      float64\n 33  BBL                 0 non-null      float64\n 34  NTA                 0 non-null      float64\ndtypes: float64(13), int64(8), object(14)\nmemory usage: 182.0+ KB\n\n\n\n\nCode\nvolunteer.dropna(axis=1, thresh=3).shape\n\n\n(665, 24)\n\n\n\n\nCode\nvolunteer.shape\n\n\n(665, 35)\n\n\n\n\n\nTaking a look at the volunteer dataset again, we want to drop rows where the category_desc column values are missing. We’re going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values.\n\n\nCode\n# Check how many values are missing in the category_desc column\nprint(volunteer['category_desc'].isnull().sum())\n\n# Subset the volunteer dataset\nvolunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n\n# Print out the shape of the subset\nprint(volunteer_subset.shape)\n\n\n48\n(617, 35)\n\n\n\n\n\n\n\ndtypes in pandas\n\nobject: string/mixed types\nint64: integer\nfloat64: float\ndatetime64 (or timedelta): datetime\n\n\n\n\nTaking another look at the dataset comprised of volunteer information from New York City, we want to know what types we’ll be working with as we start to do more preprocessing.\n\n\nCode\nvolunteer.dtypes\n\n\nopportunity_id          int64\ncontent_id              int64\nvol_requests            int64\nevent_time              int64\ntitle                  object\nhits                    int64\nsummary                object\nis_priority            object\ncategory_id           float64\ncategory_desc          object\namsl                  float64\namsl_unit             float64\norg_title              object\norg_content_id          int64\naddresses_count         int64\nlocality               object\nregion                 object\npostalcode            float64\nprimary_loc           float64\ndisplay_url            object\nrecurrence_type        object\nhours                   int64\ncreated_date           object\nlast_modified_date     object\nstart_date_date        object\nend_date_date          object\nstatus                 object\nLatitude              float64\nLongitude             float64\nCommunity Board       float64\nCommunity Council     float64\nCensus Tract          float64\nBIN                   float64\nBBL                   float64\nNTA                   float64\ndtype: object\n\n\n\n\n\nIf you take a look at the volunteer dataset types, you’ll see that the column hits is type object. But, if you actually look at the column, you’ll see that it consists of integers. Let’s convert that column to type int.\n\n\nCode\n# Print the head of the hits column\nprint(volunteer['hits'].head())\n\n# Convert the hits column to type int\nvolunteer['hits'] = volunteer['hits'].astype(int)\n\n# Look at the dtypes of the dataset\nprint(volunteer.dtypes)\n\n\n0    737\n1     22\n2     62\n3     14\n4     31\nName: hits, dtype: int64\nopportunity_id          int64\ncontent_id              int64\nvol_requests            int64\nevent_time              int64\ntitle                  object\nhits                    int32\nsummary                object\nis_priority            object\ncategory_id           float64\ncategory_desc          object\namsl                  float64\namsl_unit             float64\norg_title              object\norg_content_id          int64\naddresses_count         int64\nlocality               object\nregion                 object\npostalcode            float64\nprimary_loc           float64\ndisplay_url            object\nrecurrence_type        object\nhours                   int64\ncreated_date           object\nlast_modified_date     object\nstart_date_date        object\nend_date_date          object\nstatus                 object\nLatitude              float64\nLongitude             float64\nCommunity Board       float64\nCommunity Council     float64\nCensus Tract          float64\nBIN                   float64\nBBL                   float64\nNTA                   float64\ndtype: object\n\n\n\n\n\n\n\nStratified sampling\n\nA way of sampling that takes into account the distribution of classes or features in your dataset\n\n\n\n\nIn the volunteer dataset, we’re thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label.\n\n\nCode\nvolunteer['category_desc'].value_counts()\n\n\nStrengthening Communities    307\nHelping Neighbors in Need    119\nEducation                     92\nHealth                        52\nEnvironment                   32\nEmergency Preparedness        15\nName: category_desc, dtype: int64\n\n\n\n\n\nWe know that the distribution of variables in the category_desc column in the volunteer dataset is uneven. If we wanted to train a model to try to predict category_desc, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n# Create a data with all columns except category_desc\nvolunteer_X = volunteer.dropna(subset=['category_desc'], axis=0)\n\n# Create a category_desc labels dataset\nvolunteer_y = volunteer.dropna(subset=['category_desc'], axis=0)[['category_desc']]\n\n# Use stratified sampling to split up the dataset according to the volunteer_y dataset\nX_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n\n# Print out the category_desc counts on the training y labels\nprint(y_train['category_desc'].value_counts())\n\n\nStrengthening Communities    230\nHelping Neighbors in Need     89\nEducation                     69\nHealth                        39\nEnvironment                   24\nEmergency Preparedness        11\nName: category_desc, dtype: int64\n\n\n\nWarning: stratify sampling on train_test_split cannot handle the NaN data, so you need to drop NaN values before sampling"
  },
  {
    "objectID": "posts/Introduction to hypothesis testing/Introduction to Hypothesis Testing.html",
    "href": "posts/Introduction to hypothesis testing/Introduction to Hypothesis Testing.html",
    "title": "Introduction to Hypothesis Testing",
    "section": "",
    "text": "We will walk you through the steps of creating a one sample proportional test so that you will be able to better understand how hypothesis tests work and what problems they can solve. In doing so, we will also introduce important concepts such as z-scores, parabolae, and false negative and false positive errors.\nThis Introduction to Hypothesis Testing is part of Datacamp course: Hypothesis Testing in Python\nThis is my learning experience of data science through DataCamp\n\n\n\n\nA/B testing: also known as split testing, refers to random experiment to test variable / outcome on treatment & control group Hypothesis: a theory or assumption yet to be proved Point estimation: sample statistics or sample mean of population mean_samp = population[‘column’].mean() Standard error: standard deviation of sample statistics or sample mean in bootstrap distribution estimates standard error std_error = np.std(so_boot_distn, ddof=1)\n\n\n\nSince variables have different units & ranges, we need to standardize their value before testing our hypothesis standardized value = (value - mean) / standard deviation\nz = (sample statistics - hypothesis param value) / standard error\nStandard normal distribution: normal distribution with mean = 0 + standard deviation =1\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\n\n\n\n\nCode\nlate_shipments= pd.read_feather('dataset/late_shipments.feather')\nlate_shipments.head()\n\n\n\n\n\n\n  \n    \n      \n      id\n      country\n      managed_by\n      fulfill_via\n      vendor_inco_term\n      shipment_mode\n      late_delivery\n      late\n      product_group\n      sub_classification\n      ...\n      line_item_quantity\n      line_item_value\n      pack_price\n      unit_price\n      manufacturing_site\n      first_line_designation\n      weight_kilograms\n      freight_cost_usd\n      freight_cost_groups\n      line_item_insurance_usd\n    \n  \n  \n    \n      0\n      36203.0\n      Nigeria\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      1.0\n      Yes\n      HRDT\n      HIV test\n      ...\n      2996.0\n      266644.00\n      89.00\n      0.89\n      Alere Medical Co., Ltd.\n      Yes\n      1426.0\n      33279.83\n      expensive\n      373.83\n    \n    \n      1\n      30998.0\n      Botswana\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      HRDT\n      HIV test\n      ...\n      25.0\n      800.00\n      32.00\n      1.60\n      Trinity Biotech, Plc\n      Yes\n      10.0\n      559.89\n      reasonable\n      1.72\n    \n    \n      2\n      69871.0\n      Vietnam\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      ARV\n      Adult\n      ...\n      22925.0\n      110040.00\n      4.80\n      0.08\n      Hetero Unit III Hyderabad IN\n      Yes\n      3723.0\n      19056.13\n      expensive\n      181.57\n    \n    \n      3\n      17648.0\n      South Africa\n      PMO - US\n      Direct Drop\n      DDP\n      Ocean\n      0.0\n      No\n      ARV\n      Adult\n      ...\n      152535.0\n      361507.95\n      2.37\n      0.04\n      Aurobindo Unit III, India\n      Yes\n      7698.0\n      11372.23\n      expensive\n      779.41\n    \n    \n      4\n      5647.0\n      Uganda\n      PMO - US\n      Direct Drop\n      EXW\n      Air\n      0.0\n      No\n      HRDT\n      HIV test - Ancillary\n      ...\n      850.0\n      8.50\n      0.01\n      0.00\n      Inverness Japan\n      Yes\n      56.0\n      360.00\n      reasonable\n      0.01\n    \n  \n\n5 rows × 27 columns\n\n\n\n\n\n\nWe’ll begin our analysis by calculating a point estimate (or sample statistic), namely the proportion of late shipments.\n\n\nCode\n# Print the late_shipments dataset\nprint(late_shipments)\n\n# Calculate the proportion of late shipments\nlate_prop_samp = (late_shipments['late']=='Yes').mean()\n\n# Print the results\nprint(late_prop_samp)\nprint(\"\\nThe proportion of late shipments in the sample is 0.061, or 6.1%\")\n\n\n          id       country managed_by  fulfill_via vendor_inco_term  \\\n0    36203.0       Nigeria   PMO - US  Direct Drop              EXW   \n1    30998.0      Botswana   PMO - US  Direct Drop              EXW   \n2    69871.0       Vietnam   PMO - US  Direct Drop              EXW   \n3    17648.0  South Africa   PMO - US  Direct Drop              DDP   \n4     5647.0        Uganda   PMO - US  Direct Drop              EXW   \n..       ...           ...        ...          ...              ...   \n995  13608.0        Uganda   PMO - US  Direct Drop              DDP   \n996  80394.0    Congo, DRC   PMO - US  Direct Drop              EXW   \n997  61675.0        Zambia   PMO - US  Direct Drop              EXW   \n998  39182.0  South Africa   PMO - US  Direct Drop              DDP   \n999   5645.0      Botswana   PMO - US  Direct Drop              EXW   \n\n    shipment_mode  late_delivery late product_group    sub_classification  \\\n0             Air            1.0  Yes          HRDT              HIV test   \n1             Air            0.0   No          HRDT              HIV test   \n2             Air            0.0   No           ARV                 Adult   \n3           Ocean            0.0   No           ARV                 Adult   \n4             Air            0.0   No          HRDT  HIV test - Ancillary   \n..            ...            ...  ...           ...                   ...   \n995           Air            0.0   No           ARV                 Adult   \n996           Air            0.0   No          HRDT              HIV test   \n997           Air            1.0  Yes          HRDT              HIV test   \n998         Ocean            0.0   No           ARV                 Adult   \n999           Air            0.0   No          HRDT              HIV test   \n\n     ... line_item_quantity line_item_value pack_price unit_price  \\\n0    ...             2996.0       266644.00      89.00       0.89   \n1    ...               25.0          800.00      32.00       1.60   \n2    ...            22925.0       110040.00       4.80       0.08   \n3    ...           152535.0       361507.95       2.37       0.04   \n4    ...              850.0            8.50       0.01       0.00   \n..   ...                ...             ...        ...        ...   \n995  ...              121.0         9075.00      75.00       0.62   \n996  ...              292.0         9344.00      32.00       1.60   \n997  ...             2127.0       170160.00      80.00       0.80   \n998  ...           191011.0       861459.61       4.51       0.15   \n999  ...              200.0        14398.00      71.99       0.72   \n\n               manufacturing_site first_line_designation  weight_kilograms  \\\n0         Alere Medical Co., Ltd.                    Yes            1426.0   \n1            Trinity Biotech, Plc                    Yes              10.0   \n2    Hetero Unit III Hyderabad IN                    Yes            3723.0   \n3       Aurobindo Unit III, India                    Yes            7698.0   \n4                 Inverness Japan                    Yes              56.0   \n..                            ...                    ...               ...   \n995     Janssen-Cilag, Latina, IT                    Yes              43.0   \n996          Trinity Biotech, Plc                    Yes              99.0   \n997       Alere Medical Co., Ltd.                    Yes             881.0   \n998     Aurobindo Unit III, India                    Yes           16234.0   \n999               Inverness Japan                    Yes              46.0   \n\n     freight_cost_usd  freight_cost_groups  line_item_insurance_usd  \n0            33279.83            expensive                   373.83  \n1              559.89           reasonable                     1.72  \n2            19056.13            expensive                   181.57  \n3            11372.23            expensive                   779.41  \n4              360.00           reasonable                     0.01  \n..                ...                  ...                      ...  \n995            199.00           reasonable                    12.72  \n996           2162.55           reasonable                    13.10  \n997          14019.38            expensive                   210.49  \n998          14439.17            expensive                  1421.41  \n999           1028.18           reasonable                    23.04  \n\n[1000 rows x 27 columns]\n0.061\n\nThe proportion of late shipments in the sample is 0.061, or 6.1%\n\n\n\n\nCode\nlate_prop_Yes=late_shipments[late_shipments['late']=='Yes']\nlate_prop_Yes.head(10)\nlate_prop_Yes.shape\n\n\n(61, 27)\n\n\n\n\n\n\n\nCode\nlate_shipments_boot_distn=[0.064,\n 0.049,\n 0.06,\n 0.066,\n 0.052,\n 0.066,\n 0.071,\n 0.061,\n 0.051,\n 0.06,\n 0.053,\n 0.066,\n 0.069,\n 0.068,\n 0.063,\n 0.061,\n 0.052,\n 0.045,\n 0.054,\n 0.054,\n 0.064,\n 0.064,\n 0.058,\n 0.062,\n 0.05,\n 0.053,\n 0.064,\n 0.058,\n 0.071,\n 0.064,\n 0.052,\n 0.063,\n 0.056,\n 0.05,\n 0.058,\n 0.06,\n 0.068,\n 0.065,\n 0.056,\n 0.052,\n 0.061,\n 0.059,\n 0.054,\n 0.071,\n 0.067,\n 0.079,\n 0.069,\n 0.069,\n 0.05,\n 0.059,\n 0.062,\n 0.046,\n 0.068,\n 0.057,\n 0.067,\n 0.042,\n 0.074,\n 0.063,\n 0.056,\n 0.063,\n 0.068,\n 0.06,\n 0.068,\n 0.064,\n 0.052,\n 0.045,\n 0.058,\n 0.072,\n 0.078,\n 0.055,\n 0.069,\n 0.048,\n 0.047,\n 0.061,\n 0.066,\n 0.062,\n 0.059,\n 0.062,\n 0.054,\n 0.063,\n 0.061,\n 0.059,\n 0.057,\n 0.059,\n 0.058,\n 0.068,\n 0.067,\n 0.059,\n 0.054,\n 0.064,\n 0.047,\n 0.054,\n 0.065,\n 0.063,\n 0.057,\n 0.062,\n 0.058,\n 0.046,\n 0.052,\n 0.065,\n 0.053,\n 0.069,\n 0.068,\n 0.065,\n 0.052,\n 0.061,\n 0.058,\n 0.042,\n 0.064,\n 0.063,\n 0.068,\n 0.067,\n 0.061,\n 0.056,\n 0.061,\n 0.044,\n 0.058,\n 0.051,\n 0.075,\n 0.064,\n 0.073,\n 0.058,\n 0.056,\n 0.055,\n 0.063,\n 0.056,\n 0.067,\n 0.075,\n 0.061,\n 0.063,\n 0.051,\n 0.065,\n 0.069,\n 0.066,\n 0.05,\n 0.066,\n 0.057,\n 0.064,\n 0.065,\n 0.062,\n 0.071,\n 0.062,\n 0.065,\n 0.062,\n 0.066,\n 0.071,\n 0.058,\n 0.053,\n 0.062,\n 0.051,\n 0.056,\n 0.061,\n 0.074,\n 0.054,\n 0.059,\n 0.069,\n 0.073,\n 0.066,\n 0.052,\n 0.065,\n 0.072,\n 0.071,\n 0.059,\n 0.065,\n 0.06,\n 0.055,\n 0.053,\n 0.059,\n 0.066,\n 0.061,\n 0.053,\n 0.053,\n 0.06,\n 0.058,\n 0.074,\n 0.05,\n 0.059,\n 0.067,\n 0.06,\n 0.064,\n 0.061,\n 0.072,\n 0.06,\n 0.048,\n 0.066,\n 0.059,\n 0.08,\n 0.062,\n 0.066,\n 0.065,\n 0.06,\n 0.048,\n 0.064,\n 0.07,\n 0.053,\n 0.035,\n 0.071,\n 0.061,\n 0.051,\n 0.052,\n 0.051,\n 0.069,\n 0.052,\n 0.052,\n 0.065,\n 0.053,\n 0.055,\n 0.063,\n 0.066,\n 0.062,\n 0.067,\n 0.079,\n 0.062,\n 0.056,\n 0.058,\n 0.068,\n 0.062,\n 0.045,\n 0.063,\n 0.069,\n 0.054,\n 0.065,\n 0.061,\n 0.057,\n 0.05,\n 0.048,\n 0.069,\n 0.058,\n 0.052,\n 0.056,\n 0.057,\n 0.071,\n 0.059,\n 0.062,\n 0.064,\n 0.053,\n 0.065,\n 0.056,\n 0.06,\n 0.062,\n 0.042,\n 0.054,\n 0.051,\n 0.061,\n 0.049,\n 0.071,\n 0.072,\n 0.059,\n 0.063,\n 0.049,\n 0.074,\n 0.063,\n 0.052,\n 0.055,\n 0.072,\n 0.054,\n 0.067,\n 0.067,\n 0.067,\n 0.055,\n 0.073,\n 0.064,\n 0.069,\n 0.06,\n 0.053,\n 0.057,\n 0.056,\n 0.058,\n 0.067,\n 0.065,\n 0.064,\n 0.053,\n 0.055,\n 0.069,\n 0.058,\n 0.07,\n 0.068,\n 0.062,\n 0.062,\n 0.05,\n 0.069,\n 0.061,\n 0.057,\n 0.066,\n 0.056,\n 0.053,\n 0.055,\n 0.062,\n 0.064,\n 0.055,\n 0.056,\n 0.061,\n 0.058,\n 0.068,\n 0.079,\n 0.057,\n 0.049,\n 0.052,\n 0.063,\n 0.064,\n 0.059,\n 0.071,\n 0.064,\n 0.052,\n 0.066,\n 0.063,\n 0.069,\n 0.056,\n 0.057,\n 0.062,\n 0.057,\n 0.055,\n 0.062,\n 0.06,\n 0.064,\n 0.057,\n 0.062,\n 0.069,\n 0.067,\n 0.052,\n 0.061,\n 0.056,\n 0.055,\n 0.056,\n 0.055,\n 0.064,\n 0.068,\n 0.051,\n 0.054,\n 0.057,\n 0.054,\n 0.07,\n 0.049,\n 0.058,\n 0.063,\n 0.07,\n 0.046,\n 0.059,\n 0.064,\n 0.059,\n 0.061,\n 0.066,\n 0.06,\n 0.073,\n 0.08,\n 0.069,\n 0.061,\n 0.071,\n 0.068,\n 0.065,\n 0.063,\n 0.054,\n 0.07,\n 0.061,\n 0.053,\n 0.059,\n 0.047,\n 0.064,\n 0.071,\n 0.068,\n 0.049,\n 0.063,\n 0.057,\n 0.057,\n 0.059,\n 0.061,\n 0.048,\n 0.084,\n 0.07,\n 0.077,\n 0.043,\n 0.065,\n 0.057,\n 0.057,\n 0.054,\n 0.064,\n 0.062,\n 0.067,\n 0.068,\n 0.06,\n 0.054,\n 0.066,\n 0.048,\n 0.048,\n 0.06,\n 0.054,\n 0.067,\n 0.064,\n 0.064,\n 0.067,\n 0.058,\n 0.066,\n 0.06,\n 0.048,\n 0.058,\n 0.054,\n 0.056,\n 0.055,\n 0.068,\n 0.077,\n 0.06,\n 0.061,\n 0.055,\n 0.065,\n 0.064,\n 0.058,\n 0.058,\n 0.058,\n 0.055,\n 0.067,\n 0.061,\n 0.063,\n 0.065,\n 0.071,\n 0.051,\n 0.066,\n 0.066,\n 0.066,\n 0.07,\n 0.068,\n 0.061,\n 0.062,\n 0.054,\n 0.058,\n 0.066,\n 0.059,\n 0.061,\n 0.058,\n 0.057,\n 0.065,\n 0.053,\n 0.053,\n 0.06,\n 0.068,\n 0.067,\n 0.068,\n 0.061,\n 0.067,\n 0.059,\n 0.057,\n 0.055,\n 0.067,\n 0.058,\n 0.055,\n 0.055,\n 0.054,\n 0.061,\n 0.074,\n 0.071,\n 0.057,\n 0.056,\n 0.047,\n 0.07,\n 0.054,\n 0.052,\n 0.072,\n 0.054,\n 0.064,\n 0.063,\n 0.075,\n 0.064,\n 0.051,\n 0.061,\n 0.064,\n 0.047,\n 0.067,\n 0.061,\n 0.06,\n 0.057,\n 0.059,\n 0.058,\n 0.07,\n 0.06,\n 0.056,\n 0.064,\n 0.056,\n 0.066,\n 0.051,\n 0.064,\n 0.054,\n 0.058,\n 0.064,\n 0.041,\n 0.057,\n 0.055,\n 0.06,\n 0.06,\n 0.051,\n 0.054,\n 0.07,\n 0.053,\n 0.063,\n 0.058,\n 0.066,\n 0.059,\n 0.051,\n 0.067,\n 0.078,\n 0.056,\n 0.068,\n 0.057,\n 0.059,\n 0.062,\n 0.053,\n 0.064,\n 0.067,\n 0.068,\n 0.071,\n 0.066,\n 0.057,\n 0.063,\n 0.067,\n 0.059,\n 0.057,\n 0.064,\n 0.049,\n 0.066,\n 0.055,\n 0.071,\n 0.061,\n 0.078,\n 0.062,\n 0.052,\n 0.058,\n 0.066,\n 0.06,\n 0.054,\n 0.058,\n 0.054,\n 0.062,\n 0.072,\n 0.068,\n 0.057,\n 0.059,\n 0.066,\n 0.066,\n 0.065,\n 0.067,\n 0.071,\n 0.064,\n 0.072,\n 0.067,\n 0.064,\n 0.064,\n 0.051,\n 0.061,\n 0.047,\n 0.07,\n 0.073,\n 0.06,\n 0.066,\n 0.058,\n 0.056,\n 0.064,\n 0.059,\n 0.062,\n 0.046,\n 0.07,\n 0.07,\n 0.071,\n 0.056,\n 0.061,\n 0.066,\n 0.058,\n 0.055,\n 0.073,\n 0.068,\n 0.073,\n 0.055,\n 0.074,\n 0.063,\n 0.049,\n 0.063,\n 0.063,\n 0.056,\n 0.061,\n 0.065,\n 0.066,\n 0.06,\n 0.057,\n 0.07,\n 0.06,\n 0.053,\n 0.055,\n 0.066,\n 0.07,\n 0.069,\n 0.051,\n 0.067,\n 0.055,\n 0.06,\n 0.074,\n 0.06,\n 0.057,\n 0.06,\n 0.054,\n 0.054,\n 0.058,\n 0.06,\n 0.057,\n 0.059,\n 0.065,\n 0.061,\n 0.073,\n 0.067,\n 0.063,\n 0.079,\n 0.063,\n 0.063,\n 0.051,\n 0.074,\n 0.06,\n 0.07,\n 0.063,\n 0.072,\n 0.066,\n 0.058,\n 0.046,\n 0.059,\n 0.064,\n 0.058,\n 0.071,\n 0.055,\n 0.062,\n 0.05,\n 0.055,\n 0.061,\n 0.052,\n 0.059,\n 0.063,\n 0.058,\n 0.044,\n 0.052,\n 0.069,\n 0.056,\n 0.057,\n 0.064,\n 0.067,\n 0.058,\n 0.07,\n 0.065,\n 0.068,\n 0.061,\n 0.055,\n 0.06,\n 0.053,\n 0.066,\n 0.052,\n 0.064,\n 0.051,\n 0.076,\n 0.069,\n 0.056,\n 0.057,\n 0.068,\n 0.07,\n 0.065,\n 0.062,\n 0.066,\n 0.063,\n 0.066,\n 0.054,\n 0.061,\n 0.061,\n 0.055,\n 0.053,\n 0.054,\n 0.065,\n 0.073,\n 0.064,\n 0.054,\n 0.065,\n 0.06,\n 0.059,\n 0.056,\n 0.064,\n 0.057,\n 0.06,\n 0.07,\n 0.063,\n 0.064,\n 0.067,\n 0.061,\n 0.053,\n 0.06,\n 0.064,\n 0.064,\n 0.057,\n 0.046,\n 0.057,\n 0.065,\n 0.074,\n 0.062,\n 0.063,\n 0.054,\n 0.074,\n 0.064,\n 0.077,\n 0.068,\n 0.06,\n 0.063,\n 0.059,\n 0.06,\n 0.068,\n 0.052,\n 0.064,\n 0.057,\n 0.059,\n 0.069,\n 0.061,\n 0.064,\n 0.047,\n 0.062,\n 0.069,\n 0.054,\n 0.069,\n 0.063,\n 0.077,\n 0.06,\n 0.061,\n 0.055,\n 0.069,\n 0.061,\n 0.06,\n 0.061,\n 0.067,\n 0.05,\n 0.061,\n 0.062,\n 0.081,\n 0.071,\n 0.057,\n 0.055,\n 0.054,\n 0.07,\n 0.068,\n 0.063,\n 0.056,\n 0.081,\n 0.049,\n 0.07,\n 0.048,\n 0.046,\n 0.069,\n 0.056,\n 0.066,\n 0.058,\n 0.058,\n 0.062,\n 0.052,\n 0.065,\n 0.043,\n 0.062,\n 0.063,\n 0.053,\n 0.073,\n 0.058,\n 0.064,\n 0.071,\n 0.073,\n 0.059,\n 0.08,\n 0.052,\n 0.053,\n 0.053,\n 0.053,\n 0.057,\n 0.061,\n 0.069,\n 0.046,\n 0.063,\n 0.078,\n 0.06,\n 0.06,\n 0.064,\n 0.063,\n 0.065,\n 0.069,\n 0.059,\n 0.068,\n 0.061,\n 0.066,\n 0.064,\n 0.064,\n 0.058,\n 0.046,\n 0.073,\n 0.06,\n 0.056,\n 0.073,\n 0.07,\n 0.058,\n 0.056,\n 0.064,\n 0.069,\n 0.065,\n 0.063,\n 0.063,\n 0.054,\n 0.081,\n 0.044,\n 0.048,\n 0.059,\n 0.058,\n 0.046,\n 0.063,\n 0.072,\n 0.063,\n 0.059,\n 0.063,\n 0.047,\n 0.063,\n 0.065,\n 0.071,\n 0.061,\n 0.05,\n 0.063,\n 0.065,\n 0.054,\n 0.053,\n 0.061,\n 0.054,\n 0.063,\n 0.056,\n 0.071,\n 0.057,\n 0.058,\n 0.049,\n 0.074,\n 0.057,\n 0.058,\n 0.07,\n 0.063,\n 0.057,\n 0.052,\n 0.064,\n 0.074,\n 0.047,\n 0.071,\n 0.051,\n 0.059,\n 0.05,\n 0.059,\n 0.05,\n 0.05,\n 0.057,\n 0.075,\n 0.053,\n 0.07,\n 0.062,\n 0.062,\n 0.075,\n 0.058,\n 0.057,\n 0.05,\n 0.062,\n 0.061,\n 0.067,\n 0.062,\n 0.059,\n 0.059,\n 0.049,\n 0.052,\n 0.062,\n 0.069,\n 0.062,\n 0.054,\n 0.05,\n 0.063,\n 0.052,\n 0.063,\n 0.069,\n 0.057,\n 0.067,\n 0.064,\n 0.057,\n 0.057,\n 0.057,\n 0.05,\n 0.062,\n 0.069,\n 0.075,\n 0.075,\n 0.05,\n 0.06,\n 0.065,\n 0.051,\n 0.063,\n 0.075,\n 0.06,\n 0.058,\n 0.063,\n 0.069,\n 0.055,\n 0.062,\n 0.06,\n 0.057,\n 0.079,\n 0.046,\n 0.059,\n 0.07,\n 0.055,\n 0.08,\n 0.048,\n 0.061,\n 0.042,\n 0.068,\n 0.082,\n 0.044,\n 0.054,\n 0.063,\n 0.054,\n 0.071,\n 0.053,\n 0.061,\n 0.06,\n 0.065,\n 0.072,\n 0.063,\n 0.062,\n 0.053,\n 0.072,\n 0.067,\n 0.058,\n 0.075,\n 0.07,\n 0.052,\n 0.056,\n 0.056,\n 0.082,\n 0.055,\n 0.056,\n 0.057,\n 0.056,\n 0.054,\n 0.073,\n 0.081,\n 0.063,\n 0.063,\n 0.054,\n 0.058,\n 0.062,\n 0.065,\n 0.063,\n 0.062,\n 0.056,\n 0.063,\n 0.06,\n 0.061,\n 0.068,\n 0.067,\n 0.07,\n 0.059,\n 0.06,\n 0.063,\n 0.057,\n 0.052,\n 0.062,\n 0.064,\n 0.065,\n 0.07,\n 0.063,\n 0.062,\n 0.052,\n 0.055,\n 0.055,\n 0.053,\n 0.057,\n 0.058,\n 0.062,\n 0.06,\n 0.056,\n 0.064,\n 0.074,\n 0.071,\n 0.059,\n 0.056,\n 0.063,\n 0.059,\n 0.058,\n 0.054,\n 0.058,\n 0.069,\n 0.06,\n 0.063,\n 0.054,\n 0.047,\n 0.061,\n 0.057,\n 0.059,\n 0.057,\n 0.063,\n 0.06,\n 0.071,\n 0.062,\n 0.06,\n 0.071,\n 0.059,\n 0.049,\n 0.077]\n\n\n\n\nCode\nlate_shipments['late'].unique()\n\n\narray(['Yes', 'No'], dtype=object)\n\n\n\n\nCode\n# Hypothesize that the proportion is 6%\nlate_prop_hyp = 0.06\n\n#for i in range(5000):\n    #np.mean(late_shipments_boot_distn.append(late_shipments.sample(frac=1, replace=True)['late']))\n\n#print(late_shipments_boot_distn)\n# Calculate the standard error\nstd_error = np.std(late_shipments_boot_distn,ddof=1)\n\n# Find z-score of late_prop_samp\nz_score = (late_prop_samp - late_prop_hyp) / std_error\n\n# Print z_score\nprint(z_score)\nprint(\"\\nThe z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic\")\n\n\n0.13387997080083944\n\nThe z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic\n\n\n\n\n\nHypothesis tests check if the sample statistics lie in the tails of the null distribution\nalternative different from null : Two Tail test alternative greater than null : right tail test alternative lower than null: left tail test\n\n\n\np-values measure the strength of support for the null hypothesis, or in other words, they measure the probability of obtaining a result, assuming the null hypothesis is true. Large p-values mean our statistic is producing a result that is likely not in a tail of our null distribution, and chance could be a good explanation for the result. Small p-values mean our statistic is producing a result likely in the tail of our null distribution. Because p-values are probabilities, they are always between zero and one\nCalculating p-value Left tail test: norm.cdf() right tail test: 1-norm.cdf()\np-values quantify evidence for the null hypothesis large p-value => fail to reject null hypothesis small p-value => reject null hypothesis\n\n\nCode\n# Calculate the z-score of late_prop_samp\nz_score = (late_prop_samp - late_prop_hyp) / std_error\n\n# Calculate the p-value\np_value = 1-norm.cdf(z_score, loc=0, scale=1)\n\n# Print the p-value\nprint(p_value)\n\n\n0.44674874433656875\n\n\n\n\n\nType I and type II errors\nFor hypothesis tests and for criminal trials, there are two states of truth and two possible outcomes. Two combinations are correct test outcomes, and there are two ways it can go wrong.\nThe errors are known as false positives (or “type I errors”), and false negatives (or “type II errors”)."
  },
  {
    "objectID": "posts/Introduction to networks in network analysis/Introduction to networks.html",
    "href": "posts/Introduction to networks in network analysis/Introduction to networks.html",
    "title": "Introduction to networks",
    "section": "",
    "text": "During this course, we will explore a dataset of Twitter networks to learn about fundamental concepts in network analytics. We will also learn about NetworkX, a library for manipulating, analyzing, and modeling graphs. In addition to learning about the different types of graphs, we’ll learn how to visualize them rationally.\nThis Introduction to networks is part of Datacamp course: Introduction to Network Analysis in Python\nThis is my learning experience of data science through DataCamp. These repository contributions are part of my learning journey through my graduate program masters of applied data sciences (MADS) at University Of Michigan, DeepLearning.AI, Coursera & DataCamp. You can find my similar articles & more stories at my medium & LinkedIn profile. I am available at kaggle & github blogs & github repos. Thank you for your motivation, support & valuable feedback.\nThese include projects, coursework & notebook which I learned through my data science journey. They are created for reproducible & future reference purpose only. All source code, slides or screenshot are intellactual property of respective content authors. If you find these contents beneficial, kindly consider learning subscription from DeepLearning.AI Subscription, Coursera, DataCamp\n\n\nCode\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pandas as pd\n\n\n\n\nCode\nT = pd.read_pickle('dataset/ego-twitter.p')\n\n\n\n\n\n\nCode\nnx.draw(T)\nplt.show()\n\n\n\n\n\n\n\nCode\nT\n\n\n<networkx.classes.digraph.DiGraph at 0x105c5ab20>\n\n\n\n\nCode\nT.nodes()\n\n\nNodeView((1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 7700, 7701, 7702, 7703, 7704, 7705, 7706, 7707, 7708, 7709, 7710, 7711, 7712, 7713, 7714, 7715, 7716, 7717, 7718, 7719, 7720, 7721, 7722, 7723, 7724, 7725, 7726, 7727, 7728, 7729, 7730, 7731, 7732, 7733, 7734, 7735, 7736, 7737, 7738, 7739, 7740, 7741, 7742, 7743, 7744, 7745, 7746, 7747, 7748, 7749, 7750, 7751, 7752, 7753, 7754, 7755, 7756, 7757, 7758, 7759, 7760, 7761, 7762, 7763, 7764, 7765, 7766, 7767, 7768, 7769, 7770, 7771, 7772, 7773, 7774, 7775, 7776, 7777, 7778, 7779, 7780, 7781, 7782, 7783, 7784, 7785, 7786, 7787, 7788, 7789, 7790, 7791, 7792, 7793, 7794, 7795, 7796, 7797, 7798, 7799, 7800, 7801, 7802, 7803, 7804, 7805, 7806, 7807, 7808, 7809, 7810, 7811, 7812, 7813, 7814, 7815, 7816, 7817, 7818, 7819, 7820, 7821, 7822, 7823, 7824, 7825, 7826, 7827, 7828, 7829, 7830, 7831, 7832, 7833, 7834, 7835, 7836, 7837, 7838, 7839, 7840, 7841, 7842, 7843, 7844, 7845, 7846, 7847, 7848, 7849, 7850, 7851, 7852, 7853, 7854, 7855, 7856, 7857, 7858, 7859, 7860, 7861, 7862, 7863, 7864, 7865, 7866, 7867, 7868, 7869, 7870, 7871, 7872, 7873, 7874, 7875, 7876, 7877, 7878, 7879, 7880, 7881, 7882, 7883, 7884, 7885, 7886, 7887, 7888, 7889, 7890, 7891, 7892, 7893, 7894, 7895, 7896, 7897, 7898, 7899, 7900, 7901, 7902, 7903, 7904, 7905, 7906, 7907, 7908, 7909, 7910, 7911, 7912, 7913, 7914, 7915, 7916, 7917, 7918, 7919, 7920, 7921, 7922, 7923, 7924, 7925, 7926, 7927, 7928, 7929, 7930, 7931, 7932, 7933, 7934, 7935, 7936, 7937, 7938, 7939, 7940, 7941, 7942, 7943, 7944, 7945, 7946, 7947, 7948, 7949, 7950, 7951, 7952, 7953, 7954, 7955, 7956, 7957, 7958, 7959, 7960, 7961, 7962, 7963, 7964, 7965, 7966, 7967, 7968, 7969, 7970, 7971, 7972, 7973, 7974, 7975, 7976, 7977, 7978, 7979, 7980, 7981, 7982, 7983, 7984, 7985, 7986, 7987, 7988, 7989, 7990, 7991, 7992, 7993, 7994, 7995, 7996, 7997, 7998, 7999, 8000, 8001, 8002, 8003, 8004, 8005, 8006, 8007, 8008, 8009, 8010, 8011, 8012, 8013, 8014, 8015, 8016, 8017, 8018, 8019, 8020, 8021, 8022, 8023, 8024, 8025, 8026, 8027, 8028, 8029, 8030, 8031, 8032, 8033, 8034, 8035, 8036, 8037, 8038, 8039, 8040, 8041, 8042, 8043, 8044, 8045, 8046, 8047, 8048, 8049, 8050, 8051, 8052, 8053, 8054, 8055, 8056, 8057, 8058, 8059, 8060, 8061, 8062, 8063, 8064, 8065, 8066, 8067, 8068, 8069, 8070, 8071, 8072, 8073, 8074, 8075, 8076, 8077, 8078, 8079, 8080, 8081, 8082, 8083, 8084, 8085, 8086, 8087, 8088, 8089, 8090, 8091, 8092, 8093, 8094, 8095, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 8104, 8105, 8106, 8107, 8108, 8109, 8110, 8111, 8112, 8113, 8114, 8115, 8116, 8117, 8118, 8119, 8120, 8121, 8122, 8123, 8124, 8125, 8126, 8127, 8128, 8129, 8130, 8131, 8132, 8133, 8134, 8135, 8136, 8137, 8138, 8139, 8140, 8141, 8142, 8143, 8144, 8145, 8146, 8147, 8148, 8149, 8150, 8151, 8152, 8153, 8154, 8155, 8156, 8157, 8158, 8159, 8160, 8161, 8162, 8163, 8164, 8165, 8166, 8167, 8168, 8169, 8170, 8171, 8172, 8173, 8174, 8175, 8176, 8177, 8178, 8179, 8180, 8181, 8182, 8183, 8184, 8185, 8186, 8187, 8188, 8189, 8190, 8191, 8192, 8193, 8194, 8195, 8196, 8197, 8198, 8199, 8200, 8201, 8202, 8203, 8204, 8205, 8206, 8207, 8208, 8209, 8210, 8211, 8212, 8213, 8214, 8215, 8216, 8217, 8218, 8219, 8220, 8221, 8222, 8223, 8224, 8225, 8226, 8227, 8228, 8229, 8230, 8231, 8232, 8233, 8234, 8235, 8236, 8237, 8238, 8239, 8240, 8241, 8242, 8243, 8244, 8245, 8246, 8247, 8248, 8249, 8250, 8251, 8252, 8253, 8254, 8255, 8256, 8257, 8258, 8259, 8260, 8261, 8262, 8263, 8264, 8265, 8266, 8267, 8268, 8269, 8270, 8271, 8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281, 8282, 8283, 8284, 8285, 8286, 8287, 8288, 8289, 8290, 8291, 8292, 8293, 8294, 8295, 8296, 8297, 8298, 8299, 8300, 8301, 8302, 8303, 8304, 8305, 8306, 8307, 8308, 8309, 8310, 8311, 8312, 8313, 8314, 8315, 8316, 8317, 8318, 8319, 8320, 8321, 8322, 8323, 8324, 8325, 8326, 8327, 8328, 8329, 8330, 8331, 8332, 8333, 8334, 8335, 8336, 8337, 8338, 8339, 8340, 8341, 8342, 8343, 8344, 8345, 8346, 8347, 8348, 8349, 8350, 8351, 8352, 8353, 8354, 8355, 8356, 8357, 8358, 8359, 8360, 8361, 8362, 8363, 8364, 8365, 8366, 8367, 8368, 8369, 8370, 8371, 8372, 8373, 8374, 8375, 8376, 8377, 8378, 8379, 8380, 8381, 8382, 8383, 8384, 8385, 8386, 8387, 8388, 8389, 8390, 8391, 8392, 8393, 8394, 8395, 8396, 8397, 8398, 8399, 8400, 8401, 8402, 8403, 8404, 8405, 8406, 8407, 8408, 8409, 8410, 8411, 8412, 8413, 8414, 8415, 8416, 8417, 8418, 8419, 8420, 8421, 8422, 8423, 8424, 8425, 8426, 8427, 8428, 8429, 8430, 8431, 8432, 8433, 8434, 8435, 8436, 8437, 8438, 8439, 8440, 8441, 8442, 8443, 8444, 8445, 8446, 8447, 8448, 8449, 8450, 8451, 8452, 8453, 8454, 8455, 8456, 8457, 8458, 8459, 8460, 8461, 8462, 8463, 8464, 8465, 8466, 8467, 8468, 8469, 8470, 8471, 8472, 8473, 8474, 8475, 8476, 8477, 8478, 8479, 8480, 8481, 8482, 8483, 8484, 8485, 8486, 8487, 8488, 8489, 8490, 8491, 8492, 8493, 8494, 8495, 8496, 8497, 8498, 8499, 8500, 8501, 8502, 8503, 8504, 8505, 8506, 8507, 8508, 8509, 8510, 8511, 8512, 8513, 8514, 8515, 8516, 8517, 8518, 8519, 8520, 8521, 8522, 8523, 8524, 8525, 8526, 8527, 8528, 8529, 8530, 8531, 8532, 8533, 8534, 8535, 8536, 8537, 8538, 8539, 8540, 8541, 8542, 8543, 8544, 8545, 8546, 8547, 8548, 8549, 8550, 8551, 8552, 8553, 8554, 8555, 8556, 8557, 8558, 8559, 8560, 8561, 8562, 8563, 8564, 8565, 8566, 8567, 8568, 8569, 8570, 8571, 8572, 8573, 8574, 8575, 8576, 8577, 8578, 8579, 8580, 8581, 8582, 8583, 8584, 8585, 8586, 8587, 8588, 8589, 8590, 8591, 8592, 8593, 8594, 8595, 8596, 8597, 8598, 8599, 8600, 8601, 8602, 8603, 8604, 8605, 8606, 8607, 8608, 8609, 8610, 8611, 8612, 8613, 8614, 8615, 8616, 8617, 8618, 8619, 8620, 8621, 8622, 8623, 8624, 8625, 8626, 8627, 8628, 8629, 8630, 8631, 8632, 8633, 8634, 8635, 8636, 8637, 8638, 8639, 8640, 8641, 8642, 8643, 8644, 8645, 8646, 8647, 8648, 8649, 8650, 8651, 8652, 8653, 8654, 8655, 8656, 8657, 8658, 8659, 8660, 8661, 8662, 8663, 8664, 8665, 8666, 8667, 8668, 8669, 8670, 8671, 8672, 8673, 8674, 8675, 8676, 8677, 8678, 8679, 8680, 8681, 8682, 8683, 8684, 8685, 8686, 8687, 8688, 8689, 8690, 8691, 8692, 8693, 8694, 8695, 8696, 8697, 8698, 8699, 8700, 8701, 8702, 8703, 8704, 8705, 8706, 8707, 8708, 8709, 8710, 8711, 8712, 8713, 8714, 8715, 8716, 8717, 8718, 8719, 8720, 8721, 8722, 8723, 8724, 8725, 8726, 8727, 8728, 8729, 8730, 8731, 8732, 8733, 8734, 8735, 8736, 8737, 8738, 8739, 8740, 8741, 8742, 8743, 8744, 8745, 8746, 8747, 8748, 8749, 8750, 8751, 8752, 8753, 8754, 8755, 8756, 8757, 8758, 8759, 8760, 8761, 8762, 8763, 8764, 8765, 8766, 8767, 8768, 8769, 8770, 8771, 8772, 8773, 8774, 8775, 8776, 8777, 8778, 8779, 8780, 8781, 8782, 8783, 8784, 8785, 8786, 8787, 8788, 8789, 8790, 8791, 8792, 8793, 8794, 8795, 8796, 8797, 8798, 8799, 8800, 8801, 8802, 8803, 8804, 8805, 8806, 8807, 8808, 8809, 8810, 8811, 8812, 8813, 8814, 8815, 8816, 8817, 8818, 8819, 8820, 8821, 8822, 8823, 8824, 8825, 8826, 8827, 8828, 8829, 8830, 8831, 8832, 8833, 8834, 8835, 8836, 8837, 8838, 8839, 8840, 8841, 8842, 8843, 8844, 8845, 8846, 8847, 8848, 8849, 8850, 8851, 8852, 8853, 8854, 8855, 8856, 8857, 8858, 8859, 8860, 8861, 8862, 8863, 8864, 8865, 8866, 8867, 8868, 8869, 8870, 8871, 8872, 8873, 8874, 8875, 8876, 8877, 8878, 8879, 8880, 8881, 8882, 8883, 8884, 8885, 8886, 8887, 8888, 8889, 8890, 8891, 8892, 8893, 8894, 8895, 8896, 8897, 8898, 8899, 8900, 8901, 8902, 8903, 8904, 8905, 8906, 8907, 8908, 8909, 8910, 8911, 8912, 8913, 8914, 8915, 8916, 8917, 8918, 8919, 8920, 8921, 8922, 8923, 8924, 8925, 8926, 8927, 8928, 8929, 8930, 8931, 8932, 8933, 8934, 8935, 8936, 8937, 8938, 8939, 8940, 8941, 8942, 8943, 8944, 8945, 8946, 8947, 8948, 8949, 8950, 8951, 8952, 8953, 8954, 8955, 8956, 8957, 8958, 8959, 8960, 8961, 8962, 8963, 8964, 8965, 8966, 8967, 8968, 8969, 8970, 8971, 8972, 8973, 8974, 8975, 8976, 8977, 8978, 8979, 8980, 8981, 8982, 8983, 8984, 8985, 8986, 8987, 8988, 8989, 8990, 8991, 8992, 8993, 8994, 8995, 8996, 8997, 8998, 8999, 9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016, 9017, 9018, 9019, 9020, 9021, 9022, 9023, 9024, 9025, 9026, 9027, 9028, 9029, 9030, 9031, 9032, 9033, 9034, 9035, 9036, 9037, 9038, 9039, 9040, 9041, 9042, 9043, 9044, 9045, 9046, 9047, 9048, 9049, 9050, 9051, 9052, 9053, 9054, 9055, 9056, 9057, 9058, 9059, 9060, 9061, 9062, 9063, 9064, 9065, 9066, 9067, 9068, 9069, 9070, 9071, 9072, 9073, 9074, 9075, 9076, 9077, 9078, 9079, 9080, 9081, 9082, 9083, 9084, 9085, 9086, 9087, 9088, 9089, 9090, 9091, 9092, 9093, 9094, 9095, 9096, 9097, 9098, 9099, 9100, 9101, 9102, 9103, 9104, 9105, 9106, 9107, 9108, 9109, 9110, 9111, 9112, 9113, 9114, 9115, 9116, 9117, 9118, 9119, 9120, 9121, 9122, 9123, 9124, 9125, 9126, 9127, 9128, 9129, 9130, 9131, 9132, 9133, 9134, 9135, 9136, 9137, 9138, 9139, 9140, 9141, 9142, 9143, 9144, 9145, 9146, 9147, 9148, 9149, 9150, 9151, 9152, 9153, 9154, 9155, 9156, 9157, 9158, 9159, 9160, 9161, 9162, 9163, 9164, 9165, 9166, 9167, 9168, 9169, 9170, 9171, 9172, 9173, 9174, 9175, 9176, 9177, 9178, 9179, 9180, 9181, 9182, 9183, 9184, 9185, 9186, 9187, 9188, 9189, 9190, 9191, 9192, 9193, 9194, 9195, 9196, 9197, 9198, 9199, 9200, 9201, 9202, 9203, 9204, 9205, 9206, 9207, 9208, 9209, 9210, 9211, 9212, 9213, 9214, 9215, 9216, 9217, 9218, 9219, 9220, 9221, 9222, 9223, 9224, 9225, 9226, 9227, 9228, 9229, 9230, 9231, 9232, 9233, 9234, 9235, 9236, 9237, 9238, 9239, 9240, 9241, 9242, 9243, 9244, 9245, 9246, 9247, 9248, 9249, 9250, 9251, 9252, 9253, 9254, 9255, 9256, 9257, 9258, 9259, 9260, 9261, 9262, 9263, 9264, 9265, 9266, 9267, 9268, 9269, 9270, 9271, 9272, 9273, 9274, 9275, 9276, 9277, 9278, 9279, 9280, 9281, 9282, 9283, 9284, 9285, 9286, 9287, 9288, 9289, 9290, 9291, 9292, 9293, 9294, 9295, 9296, 9297, 9298, 9299, 9300, 9301, 9302, 9303, 9304, 9305, 9306, 9307, 9308, 9309, 9310, 9311, 9312, 9313, 9314, 9315, 9316, 9317, 9318, 9319, 9320, 9321, 9322, 9323, 9324, 9325, 9326, 9327, 9328, 9329, 9330, 9331, 9332, 9333, 9334, 9335, 9336, 9337, 9338, 9339, 9340, 9341, 9342, 9343, 9344, 9345, 9346, 9347, 9348, 9349, 9350, 9351, 9352, 9353, 9354, 9355, 9356, 9357, 9358, 9359, 9360, 9361, 9362, 9363, 9364, 9365, 9366, 9367, 9368, 9369, 9370, 9371, 9372, 9373, 9374, 9375, 9376, 9377, 9378, 9379, 9380, 9381, 9382, 9383, 9384, 9385, 9386, 9387, 9388, 9389, 9390, 9391, 9392, 9393, 9394, 9395, 9396, 9397, 9398, 9399, 9400, 9401, 9402, 9403, 9404, 9405, 9406, 9407, 9408, 9409, 9410, 9411, 9412, 9413, 9414, 9415, 9416, 9417, 9418, 9419, 9420, 9421, 9422, 9423, 9424, 9425, 9426, 9427, 9428, 9429, 9430, 9431, 9432, 9433, 9434, 9435, 9436, 9437, 9438, 9439, 9440, 9441, 9442, 9443, 9444, 9445, 9446, 9447, 9448, 9449, 9450, 9451, 9452, 9453, 9454, 9455, 9456, 9457, 9458, 9459, 9460, 9461, 9462, 9463, 9464, 9465, 9466, 9467, 9468, 9469, 9470, 9471, 9472, 9473, 9474, 9475, 9476, 9477, 9478, 9479, 9480, 9481, 9482, 9483, 9484, 9485, 9486, 9487, 9488, 9489, 9490, 9491, 9492, 9493, 9494, 9495, 9496, 9497, 9498, 9499, 9500, 9501, 9502, 9503, 9504, 9505, 9506, 9507, 9508, 9509, 9510, 9511, 9512, 9513, 9514, 9515, 9516, 9517, 9518, 9519, 9520, 9521, 9522, 9523, 9524, 9525, 9526, 9527, 9528, 9529, 9530, 9531, 9532, 9533, 9534, 9535, 9536, 9537, 9538, 9539, 9540, 9541, 9542, 9543, 9544, 9545, 9546, 9547, 9548, 9549, 9550, 9551, 9552, 9553, 9554, 9555, 9556, 9557, 9558, 9559, 9560, 9561, 9562, 9563, 9564, 9565, 9566, 9567, 9568, 9569, 9570, 9571, 9572, 9573, 9574, 9575, 9576, 9577, 9578, 9579, 9580, 9581, 9582, 9583, 9584, 9585, 9586, 9587, 9588, 9589, 9590, 9591, 9592, 9593, 9594, 9595, 9596, 9597, 9598, 9599, 9600, 9601, 9602, 9603, 9604, 9605, 9606, 9607, 9608, 9609, 9610, 9611, 9612, 9613, 9614, 9615, 9616, 9617, 9618, 9619, 9620, 9621, 9622, 9623, 9624, 9625, 9626, 9627, 9628, 9629, 9630, 9631, 9632, 9633, 9634, 9635, 9636, 9637, 9638, 9639, 9640, 9641, 9642, 9643, 9644, 9645, 9646, 9647, 9648, 9649, 9650, 9651, 9652, 9653, 9654, 9655, 9656, 9657, 9658, 9659, 9660, 9661, 9662, 9663, 9664, 9665, 9666, 9667, 9668, 9669, 9670, 9671, 9672, 9673, 9674, 9675, 9676, 9677, 9678, 9679, 9680, 9681, 9682, 9683, 9684, 9685, 9686, 9687, 9688, 9689, 9690, 9691, 9692, 9693, 9694, 9695, 9696, 9697, 9698, 9699, 9700, 9701, 9702, 9703, 9704, 9705, 9706, 9707, 9708, 9709, 9710, 9711, 9712, 9713, 9714, 9715, 9716, 9717, 9718, 9719, 9720, 9721, 9722, 9723, 9724, 9725, 9726, 9727, 9728, 9729, 9730, 9731, 9732, 9733, 9734, 9735, 9736, 9737, 9738, 9739, 9740, 9741, 9742, 9743, 9744, 9745, 9746, 9747, 9748, 9749, 9750, 9751, 9752, 9753, 9754, 9755, 9756, 9757, 9758, 9759, 9760, 9761, 9762, 9763, 9764, 9765, 9766, 9767, 9768, 9769, 9770, 9771, 9772, 9773, 9774, 9775, 9776, 9777, 9778, 9779, 9780, 9781, 9782, 9783, 9784, 9785, 9786, 9787, 9788, 9789, 9790, 9791, 9792, 9793, 9794, 9795, 9796, 9797, 9798, 9799, 9800, 9801, 9802, 9803, 9804, 9805, 9806, 9807, 9808, 9809, 9810, 9811, 9812, 9813, 9814, 9815, 9816, 9817, 9818, 9819, 9820, 9821, 9822, 9823, 9824, 9825, 9826, 9827, 9828, 9829, 9830, 9831, 9832, 9833, 9834, 9835, 9836, 9837, 9838, 9839, 9840, 9841, 9842, 9843, 9844, 9845, 9846, 9847, 9848, 9849, 9850, 9851, 9852, 9853, 9854, 9855, 9856, 9857, 9858, 9859, 9860, 9861, 9862, 9863, 9864, 9865, 9866, 9867, 9868, 9869, 9870, 9871, 9872, 9873, 9874, 9875, 9876, 9877, 9878, 9879, 9880, 9881, 9882, 9883, 9884, 9885, 9886, 9887, 9888, 9889, 9890, 9891, 9892, 9893, 9894, 9895, 9896, 9897, 9898, 9899, 9900, 9901, 9902, 9903, 9904, 9905, 9906, 9907, 9908, 9909, 9910, 9911, 9912, 9913, 9914, 9915, 9916, 9917, 9918, 9919, 9920, 9921, 9922, 9923, 9924, 9925, 9926, 9927, 9928, 9929, 9930, 9931, 9932, 9933, 9934, 9935, 9936, 9937, 9938, 9939, 9940, 9941, 9942, 9943, 9944, 9945, 9946, 9947, 9948, 9949, 9950, 9951, 9952, 9953, 9954, 9955, 9956, 9957, 9958, 9959, 9960, 9961, 9962, 9963, 9964, 9965, 9966, 9967, 9968, 9969, 9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009, 10010, 10011, 10012, 10013, 10014, 10015, 10016, 10017, 10018, 10019, 10020, 10021, 10022, 10023, 10024, 10025, 10026, 10027, 10028, 10029, 10030, 10031, 10032, 10033, 10034, 10035, 10036, 10037, 10038, 10039, 10040, 10041, 10042, 10043, 10044, 10045, 10046, 10047, 10048, 10049, 10050, 10051, 10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059, 10060, 10061, 10062, 10063, 10064, 10065, 10066, 10067, 10068, 10069, 10070, 10071, 10072, 10073, 10074, 10075, 10076, 10077, 10078, 10079, 10080, 10081, 10082, 10083, 10084, 10085, 10086, 10087, 10088, 10089, 10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099, 10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109, 10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119, 10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129, 10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139, 10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149, 10150, 10151, 10152, 10153, 10154, 10155, 10156, 10157, 10158, 10159, 10160, 10161, 10162, 10163, 10164, 10165, 10166, 10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176, 10177, 10178, 10179, 10180, 10181, 10182, 10183, 10184, 10185, 10186, 10187, 10188, 10189, 10190, 10191, 10192, 10193, 10194, 10195, 10196, 10197, 10198, 10199, 10200, 10201, 10202, 10203, 10204, 10205, 10206, 10207, 10208, 10209, 10210, 10211, 10212, 10213, 10214, 10215, 10216, 10217, 10218, 10219, 10220, 10221, 10222, 10223, 10224, 10225, 10226, 10227, 10228, 10229, 10230, 10231, 10232, 10233, 10234, 10235, 10236, 10237, 10238, 10239, 10240, 10241, 10242, 10243, 10244, 10245, 10246, 10247, 10248, 10249, 10250, 10251, 10252, 10253, 10254, 10255, 10256, 10257, 10258, 10259, 10260, 10261, 10262, 10263, 10264, 10265, 10266, 10267, 10268, 10269, 10270, 10271, 10272, 10273, 10274, 10275, 10276, 10277, 10278, 10279, 10280, 10281, 10282, 10283, 10284, 10285, 10286, 10287, 10288, 10289, 10290, 10291, 10292, 10293, 10294, 10295, 10296, 10297, 10298, 10299, 10300, 10301, 10302, 10303, 10304, 10305, 10306, 10307, 10308, 10309, 10310, 10311, 10312, 10313, 10314, 10315, 10316, 10317, 10318, 10319, 10320, 10321, 10322, 10323, 10324, 10325, 10326, 10327, 10328, 10329, 10330, 10331, 10332, 10333, 10334, 10335, 10336, 10337, 10338, 10339, 10340, 10341, 10342, 10343, 10344, 10345, 10346, 10347, 10348, 10349, 10350, 10351, 10352, 10353, 10354, 10355, 10356, 10357, 10358, 10359, 10360, 10361, 10362, 10363, 10364, 10365, 10366, 10367, 10368, 10369, 10370, 10371, 10372, 10373, 10374, 10375, 10376, 10377, 10378, 10379, 10380, 10381, 10382, 10383, 10384, 10385, 10386, 10387, 10388, 10389, 10390, 10391, 10392, 10393, 10394, 10395, 10396, 10397, 10398, 10399, 10400, 10401, 10402, 10403, 10404, 10405, 10406, 10407, 10408, 10409, 10410, 10411, 10412, 10413, 10414, 10415, 10416, 10417, 10418, 10419, 10420, 10421, 10422, 10423, 10424, 10425, 10426, 10427, 10428, 10429, 10430, 10431, 10432, 10433, 10434, 10435, 10436, 10437, 10438, 10439, 10440, 10441, 10442, 10443, 10444, 10445, 10446, 10447, 10448, 10449, 10450, 10451, 10452, 10453, 10454, 10455, 10456, 10457, 10458, 10459, 10460, 10461, 10462, 10463, 10464, 10465, 10466, 10467, 10468, 10469, 10470, 10471, 10472, 10473, 10474, 10475, 10476, 10477, 10478, 10479, 10480, 10481, 10482, 10483, 10484, 10485, 10486, 10487, 10488, 10489, 10490, 10491, 10492, 10493, 10494, 10495, 10496, 10497, 10498, 10499, 10500, 10501, 10502, 10503, 10504, 10505, 10506, 10507, 10508, 10509, 10510, 10511, 10512, 10513, 10514, 10515, 10516, 10517, 10518, 10519, 10520, 10521, 10522, 10523, 10524, 10525, 10526, 10527, 10528, 10529, 10530, 10531, 10532, 10533, 10534, 10535, 10536, 10537, 10538, 10539, 10540, 10541, 10542, 10543, 10544, 10545, 10546, 10547, 10548, 10549, 10550, 10551, 10552, 10553, 10554, 10555, 10556, 10557, 10558, 10559, 10560, 10561, 10562, 10563, 10564, 10565, 10566, 10567, 10568, 10569, 10570, 10571, 10572, 10573, 10574, 10575, 10576, 10577, 10578, 10579, 10580, 10581, 10582, 10583, 10584, 10585, 10586, 10587, 10588, 10589, 10590, 10591, 10592, 10593, 10594, 10595, 10596, 10597, 10598, 10599, 10600, 10601, 10602, 10603, 10604, 10605, 10606, 10607, 10608, 10609, 10610, 10611, 10612, 10613, 10614, 10615, 10616, 10617, 10618, 10619, 10620, 10621, 10622, 10623, 10624, 10625, 10626, 10627, 10628, 10629, 10630, 10631, 10632, 10633, 10634, 10635, 10636, 10637, 10638, 10639, 10640, 10641, 10642, 10643, 10644, 10645, 10646, 10647, 10648, 10649, 10650, 10651, 10652, 10653, 10654, 10655, 10656, 10657, 10658, 10659, 10660, 10661, 10662, 10663, 10664, 10665, 10666, 10667, 10668, 10669, 10670, 10671, 10672, 10673, 10674, 10675, 10676, 10677, 10678, 10679, 10680, 10681, 10682, 10683, 10684, 10685, 10686, 10687, 10688, 10689, 10690, 10691, 10692, 10693, 10694, 10695, 10696, 10697, 10698, 10699, 10700, 10701, 10702, 10703, 10704, 10705, 10706, 10707, 10708, 10709, 10710, 10711, 10712, 10713, 10714, 10715, 10716, 10717, 10718, 10719, 10720, 10721, 10722, 10723, 10724, 10725, 10726, 10727, 10728, 10729, 10730, 10731, 10732, 10733, 10734, 10735, 10736, 10737, 10738, 10739, 10740, 10741, 10742, 10743, 10744, 10745, 10746, 10747, 10748, 10749, 10750, 10751, 10752, 10753, 10754, 10755, 10756, 10757, 10758, 10759, 10760, 10761, 10762, 10763, 10764, 10765, 10766, 10767, 10768, 10769, 10770, 10771, 10772, 10773, 10774, 10775, 10776, 10777, 10778, 10779, 10780, 10781, 10782, 10783, 10784, 10785, 10786, 10787, 10788, 10789, 10790, 10791, 10792, 10793, 10794, 10795, 10796, 10797, 10798, 10799, 10800, 10801, 10802, 10803, 10804, 10805, 10806, 10807, 10808, 10809, 10810, 10811, 10812, 10813, 10814, 10815, 10816, 10817, 10818, 10819, 10820, 10821, 10822, 10823, 10824, 10825, 10826, 10827, 10828, 10829, 10830, 10831, 10832, 10833, 10834, 10835, 10836, 10837, 10838, 10839, 10840, 10841, 10842, 10843, 10844, 10845, 10846, 10847, 10848, 10849, 10850, 10851, 10852, 10853, 10854, 10855, 10856, 10857, 10858, 10859, 10860, 10861, 10862, 10863, 10864, 10865, 10866, 10867, 10868, 10869, 10870, 10871, 10872, 10873, 10874, 10875, 10876, 10877, 10878, 10879, 10880, 10881, 10882, 10883, 10884, 10885, 10886, 10887, 10888, 10889, 10890, 10891, 10892, 10893, 10894, 10895, 10896, 10897, 10898, 10899, 10900, 10901, 10902, 10903, 10904, 10905, 10906, 10907, 10908, 10909, 10910, 10911, 10912, 10913, 10914, 10915, 10916, 10917, 10918, 10919, 10920, 10921, 10922, 10923, 10924, 10925, 10926, 10927, 10928, 10929, 10930, 10931, 10932, 10933, 10934, 10935, 10936, 10937, 10938, 10939, 10940, 10941, 10942, 10943, 10944, 10945, 10946, 10947, 10948, 10949, 10950, 10951, 10952, 10953, 10954, 10955, 10956, 10957, 10958, 10959, 10960, 10961, 10962, 10963, 10964, 10965, 10966, 10967, 10968, 10969, 10970, 10971, 10972, 10973, 10974, 10975, 10976, 10977, 10978, 10979, 10980, 10981, 10982, 10983, 10984, 10985, 10986, 10987, 10988, 10989, 10990, 10991, 10992, 10993, 10994, 10995, 10996, 10997, 10998, 10999, 11000, 11001, 11002, 11003, 11004, 11005, 11006, 11007, 11008, 11009, 11010, 11011, 11012, 11013, 11014, 11015, 11016, 11017, 11018, 11019, 11020, 11021, 11022, 11023, 11024, 11025, 11026, 11027, 11028, 11029, 11030, 11031, 11032, 11033, 11034, 11035, 11036, 11037, 11038, 11039, 11040, 11041, 11042, 11043, 11044, 11045, 11046, 11047, 11048, 11049, 11050, 11051, 11052, 11053, 11054, 11055, 11056, 11057, 11058, 11059, 11060, 11061, 11062, 11063, 11064, 11065, 11066, 11067, 11068, 11069, 11070, 11071, 11072, 11073, 11074, 11075, 11076, 11077, 11078, 11079, 11080, 11081, 11082, 11083, 11084, 11085, 11086, 11087, 11088, 11089, 11090, 11091, 11092, 11093, 11094, 11095, 11096, 11097, 11098, 11099, 11100, 11101, 11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110, 11111, 11112, 11113, 11114, 11115, 11116, 11117, 11118, 11119, 11120, 11121, 11122, 11123, 11124, 11125, 11126, 11127, 11128, 11129, 11130, 11131, 11132, 11133, 11134, 11135, 11136, 11137, 11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146, 11147, 11148, 11149, 11150, 11151, 11152, 11153, 11154, 11155, 11156, 11157, 11158, 11159, 11160, 11161, 11162, 11163, 11164, 11165, 11166, 11167, 11168, 11169, 11170, 11171, 11172, 11173, 11174, 11175, 11176, 11177, 11178, 11179, 11180, 11181, 11182, 11183, 11184, 11185, 11186, 11187, 11188, 11189, 11190, 11191, 11192, 11193, 11194, 11195, 11196, 11197, 11198, 11199, 11200, 11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 11237, 11238, 11239, 11240, 11241, 11242, 11243, 11244, 11245, 11246, 11247, 11248, 11249, 11250, 11251, 11252, 11253, 11254, 11255, 11256, 11257, 11258, 11259, 11260, 11261, 11262, 11263, 11264, 11265, 11266, 11267, 11268, 11269, 11270, 11271, 11272, 11273, 11274, 11275, 11276, 11277, 11278, 11279, 11280, 11281, 11282, 11283, 11284, 11285, 11286, 11287, 11288, 11289, 11290, 11291, 11292, 11293, 11294, 11295, 11296, 11297, 11298, 11299, 11300, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308, 11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317, 11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326, 11327, 11328, 11329, 11330, 11331, 11332, 11333, 11334, 11335, 11336, 11337, 11338, 11339, 11340, 11341, 11342, 11343, 11344, 11345, 11346, 11347, 11348, 11349, 11350, 11351, 11352, 11353, 11354, 11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363, 11364, 11365, 11366, 11367, 11368, 11369, 11370, 11371, 11372, 11373, 11374, 11375, 11376, 11377, 11378, 11379, 11380, 11381, 11382, 11383, 11384, 11385, 11386, 11387, 11388, 11389, 11390, 11391, 11392, 11393, 11394, 11395, 11396, 11397, 11398, 11399, 11400, 11401, 11402, 11403, 11404, 11405, 11406, 11407, 11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425, 11426, 11427, 11428, 11429, 11430, 11431, 11432, 11433, 11434, 11435, 11436, 11437, 11438, 11439, 11440, 11441, 11442, 11443, 11444, 11445, 11446, 11447, 11448, 11449, 11450, 11451, 11452, 11453, 11454, 11455, 11456, 11457, 11458, 11459, 11460, 11461, 11462, 11463, 11464, 11465, 11466, 11467, 11468, 11469, 11470, 11471, 11472, 11473, 11474, 11475, 11476, 11477, 11478, 11479, 11480, 11481, 11482, 11483, 11484, 11485, 11486, 11487, 11488, 11489, 11490, 11491, 11492, 11493, 11494, 11495, 11496, 11497, 11498, 11499, 11500, 11501, 11502, 11503, 11504, 11505, 11506, 11507, 11508, 11509, 11510, 11511, 11512, 11513, 11514, 11515, 11516, 11517, 11518, 11519, 11520, 11521, 11522, 11523, 11524, 11525, 11526, 11527, 11528, 11529, 11530, 11531, 11532, 11533, 11534, 11535, 11536, 11537, 11538, 11539, 11540, 11541, 11542, 11543, 11544, 11545, 11546, 11547, 11548, 11549, 11550, 11551, 11552, 11553, 11554, 11555, 11556, 11557, 11558, 11559, 11560, 11561, 11562, 11563, 11564, 11565, 11566, 11567, 11568, 11569, 11570, 11571, 11572, 11573, 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583, 11584, 11585, 11586, 11587, 11588, 11589, 11590, 11591, 11592, 11593, 11594, 11595, 11596, 11597, 11598, 11599, 11600, 11601, 11602, 11603, 11604, 11605, 11606, 11607, 11608, 11609, 11610, 11611, 11612, 11613, 11614, 11615, 11616, 11617, 11618, 11619, 11620, 11621, 11622, 11623, 11624, 11625, 11626, 11627, 11628, 11629, 11630, 11631, 11632, 11633, 11634, 11635, 11636, 11637, 11638, 11639, 11640, 11641, 11642, 11643, 11644, 11645, 11646, 11647, 11648, 11649, 11650, 11651, 11652, 11653, 11654, 11655, 11656, 11657, 11658, 11659, 11660, 11661, 11662, 11663, 11664, 11665, 11666, 11667, 11668, 11669, 11670, 11671, 11672, 11673, 11674, 11675, 11676, 11677, 11678, 11679, 11680, 11681, 11682, 11683, 11684, 11685, 11686, 11687, 11688, 11689, 11690, 11691, 11692, 11693, 11694, 11695, 11696, 11697, 11698, 11699, 11700, 11701, 11702, 11703, 11704, 11705, 11706, 11707, 11708, 11709, 11710, 11711, 11712, 11713, 11714, 11715, 11716, 11717, 11718, 11719, 11720, 11721, 11722, 11723, 11724, 11725, 11726, 11727, 11728, 11729, 11730, 11731, 11732, 11733, 11734, 11735, 11736, 11737, 11738, 11739, 11740, 11741, 11742, 11743, 11744, 11745, 11746, 11747, 11748, 11749, 11750, 11751, 11752, 11753, 11754, 11755, 11756, 11757, 11758, 11759, 11760, 11761, 11762, 11763, 11764, 11765, 11766, 11767, 11768, 11769, 11770, 11771, 11772, 11773, 11774, 11775, 11776, 11777, 11778, 11779, 11780, 11781, 11782, 11783, 11784, 11785, 11786, 11787, 11788, 11789, 11790, 11791, 11792, 11793, 11794, 11795, 11796, 11797, 11798, 11799, 11800, 11801, 11802, 11803, 11804, 11805, 11806, 11807, 11808, 11809, 11810, 11811, 11812, 11813, 11814, 11815, 11816, 11817, 11818, 11819, 11820, 11821, 11822, 11823, 11824, 11825, 11826, 11827, 11828, 11829, 11830, 11831, 11832, 11833, 11834, 11835, 11836, 11837, 11838, 11839, 11840, 11841, 11842, 11843, 11844, 11845, 11846, 11847, 11848, 11849, 11850, 11851, 11852, 11853, 11854, 11855, 11856, 11857, 11858, 11859, 11860, 11861, 11862, 11863, 11864, 11865, 11866, 11867, 11868, 11869, 11870, 11871, 11872, 11873, 11874, 11875, 11876, 11877, 11878, 11879, 11880, 11881, 11882, 11883, 11884, 11885, 11886, 11887, 11888, 11889, 11890, 11891, 11892, 11893, 11894, 11895, 11896, 11897, 11898, 11899, 11900, 11901, 11902, 11903, 11904, 11905, 11906, 11907, 11908, 11909, 11910, 11911, 11912, 11913, 11914, 11915, 11916, 11917, 11918, 11919, 11920, 11921, 11922, 11923, 11924, 11925, 11926, 11927, 11928, 11929, 11930, 11931, 11932, 11933, 11934, 11935, 11936, 11937, 11938, 11939, 11940, 11941, 11942, 11943, 11944, 11945, 11946, 11947, 11948, 11949, 11950, 11951, 11952, 11953, 11954, 11955, 11956, 11957, 11958, 11959, 11960, 11961, 11962, 11963, 11964, 11965, 11966, 11967, 11968, 11969, 11970, 11971, 11972, 11973, 11974, 11975, 11976, 11977, 11978, 11979, 11980, 11981, 11982, 11983, 11984, 11985, 11986, 11987, 11988, 11989, 11990, 11991, 11992, 11993, 11994, 11995, 11996, 11997, 11998, 11999, 12000, 12001, 12002, 12003, 12004, 12005, 12006, 12007, 12008, 12009, 12010, 12011, 12012, 12013, 12014, 12015, 12016, 12017, 12018, 12019, 12020, 12021, 12022, 12023, 12024, 12025, 12026, 12027, 12028, 12029, 12030, 12031, 12032, 12033, 12034, 12035, 12036, 12037, 12038, 12039, 12040, 12041, 12042, 12043, 12044, 12045, 12046, 12047, 12048, 12049, 12050, 12051, 12052, 12053, 12054, 12055, 12056, 12057, 12058, 12059, 12060, 12061, 12062, 12063, 12064, 12065, 12066, 12067, 12068, 12069, 12070, 12071, 12072, 12073, 12074, 12075, 12076, 12077, 12078, 12079, 12080, 12081, 12082, 12083, 12084, 12085, 12086, 12087, 12088, 12089, 12090, 12091, 12092, 12093, 12094, 12095, 12096, 12097, 12098, 12099, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397, 20398, 20399, 20400, 20401, 20402, 20403, 20404, 20405, 20406, 20407, 20408, 20409, 20410, 20411, 20412, 20413, 20414, 20415, 20416, 20417, 20418, 20419, 20420, 20421, 20422, 20423, 20424, 20425, 20426, 20427, 20428, 20429, 20430, 20431, 20432, 20433, 20434, 20435, 20436, 20437, 20438, 20439, 20440, 20441, 20442, 20443, 20444, 20445, 20446, 20447, 20448, 20449, 20450, 20451, 20452, 20453, 20454, 20455, 20456, 20457, 20458, 20459, 20460, 20461, 20462, 20463, 20464, 20465, 20466, 20467, 20468, 20469, 20470, 20471, 20472, 20473, 20474, 20475, 20476, 20477, 20478, 20479, 20480, 20481, 20482, 20483, 20484, 20485, 20486, 20487, 20488, 20489, 20490, 20491, 20492, 20493, 20494, 20495, 20496, 20497, 20498, 20499, 20500, 20501, 20502, 20503, 20504, 20505, 20506, 20507, 20508, 20509, 20510, 20511, 20512, 20513, 20514, 20515, 20516, 20517, 20518, 20519, 20520, 20521, 20522, 20523, 20524, 20525, 20526, 20527, 20528, 20529, 20530, 20531, 20532, 20533, 20534, 20535, 20536, 20537, 20538, 20539, 20540, 20541, 20542, 20543, 20544, 20545, 20546, 20547, 20548, 20549, 20550, 20551, 20552, 20553, 20554, 20555, 20556, 20557, 20558, 20559, 20560, 20561, 20562, 20563, 20564, 20565, 20566, 20567, 20568, 20569, 20570, 20571, 20572, 20573, 20574, 20575, 20576, 20577, 20578, 20579, 20580, 20581, 20582, 20583, 20584, 20585, 20586, 20587, 20588, 20589, 20590, 20591, 20592, 20593, 20594, 20595, 20596, 20597, 20598, 20599, 20600, 20601, 20602, 20603, 20604, 20605, 20606, 20607, 20608, 20609, 20610, 20611, 20612, 20613, 20614, 20615, 20616, 20617, 20618, 20619, 20620, 20621, 20622, 20623, 20624, 20625, 20626, 20627, 20628, 20629, 20630, 20631, 20632, 20633, 20634, 20635, 20636, 20637, 20638, 20639, 20640, 20641, 20642, 20643, 20644, 20645, 20646, 20647, 20648, 20649, 20650, 20651, 20652, 20653, 20654, 20655, 20656, 20657, 20658, 20659, 20660, 20661, 20662, 20663, 20664, 20665, 20666, 20667, 20668, 20669, 20670, 20671, 20672, 20673, 20674, 20675, 20676, 20677, 20678, 20679, 20680, 20681, 20682, 20683, 20684, 20685, 20686, 20687, 20688, 20689, 20690, 20691, 20692, 20693, 20694, 20695, 20696, 20697, 20698, 20699, 20700, 20701, 20702, 20703, 20704, 20705, 20706, 20707, 20708, 20709, 20710, 20711, 20712, 20713, 20714, 20715, 20716, 20717, 20718, 20719, 20720, 20721, 20722, 20723, 20724, 20725, 20726, 20727, 20728, 20729, 20730, 20731, 20732, 20733, 20734, 20735, 20736, 20737, 20738, 20739, 20740, 20741, 20742, 20743, 20744, 20745, 20746, 20747, 20748, 20749, 20750, 20751, 20752, 20753, 20754, 20755, 20756, 20757, 20758, 20759, 20760, 20761, 20762, 20763, 20764, 20765, 20766, 20767, 20768, 20769, 20770, 20771, 20772, 20773, 20774, 20775, 20776, 20777, 20778, 20779, 20780, 20781, 20782, 20783, 20784, 20785, 20786, 20787, 20788, 20789, 20790, 20791, 20792, 20793, 20794, 20795, 20796, 20797, 20798, 20799, 20800, 20801, 20802, 20803, 20804, 20805, 20806, 20807, 20808, 20809, 20810, 20811, 20812, 20813, 20814, 20815, 20816, 20817, 20818, 20819, 20820, 20821, 20822, 20823, 20824, 20825, 20826, 20827, 20828, 20829, 20830, 20831, 20832, 20833, 20834, 20835, 20836, 20837, 20838, 20839, 20840, 20841, 20842, 20843, 20844, 20845, 20846, 20847, 20848, 20849, 20850, 20851, 20852, 20853, 20854, 20855, 20856, 20857, 20858, 20859, 20860, 20861, 20862, 20863, 20864, 20865, 20866, 20867, 20868, 20869, 20870, 20871, 20872, 20873, 20874, 20875, 20876, 20877, 20878, 20879, 20880, 20881, 20882, 20883, 20884, 20885, 20886, 20887, 20888, 20889, 20890, 20891, 20892, 20893, 20894, 20895, 20896, 20897, 20898, 20899, 20900, 20901, 20902, 20903, 20904, 20905, 20906, 20907, 20908, 20909, 20910, 20911, 20912, 20913, 20914, 20915, 20916, 20917, 20918, 20919, 20920, 20921, 20922, 20923, 20924, 20925, 20926, 20927, 20928, 20929, 20930, 20931, 20932, 20933, 20934, 20935, 20936, 20937, 20938, 20939, 20940, 20941, 20942, 20943, 20944, 20945, 20946, 20947, 20948, 20949, 20950, 20951, 20952, 20953, 20954, 20955, 20956, 20957, 20958, 20959, 20960, 20961, 20962, 20963, 20964, 20965, 20966, 20967, 20968, 20969, 20970, 20971, 20972, 20973, 20974, 20975, 20976, 20977, 20978, 20979, 20980, 20981, 20982, 20983, 20984, 20985, 20986, 20987, 20988, 20989, 20990, 20991, 20992, 20993, 20994, 20995, 20996, 20997, 20998, 20999, 21000, 21001, 21002, 21003, 21004, 21005, 21006, 21007, 21008, 21009, 21010, 21011, 21012, 21013, 21014, 21015, 21016, 21017, 21018, 21019, 21020, 21021, 21022, 21023, 21024, 21025, 21026, 21027, 21028, 21029, 21030, 21031, 21032, 21033, 21034, 21035, 21036, 21037, 21038, 21039, 21040, 21041, 21042, 21043, 21044, 21045, 21046, 21047, 21048, 21049, 21050, 21051, 21052, 21053, 21054, 21055, 21056, 21057, 21058, 21059, 21060, 21061, 21062, 21063, 21064, 21065, 21066, 21067, 21068, 21069, 21070, 21071, 21072, 21073, 21074, 21075, 21076, 21077, 21078, 21079, 21080, 21081, 21082, 21083, 21084, 21085, 21086, 21087, 21088, 21089, 21090, 21091, 21092, 21093, 21094, 21095, 21096, 21097, 21098, 21099, 21100, 21101, 21102, 21103, 21104, 21105, 21106, 21107, 21108, 21109, 21110, 21111, 21112, 21113, 21114, 21115, 21116, 21117, 21118, 21119, 21120, 21121, 21122, 21123, 21124, 21125, 21126, 21127, 21128, 21129, 21130, 21131, 21132, 21133, 21134, 21135, 21136, 21137, 21138, 21139, 21140, 21141, 21142, 21143, 21144, 21145, 21146, 21147, 21148, 21149, 21150, 21151, 21152, 21153, 21154, 21155, 21156, 21157, 21158, 21159, 21160, 21161, 21162, 21163, 21164, 21165, 21166, 21167, 21168, 21169, 21170, 21171, 21172, 21173, 21174, 21175, 21176, 21177, 21178, 21179, 21180, 21181, 21182, 21183, 21184, 21185, 21186, 21187, 21188, 21189, 21190, 21191, 21192, 21193, 21194, 21195, 21196, 21197, 21198, 21199, 21200, 21201, 21202, 21203, 21204, 21205, 21206, 21207, 21208, 21209, 21210, 21211, 21212, 21213, 21214, 21215, 21216, 21217, 21218, 21219, 21220, 21221, 21222, 21223, 21224, 21225, 21226, 21227, 21228, 21229, 21230, 21231, 21232, 21233, 21234, 21235, 21236, 21237, 21238, 21239, 21240, 21241, 21242, 21243, 21244, 21245, 21246, 21247, 21248, 21249, 21250, 21251, 21252, 21253, 21254, 21255, 21256, 21257, 21258, 21259, 21260, 21261, 21262, 21263, 21264, 21265, 21266, 21267, 21268, 21269, 21270, 21271, 21272, 21273, 21274, 21275, 21276, 21277, 21278, 21279, 21280, 21281, 21282, 21283, 21284, 21285, 21286, 21287, 21288, 21289, 21290, 21291, 21292, 21293, 21294, 21295, 21296, 21297, 21298, 21299, 21300, 21301, 21302, 21303, 21304, 21305, 21306, 21307, 21308, 21309, 21310, 21311, 21312, 21313, 21314, 21315, 21316, 21317, 21318, 21319, 21320, 21321, 21322, 21323, 21324, 21325, 21326, 21327, 21328, 21329, 21330, 21331, 21332, 21333, 21334, 21335, 21336, 21337, 21338, 21339, 21340, 21341, 21342, 21343, 21344, 21345, 21346, 21347, 21348, 21349, 21350, 21351, 21352, 21353, 21354, 21355, 21356, 21357, 21358, 21359, 21360, 21361, 21362, 21363, 21364, 21365, 21366, 21367, 21368, 21369, 21370, 21371, 21372, 21373, 21374, 21375, 21376, 21377, 21378, 21379, 21380, 21381, 21382, 21383, 21384, 21385, 21386, 21387, 21388, 21389, 21390, 21391, 21392, 21393, 21394, 21395, 21396, 21397, 21398, 21399, 21400, 21401, 21402, 21403, 21404, 21405, 21406, 21407, 21408, 21409, 21410, 21411, 21412, 21413, 21414, 21415, 21416, 21417, 21418, 21419, 21420, 21421, 21422, 21423, 21424, 21425, 21426, 21427, 21428, 21429, 21430, 21431, 21432, 21433, 21434, 21435, 21436, 21437, 21438, 21439, 21440, 21441, 21442, 21443, 21444, 21445, 21446, 21447, 21448, 21449, 21450, 21451, 21452, 21453, 21454, 21455, 21456, 21457, 21458, 21459, 21460, 21461, 21462, 21463, 21464, 21465, 21466, 21467, 21468, 21469, 21470, 21471, 21472, 21473, 21474, 21475, 21476, 21477, 21478, 21479, 21480, 21481, 21482, 21483, 21484, 21485, 21486, 21487, 21488, 21489, 21490, 21491, 21492, 21493, 21494, 21495, 21496, 21497, 21498, 21499, 21500, 21501, 21502, 21503, 21504, 21505, 21506, 21507, 21508, 21509, 21510, 21511, 21512, 21513, 21514, 21515, 21516, 21517, 21518, 21519, 21520, 21521, 21522, 21523, 21524, 21525, 21526, 21527, 21528, 21529, 21530, 21531, 21532, 21533, 21534, 21535, 21536, 21537, 21538, 21539, 21540, 21541, 21542, 21543, 21544, 21545, 21546, 21547, 21548, 21549, 21550, 21551, 21552, 21553, 21554, 21555, 21556, 21557, 21558, 21559, 21560, 21561, 21562, 21563, 21564, 21565, 21566, 21567, 21568, 21569, 21570, 21571, 21572, 21573, 21574, 21575, 21576, 21577, 21578, 21579, 21580, 21581, 21582, 21583, 21584, 21585, 21586, 21587, 21588, 21589, 21590, 21591, 21592, 21593, 21594, 21595, 21596, 21597, 21598, 21599, 21600, 21601, 21602, 21603, 21604, 21605, 21606, 21607, 21608, 21609, 21610, 21611, 21612, 21613, 21614, 21615, 21616, 21617, 21618, 21619, 21620, 21621, 21622, 21623, 21624, 21625, 21626, 21627, 21628, 21629, 21630, 21631, 21632, 21633, 21634, 21635, 21636, 21637, 21638, 21639, 21640, 21641, 21642, 21643, 21644, 21645, 21646, 21647, 21648, 21649, 21650, 21651, 21652, 21653, 21654, 21655, 21656, 21657, 21658, 21659, 21660, 21661, 21662, 21663, 21664, 21665, 21666, 21667, 21668, 21669, 21670, 21671, 21672, 21673, 21674, 21675, 21676, 21677, 21678, 21679, 21680, 21681, 21682, 21683, 21684, 21685, 21686, 21687, 21688, 21689, 21690, 21691, 21692, 21693, 21694, 21695, 21696, 21697, 21698, 21699, 21700, 21701, 21702, 21703, 21704, 21705, 21706, 21707, 21708, 21709, 21710, 21711, 21712, 21713, 21714, 21715, 21716, 21717, 21718, 21719, 21720, 21721, 21722, 21723, 21724, 21725, 21726, 21727, 21728, 21729, 21730, 21731, 21732, 21733, 21734, 21735, 21736, 21737, 21738, 21739, 21740, 21741, 21742, 21743, 21744, 21745, 21746, 21747, 21748, 21749, 21750, 21751, 21752, 21753, 21754, 21755, 21756, 21757, 21758, 21759, 21760, 21761, 21762, 21763, 21764, 21765, 21766, 21767, 21768, 21769, 21770, 21771, 21772, 21773, 21774, 21775, 21776, 21777, 21778, 21779, 21780, 21781, 21782, 21783, 21784, 21785, 21786, 21787, 21788, 21789, 21790, 21791, 21792, 21793, 21794, 21795, 21796, 21797, 21798, 21799, 21800, 21801, 21802, 21803, 21804, 21805, 21806, 21807, 21808, 21809, 21810, 21811, 21812, 21813, 21814, 21815, 21816, 21817, 21818, 21819, 21820, 21821, 21822, 21823, 21824, 21825, 21826, 21827, 21828, 21829, 21830, 21831, 21832, 21833, 21834, 21835, 21836, 21837, 21838, 21839, 21840, 21841, 21842, 21843, 21844, 21845, 21846, 21847, 21848, 21849, 21850, 21851, 21852, 21853, 21854, 21855, 21856, 21857, 21858, 21859, 21860, 21861, 21862, 21863, 21864, 21865, 21866, 21867, 21868, 21869, 21870, 21871, 21872, 21873, 21874, 21875, 21876, 21877, 21878, 21879, 21880, 21881, 21882, 21883, 21884, 21885, 21886, 21887, 21888, 21889, 21890, 21891, 21892, 21893, 21894, 21895, 21896, 21897, 21898, 21899, 21900, 21901, 21902, 21903, 21904, 21905, 21906, 21907, 21908, 21909, 21910, 21911, 21912, 21913, 21914, 21915, 21916, 21917, 21918, 21919, 21920, 21921, 21922, 21923, 21924, 21925, 21926, 21927, 21928, 21929, 21930, 21931, 21932, 21933, 21934, 21935, 21936, 21937, 21938, 21939, 21940, 21941, 21942, 21943, 21944, 21945, 21946, 21947, 21948, 21949, 21950, 21951, 21952, 21953, 21954, 21955, 21956, 21957, 21958, 21959, 21960, 21961, 21962, 21963, 21964, 21965, 21966, 21967, 21968, 21969, 21970, 21971, 21972, 21973, 21974, 21975, 21976, 21977, 21978, 21979, 21980, 21981, 21982, 21983, 21984, 21985, 21986, 21987, 21988, 21989, 21990, 21991, 21992, 21993, 21994, 21995, 21996, 21997, 21998, 21999, 22000, 22001, 22002, 22003, 22004, 22005, 22006, 22007, 22008, 22009, 22010, 22011, 22012, 22013, 22014, 22015, 22016, 22017, 22018, 22019, 22020, 22021, 22022, 22023, 22024, 22025, 22026, 22027, 22028, 22029, 22030, 22031, 22032, 22033, 22034, 22035, 22036, 22037, 22038, 22039, 22040, 22041, 22042, 22043, 22044, 22045, 22046, 22047, 22048, 22049, 22050, 22051, 22052, 22053, 22054, 22055, 22056, 22057, 22058, 22059, 22060, 22061, 22062, 22063, 22064, 22065, 22066, 22067, 22068, 22069, 22070, 22071, 22072, 22073, 22074, 22075, 22076, 22077, 22078, 22079, 22080, 22081, 22082, 22083, 22084, 22085, 22086, 22087, 22088, 22089, 22090, 22091, 22092, 22093, 22094, 22095, 22096, 22097, 22098, 22099, 22100, 22101, 22102, 22103, 22104, 22105, 22106, 22107, 22108, 22109, 22110, 22111, 22112, 22113, 22114, 22115, 22116, 22117, 22118, 22119, 22120, 22121, 22122, 22123, 22124, 22125, 22126, 22127, 22128, 22129, 22130, 22131, 22132, 22133, 22134, 22135, 22136, 22137, 22138, 22139, 22140, 22141, 22142, 22143, 22144, 22145, 22146, 22147, 22148, 22149, 22150, 22151, 22152, 22153, 22154, 22155, 22156, 22157, 22158, 22159, 22160, 22161, 22162, 22163, 22164, 22165, 22166, 22167, 22168, 22169, 22170, 22171, 22172, 22173, 22174, 22175, 22176, 22177, 22178, 22179, 22180, 22181, 22182, 22183, 22184, 22185, 22186, 22187, 22188, 22189, 22190, 22191, 22192, 22193, 22194, 22195, 22196, 22197, 22198, 22199, 22200, 22201, 22202, 22203, 22204, 22205, 22206, 22207, 22208, 22209, 22210, 22211, 22212, 22213, 22214, 22215, 22216, 22217, 22218, 22219, 22220, 22221, 22222, 22223, 22224, 22225, 22226, 22227, 22228, 22229, 22230, 22231, 22232, 22233, 22234, 22235, 22236, 22237, 22238, 22239, 22240, 22241, 22242, 22243, 22244, 22245, 22246, 22247, 22248, 22249, 22250, 22251, 22252, 22253, 22254, 22255, 22256, 22257, 22258, 22259, 22260, 22261, 22262, 22263, 22264, 22265, 22266, 22267, 22268, 22269, 22270, 22271, 22272, 22273, 22274, 22275, 22276, 22277, 22278, 22279, 22280, 22281, 22282, 22283, 22284, 22285, 22286, 22287, 22288, 22289, 22290, 22291, 22292, 22293, 22294, 22295, 22296, 22297, 22298, 22299, 22300, 22301, 22302, 22303, 22304, 22305, 22306, 22307, 22308, 22309, 22310, 22311, 22312, 22313, 22314, 22315, 22316, 22317, 22318, 22319, 22320, 22321, 22322, 22323, 22324, 22325, 22326, 22327, 22328, 22329, 22330, 22331, 22332, 22333, 22334, 22335, 22336, 22337, 22338, 22339, 22340, 22341, 22342, 22343, 22344, 22345, 22346, 22347, 22348, 22349, 22350, 22351, 22352, 22353, 22354, 22355, 22356, 22357, 22358, 22359, 22360, 22361, 22362, 22363, 22364, 22365, 22366, 22367, 22368, 22369, 22370, 22371, 22372, 22373, 22374, 22375, 22376, 22377, 22378, 22379, 22380, 22381, 22382, 22383, 22384, 22385, 22386, 22387, 22388, 22389, 22390, 22391, 22392, 22393, 22394, 22395, 22396, 22397, 22398, 22399, 22400, 22401, 22402, 22403, 22404, 22405, 22406, 22407, 22408, 22409, 22410, 22411, 22412, 22413, 22414, 22415, 22416, 22417, 22418, 22419, 22420, 22421, 22422, 22423, 22424, 22425, 22426, 22427, 22428, 22429, 22430, 22431, 22432, 22433, 22434, 22435, 22436, 22437, 22438, 22439, 22440, 22441, 22442, 22443, 22444, 22445, 22446, 22447, 22448, 22449, 22450, 22451, 22452, 22453, 22454, 22455, 22456, 22457, 22458, 22459, 22460, 22461, 22462, 22463, 22464, 22465, 22466, 22467, 22468, 22469, 22470, 22471, 22472, 22473, 22474, 22475, 22476, 22477, 22478, 22479, 22480, 22481, 22482, 22483, 22484, 22485, 22486, 22487, 22488, 22489, 22490, 22491, 22492, 22493, 22494, 22495, 22496, 22497, 22498, 22499, 22500, 22501, 22502, 22503, 22504, 22505, 22506, 22507, 22508, 22509, 22510, 22511, 22512, 22513, 22514, 22515, 22516, 22517, 22518, 22519, 22520, 22521, 22522, 22523, 22524, 22525, 22526, 22527, 22528, 22529, 22530, 22531, 22532, 22533, 22534, 22535, 22536, 22537, 22538, 22539, 22540, 22541, 22542, 22543, 22544, 22545, 22546, 22547, 22548, 22549, 22550, 22551, 22552, 22553, 22554, 22555, 22556, 22557, 22558, 22559, 22560, 22561, 22562, 22563, 22564, 22565, 22566, 22567, 22568, 22569, 22570, 22571, 22572, 22573, 22574, 22575, 22576, 22577, 22578, 22579, 22580, 22581, 22582, 22583, 22584, 22585, 22586, 22587, 22588, 22589, 22590, 22591, 22592, 22593, 22594, 22595, 22596, 22597, 22598, 22599, 22600, 22601, 22602, 22603, 22604, 22605, 22606, 22607, 22608, 22609, 22610, 22611, 22612, 22613, 22614, 22615, 22616, 22617, 22618, 22619, 22620, 22621, 22622, 22623, 22624, 22625, 22626, 22627, 22628, 22629, 22630, 22631, 22632, 22633, 22634, 22635, 22636, 22637, 22638, 22639, 22640, 22641, 22642, 22643, 22644, 22645, 22646, 22647, 22648, 22649, 22650, 22651, 22652, 22653, 22654, 22655, 22656, 22657, 22658, 22659, 22660, 22661, 22662, 22663, 22664, 22665, 22666, 22667, 22668, 22669, 22670, 22671, 22672, 22673, 22674, 22675, 22676, 22677, 22678, 22679, 22680, 22681, 22682, 22683, 22684, 22685, 22686, 22687, 22688, 22689, 22690, 22691, 22692, 22693, 22694, 22695, 22696, 22697, 22698, 22699, 22700, 22701, 22702, 22703, 22704, 22705, 22706, 22707, 22708, 22709, 22710, 22711, 22712, 22713, 22714, 22715, 22716, 22717, 22718, 22719, 22720, 22721, 22722, 22723, 22724, 22725, 22726, 22727, 22728, 22729, 22730, 22731, 22732, 22733, 22734, 22735, 22736, 22737, 22738, 22739, 22740, 22741, 22742, 22743, 22744, 22745, 22746, 22747, 22748, 22749, 22750, 22751, 22752, 22753, 22754, 22755, 22756, 22757, 22758, 22759, 22760, 22761, 22762, 22763, 22764, 22765, 22766, 22767, 22768, 22769, 22770, 22771, 22772, 22773, 22774, 22775, 22776, 22777, 22778, 22779, 22780, 22781, 22782, 22783, 22784, 22785, 22786, 22787, 22788, 22789, 22790, 22791, 22792, 22793, 22794, 22795, 22796, 22797, 22798, 22799, 22800, 22801, 22802, 22803, 22804, 22805, 22806, 22807, 22808, 22809, 22810, 22811, 22812, 22813, 22814, 22815, 22816, 22817, 22818, 22819, 22820, 22821, 22822, 22823, 22824, 22825, 22826, 22827, 22828, 22829, 22830, 22831, 22832, 22833, 22834, 22835, 22836, 22837, 22838, 22839, 22840, 22841, 22842, 22843, 22844, 22845, 22846, 22847, 22848, 22849, 22850, 22851, 22852, 22853, 22854, 22855, 22856, 22857, 22858, 22859, 22860, 22861, 22862, 22863, 22864, 22865, 22866, 22867, 22868, 22869, 22870, 22871, 22872, 22873, 22874, 22875, 22876, 22877, 22878, 22879, 22880, 22881, 22882, 22883, 22884, 22885, 22886, 22887, 22888, 22889, 22890, 22891, 22892, 22893, 22894, 22895, 22896, 22897, 22898, 22899, 22900, 22901, 22902, 22903, 22904, 22905, 22906, 22907, 22908, 22909, 22910, 22911, 22912, 22913, 22914, 22915, 22916, 22917, 22918, 22919, 22920, 22921, 22922, 22923, 22924, 22925, 22926, 22927, 22928, 22929, 22930, 22931, 22932, 22933, 22934, 22935, 22936, 22937, 22938, 22939, 22940, 22941, 22942, 22943, 22944, 22945, 22946, 22947, 22948, 22949, 22950, 22951, 22952, 22953, 22954, 22955, 22956, 22957, 22958, 22959, 22960, 22961, 22962, 22963, 22964, 22965, 22966, 22967, 22968, 22969, 22970, 22971, 22972, 22973, 22974, 22975, 22976, 22977, 22978, 22979, 22980, 22981, 22982, 22983, 22984, 22985, 22986, 22987, 22988, 22989, 22990, 22991, 22992, 22993, 22994, 22995, 22996, 22997, 22998, 22999, 23000, 23001, 23002, 23003, 23004, 23005, 23006, 23007, 23008, 23009, 23010, 23011, 23012, 23013, 23014, 23015, 23016, 23017, 23018, 23019, 23020, 23021, 23022, 23023, 23024, 23025, 23026, 23027, 23028, 23029, 23030, 23031, 23032, 23033, 23034, 23035, 23036, 23037, 23038, 23039, 23040, 23041, 23042, 23043, 23044, 23045, 23046, 23047, 23048, 23049, 23050, 23051, 23052, 23053, 23054, 23055, 23056, 23057, 23058, 23059, 23060, 23061, 23062, 23063, 23064, 23065, 23066, 23067, 23068, 23069, 23070, 23071, 23072, 23073, 23074, 23075, 23076, 23077, 23078, 23079, 23080, 23081, 23082, 23083, 23084, 23085, 23086, 23087, 23088, 23089, 23090, 23091, 23092, 23093, 23094, 23095, 23096, 23097, 23098, 23099, 23100, 23101, 23102, 23103, 23104, 23105, 23106, 23107, 23108, 23109, 23110, 23111, 23112, 23113, 23114, 23115, 23116, 23117, 23118, 23119, 23120, 23121, 23122, 23123, 23124, 23125, 23126, 23127, 23128, 23129, 23130, 23131, 23132, 23133, 23134, 23135, 23136, 23137, 23138, 23139, 23140, 23141, 23142, 23143, 23144, 23145, 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23153, 23154, 23155, 23156, 23157, 23158, 23159, 23160, 23161, 23162, 23163, 23164, 23165, 23166, 23167, 23168, 23169, 23170, 23171, 23172, 23173, 23174, 23175, 23176, 23177, 23178, 23179, 23180, 23181, 23182, 23183, 23184, 23185, 23186, 23187, 23188, 23189, 23190, 23191, 23192, 23193, 23194, 23195, 23196, 23197, 23198, 23199, 23200, 23201, 23202, 23203, 23204, 23205, 23206, 23207, 23208, 23209, 23210, 23211, 23212, 23213, 23214, 23215, 23216, 23217, 23218, 23219, 23220, 23221, 23222, 23223, 23224, 23225, 23226, 23227, 23228, 23229, 23230, 23231, 23232, 23233, 23234, 23235, 23236, 23237, 23238, 23239, 23240, 23241, 23242, 23243, 23244, 23245, 23246, 23247, 23248, 23249, 23250, 23251, 23252, 23253, 23254, 23255, 23256, 23257, 23258, 23259, 23260, 23261, 23262, 23263, 23264, 23265, 23266, 23267, 23268, 23269, 23270, 23271, 23272, 23273, 23274, 23275, 23276, 23277, 23278, 23279, 23280, 23281, 23282, 23283, 23284, 23285, 23286, 23287, 23288, 23289, 23290, 23291, 23292, 23293, 23294, 23295, 23296, 23297, 23298, 23299, 23300, 23301, 23302, 23303, 23304, 23305, 23306, 23307, 23308, 23309, 23310, 23311, 23312, 23313, 23314, 23315, 23316, 23317, 23318, 23319, 23320, 23321, 23322, 23323, 23324, 23325, 23326, 23327, 23328, 23329, 23330, 23331, 23332, 23333, 23334, 23335, 23336, 23337, 23338, 23339, 23340, 23341, 23342, 23343, 23344, 23345, 23346, 23347, 23348, 23349, 23350, 23351, 23352, 23353, 23354, 23355, 23356, 23357, 23358, 23359, 23360, 23361, 23362, 23363, 23364, 23365, 23366, 23367, 23368, 23369, 23370))\n\n\n\n\nCode\nsubset = list(range(1,50))\n\n\n\n\nCode\nsubset_graph = T.subgraph(subset)\n\n\n\n\nCode\nprint(subset_graph.nodes())\n\n\n[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n\n\n\n\nCode\nprint(subset_graph.edges())\n\n\n[(1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (1, 32), (1, 33), (1, 34), (1, 35), (1, 36), (1, 37), (1, 38), (1, 39), (1, 40), (1, 41), (1, 42), (1, 43), (1, 44), (1, 45), (1, 46), (1, 47), (1, 48), (1, 49), (16, 18), (16, 35), (16, 36), (16, 48), (18, 16), (18, 24), (18, 35), (18, 36), (19, 5), (19, 8), (19, 11), (19, 13), (19, 15), (19, 17), (19, 20), (19, 21), (19, 24), (19, 30), (19, 31), (19, 35), (19, 36), (19, 37), (19, 48), (28, 1), (28, 5), (28, 7), (28, 8), (28, 11), (28, 14), (28, 15), (28, 17), (28, 20), (28, 21), (28, 24), (28, 25), (28, 27), (28, 29), (28, 30), (28, 31), (28, 35), (28, 36), (28, 37), (28, 44), (28, 48), (28, 49), (36, 5), (36, 24), (36, 35), (36, 37), (37, 24), (37, 35), (37, 36), (39, 1), (39, 24), (39, 33), (39, 35), (39, 36), (39, 38), (39, 40), (39, 41), (39, 45), (42, 1), (43, 24), (43, 29), (43, 35), (43, 36), (43, 37), (43, 47), (43, 48), (45, 1), (45, 39), (45, 41)]\n\n\n\n\nCode\nnx.draw(subset_graph)\nplt.show()\n\n\n\n\n\n\n\n\nNow that we know some basic graph properties and have practiced using NetworkX’s drawing tools, it’s time to see how we can query it for nodes and edges. We’re looking for “nodes of interest” and “edges of interest”.\n\n\nCode\nfrom datetime import date\n\n\n\n\nCode\n# nodes of intereset\nnoi = [n for n, d in T.nodes(data=True) if d['occupation'] == 'scientist']\n\n\n\n\nCode\n# edges of intereset\neoi = [(u,v) for u, v, d in T.edges(data=True) if d['date'] < date(2010,1,1)]\n\n\n\n\nCode\nnoi [:3]\n\n\n[5, 9, 13]\n\n\n\n\nCode\neoi [:3]\n\n\n[(1, 5), (1, 9), (1, 13)]"
  },
  {
    "objectID": "posts/Introduction to networks in network analysis/Introduction to networks.html#specifying-weight-on-edges",
    "href": "posts/Introduction to networks in network analysis/Introduction to networks.html#specifying-weight-on-edges",
    "title": "Introduction to networks",
    "section": "specifying weight on edges",
    "text": "specifying weight on edges\nThe strength of an edge is indicated by the weight of its edges in a graph. NetworkX’s metadata dictionary has a ‘weight’ key that indicates weight\n\n\nCode\nT.edges[1, 10]\n\n\n{'date': datetime.date(2012, 9, 8)}\n\n\n\n\nCode\n# Set the weight of the edge\nT.edges[1, 10]['weight'] = 2\n\n# Iterate over all the edges (with metadata)\nfor u, v, d in T.edges(data=True):\n\n    # Check if node 293 is involved\n    if 293 in [u, v]:\n\n        # Set the weight to 1.1\n        T.edges[u, v]['weight'] = 1.1\n\n\n\n\nCode\nT.edges[1, 10]\n\n\n{'date': datetime.date(2012, 9, 8), 'weight': 2}"
  },
  {
    "objectID": "posts/Introduction to networks in network analysis/Introduction to networks.html#checking-if-there-are-self-loops-in-graph",
    "href": "posts/Introduction to networks in network analysis/Introduction to networks.html#checking-if-there-are-self-loops-in-graph",
    "title": "Introduction to networks",
    "section": "Checking if there are self-loops in graph",
    "text": "Checking if there are self-loops in graph\nIn addition to edges beginning and ending on the same node, NetworkX lets you model data such as trip networks, where individuals begin at one place and end at another.\nWe’ll use assert quite a bit. It throws an AssertionError if the statement after it does not evaluate to True.\nUse the number_of_selfloops() method on T to get the number of edges that begin and end on the same node. The graph has been synthetically enhanced with self-loops. Write a function that returns these edges in this exercise.\n\n\nCode\nT.number_of_selfloops()\n\n\nAttributeError: 'DiGraph' object has no attribute 'number_of_selfloops'\n\n\n\n\nCode\ndef find_selfloop_nodes(G):\n    \"\"\"\n    Finds all nodes that have self-loops in the graph G.\n    \"\"\"\n    nodes_in_selfloops = []\n\n    # Iterate over all the edges of G\n    for u, v in G.edges():\n\n    # Check if node u and node v are the same\n        if u == v:\n\n            # Append node u to nodes_in_selfloops\n            nodes_in_selfloops.append(u)\n\n    return nodes_in_selfloops\n\n# Check whether number of self loops equals the number of nodes in self loops\nassert T.number_of_selfloops() == len(find_selfloop_nodes(T))\n\n\nAttributeError: 'DiGraph' object has no attribute 'number_of_selfloops'"
  },
  {
    "objectID": "posts/Introduction to networks in network analysis/Introduction to networks.html#visualizing-using-matrix-plots",
    "href": "posts/Introduction to networks in network analysis/Introduction to networks.html#visualizing-using-matrix-plots",
    "title": "Introduction to networks",
    "section": "Visualizing using Matrix plots",
    "text": "Visualizing using Matrix plots\nTime to try your first “fancy” graph visualization method: a matrix plot. Nxviz provides a MatrixPlot object for this.\nNxviz is a package that visualizes graphs rationally. MatrixPlot uses nx.to_numpy_matrix(G), which returns the matrix form of the graph. Every node has one column and one row, and an edge between two nodes is indicated by 1. By doing so, only the weight metadata is preserved; all other metadata is lost, as you’ll see in an assert statement. A corresponding nx.from_numpy_matrix(A) allows one to quickly create a graph from a NumPy matrix. The default graph type is Graph(); if you want to make it a DiGraph(), that has to be specified using the create_using keyword argument, e.g. (nx.from_numpy_matrix(A, create_using=nx.DiGraph)).\n\n\nCode\n!pip install nxviz\n\n\n\n\nCode\n# Import nxviz\nimport nxviz as nv\n\n# Create the MatrixPlot object: m\nm = nv.MatrixPlot(T)\n\n# Draw m to the screen\nm.draw()\n\n# Display the plot\nplt.show()\n\n# Convert T to a matrix format: A\nA = nx.to_numpy_matrix(T)\n\n# Convert A back to the NetworkX form as a directed graph: T_conv\nT_conv = nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n\n# Check that the `category` metadata field is lost from each node\nfor n, d in T_conv.nodes(data=True):\n    assert 'category' not in d.keys()\n\n\n/Users/kakamana/opt/anaconda3/lib/python3.9/site-packages/nxviz/api.py:275: UserWarning: As of nxviz 0.7, the object-oriented API is being deprecated in favour of a functional API. Please consider switching your plotting code! The object-oriented API wrappers remains in place to help you transition over. A few changes between the old and new API exist; please consult the nxviz documentation for more information. When the 1.0 release of nxviz happens, the object-oriented API will be dropped entirely.\n  warnings.warn(\n\n\nTypeError: draw() takes 0 positional arguments but 1 was given\n\n\n\n\n\n\n\nCode\nm\n\n\n\n\nCode\nA\n\n\n\n\nCode\nT_conv\n\n\n\n\nCode\nT_conv.nodes(data=True)\n\n\n\n\nCode\nT_conv.edges(data=True)\n\n\n\n\nCode\nT.nodes(data=True)\n\n\n\n\nCode\nT.edges(data=True)"
  },
  {
    "objectID": "posts/Introduction to networks in network analysis/Introduction to networks.html#visualizing-using-circos-plots",
    "href": "posts/Introduction to networks in network analysis/Introduction to networks.html#visualizing-using-circos-plots",
    "title": "Introduction to networks",
    "section": "Visualizing using Circos plots",
    "text": "Visualizing using Circos plots\nCircos plots are a rational, non-cluttered way of visualizing graph data, in which nodes are ordered around the circumference in some fashion, and the edges are drawn within the circle that results, giving a beautiful as well as informative visualization about the structure of the network.\n\n\nCode\nfrom nxviz import CircosPlot\n\n# Create the CircosPlot object: c\nc = CircosPlot(T)\n\n# Draw c to the screen\nc.draw()\n\n# Display the plot\nplt.show()\n\n\n/Users/kakamana/opt/anaconda3/lib/python3.9/site-packages/nxviz/api.py:275: UserWarning: As of nxviz 0.7, the object-oriented API is being deprecated in favour of a functional API. Please consider switching your plotting code! The object-oriented API wrappers remains in place to help you transition over. A few changes between the old and new API exist; please consult the nxviz documentation for more information. When the 1.0 release of nxviz happens, the object-oriented API will be dropped entirely.\n  warnings.warn(\n\n\nTypeError: draw() takes 0 positional arguments but 1 was given"
  },
  {
    "objectID": "posts/Introduction to networks in network analysis/Introduction to networks.html#visualizing-using-arc-plots",
    "href": "posts/Introduction to networks in network analysis/Introduction to networks.html#visualizing-using-arc-plots",
    "title": "Introduction to networks",
    "section": "Visualizing using Arc plots",
    "text": "Visualizing using Arc plots\nFollowing on what you’ve learned about the nxviz API, now try making an ArcPlot of the network. Two keyword arguments that you will try here are node_order=‘keyX’ and node_color=‘keyX’, in which you specify a key in the node metadata dictionary to color and order the nodes by.\n\n\nCode\nfrom nxviz import ArcPlot\n\n# Create the un-customized ArcPlot object: a\na = ArcPlot(T)\n\n# Draw a to the screen\na.draw()\n\n# Display the plot\nplt.show()\n\n# Create the customized ArcPlot object: a2\na2 = ArcPlot(T, node_order='category', node_color='category')\n\n# Draw a2 to the screen\na2.draw()\n\n# Display the plot\nplt.show()\n\n\n/Users/kakamana/opt/anaconda3/lib/python3.9/site-packages/nxviz/api.py:275: UserWarning: As of nxviz 0.7, the object-oriented API is being deprecated in favour of a functional API. Please consider switching your plotting code! The object-oriented API wrappers remains in place to help you transition over. A few changes between the old and new API exist; please consult the nxviz documentation for more information. When the 1.0 release of nxviz happens, the object-oriented API will be dropped entirely.\n  warnings.warn(\n\n\nTypeError: draw() takes 0 positional arguments but 1 was given"
  },
  {
    "objectID": "posts/Logistic regression/Logistic regression.html",
    "href": "posts/Logistic regression/Logistic regression.html",
    "title": "Logistic regression",
    "section": "",
    "text": "We will discover the conceptual framework behind logistic regression and SVMs. This will let us delve deeper into the inner workings of these models.\nThis Logistic regression is part of Datacamp course: Linear Classifiers in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = (10, 5)\n\n\n\n\n\nRegularized logistic regression\n\nHyperparameter “C” is the inverse of the regularization strength\n\nLarger “C”: less regularization\nSmaller “C”: more regularization\n\nregularized loss = original loss + large coefficient penalty\n\nmore regularization: lower training accuracy\nmore regularization: (almost always) higher test accuracy\n\n\nL1 vs. L2 regularization\n\nLasso = linear regression with L1 regularization\nRidge = linear regression with L2 regularization\n\n\n\n\nEarlier we used logistic regression on the handwritten digits data set. Here, we’ll explore the effect of L2 regularization. The handwritten digits dataset is already loaded, split, and stored in the variables X_train, y_train, X_valid, and y_valid. The variables train_errs and valid_errs are already initialized as empty lists.\n\n\nCode\nfrom sklearn.datasets import load_digits\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\n\nX_train, X_valid, y_train, y_valid = train_test_split(digits.data, digits.target)\n\n\n\n\nCode\n# Train and validation errors initialized as empty list\ntrain_errs = list()\nvalid_errs = list()\n\n# Loop over values of C_value\nC_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nfor C_value in C_values:\n    # Create LogisticRegression object and fit\n    lr = LogisticRegression(C=C_value, max_iter=10000)\n    lr.fit(X_train, y_train)\n\n    # Evalueate error rates and append to lists\n    train_errs.append(1.0 - lr.score(X_train, y_train))\n    valid_errs.append(1.0 - lr.score(X_valid, y_valid))\n\n# Plot results\nplt.semilogx(C_values, train_errs, C_values, valid_errs);\nplt.ylabel('classification error')\nplt.xlabel('C (Inverse regularization strength)')\nplt.legend((\"train\", \"validation\"));\nprint(\"\\nAs you can see, too much regularization (small C) doesn’t work well – due to underfitting – and too little regularization (large C) doesn’t work well either – due to overfitting.\")\n\n\n\nAs you can see, too much regularization (small C) doesn’t work well – due to underfitting – and too little regularization (large C) doesn’t work well either – due to overfitting.\n\n\n\n\n\n\n\n\nIn this exercise we’ll perform feature selection on the movie review sentiment data set using L1 regularization. The features and targets are already loaded for you in X_train and y_train.\nWe’ll search for the best value of C using scikit-learn’s GridSearchCV(), which was covered in the prerequisite course.\n\n\nCode\nfrom sklearn.datasets import load_svmlight_file\n\nX_train, y_train = load_svmlight_file('dataset/aclImdb_v1/aclImdb/train/labeledBow.feat')\ny_train[y_train < 5] = -1.0\ny_train[y_train >= 5] = 1.0\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# Specify L1 regularization\nlr = LogisticRegression(penalty='l1', solver='liblinear')\n\n# Instantiate the GridSearchCV object and run the search\nsearcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters\nprint(\"Best CV params\", searcher.best_params_)\n\n# Find the number of nonzero coefficients (select features)\nbest_lr = searcher.best_estimator_\ncoefs = best_lr.coef_\nprint(\"Total number of features:\", coefs.size)\nprint(\"Number of selected features:\", np.count_nonzero(coefs))\n\n\nBest CV params {'C': 0.1}\nTotal number of features: 89527\nNumber of selected features: 1106\n\n\n\n\n\nIn this exercise we’ll try to interpret the coefficients of a logistic regression fit on the movie review sentiment dataset. The model object is already instantiated and fit for you in the variable lr.\nIn addition, the words corresponding to the different features are loaded into the variable vocab. For example, since vocab[100] is “think”, that means feature 100 corresponds to the number of times the word “think” appeared in that movie review.\n\n\nCode\nvocab = pd.read_csv('dataset/vocab.csv').to_numpy()\n\n\n\n\nCode\nlr\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\n\nLogisticRegression(multi_class='ovr', n_jobs=1, solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(multi_class='ovr', n_jobs=1, solver='liblinear')\n\n\n\n\nCode\nvocab.shape\n\n\n(2500, 1)\n\n\n\n\nCode\nvocab[:3]\n\n\narray([['the'],\n       ['and'],\n       ['a']], dtype=object)\n\n\n\n\nCode\nvocab[-3:]\n\n\narray([['birth'],\n       ['sorts'],\n       ['gritty']], dtype=object)\n\n\n\n\nCode\ninds_ascending = np.argsort(best_lr.coef_.flatten())\ninds_descending = inds_ascending[::-1]\n\n\n\n\nCode\n# Print the most positive words\nprint(\"Most positive words: \", end=\"\")\nfor i in range(5):\n    print(vocab[inds_descending[i]], end=\", \")\nprint(\"\\n\")\n\n# Print most negative words\nprint(\"Most negative words: \", end=\"\")\nfor i in range(5):\n    print(vocab[inds_ascending[i]], end=\", \")\nprint(\"\\n\")\n\n\nMost positive words: ['excellent'], ['refreshing'], ['wonderfully'], ['perfect'], ['superb'], \n\nMost negative words: ['waste'], ['worst'], ['disappointment'], ['poorly'], ['awful'], \n\n\n\n\n\n\n\n\nRegularization is supposed to combat overfitting, and there is a connection between overconfidence and overfitting\nHow are these probabilities computed?\n\nlogistic regression predictions: sign of raw model output\nlogistic regression probabilities: “squashed” raw model output\n\n\n\n\nIn this exercise, you will observe the effects of changing the regularization strength on the predicted probabilities.\n\n\nCode\nX = pd.read_csv('./dataset/binary_X.csv').to_numpy()\ny = pd.read_csv('./dataset/binary_y.csv').to_numpy().ravel()\n\n\n\n\nCode\ndef make_meshgrid(x, y, h=.02, lims=None):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n        x: data to base x-axis meshgrid on\n        y: data to base y-axis meshgrid on\n        h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n        xx, yy : ndarray\n    \"\"\"\n\n    if lims is None:\n        x_min, x_max = x.min() - 1, x.max() + 1\n        y_min, y_max = y.min() - 1, y.max() + 1\n    else:\n        x_min, x_max, y_min, y_max = lims\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, proba=False, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n        ax: matplotlib axes object\n        clf: a classifier\n        xx: meshgrid ndarray\n        yy: meshgrid ndarray\n        params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    if proba:\n        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,-1]\n        Z = Z.reshape(xx.shape)\n        out = ax.imshow(Z,extent=(np.min(xx), np.max(xx), np.min(yy), np.max(yy)),\n                        origin='lower', vmin=0, vmax=1, **params)\n        ax.contour(xx, yy, Z, levels=[0.5])\n    else:\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        out = ax.contourf(xx, yy, Z, **params)\n    return out\n\ndef plot_classifier(X, y, clf, ax=None, ticks=False, proba=False, lims=None):\n    # assumes classifier \"clf\" is already fit\n    X0, X1 = X[:, 0], X[:, 1]\n    xx, yy = make_meshgrid(X0, X1, lims=lims)\n\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n        show = True\n    else:\n        show = False\n\n    # can abstract some of this into a higher-level function for learners to call\n    cs = plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8, proba=proba)\n    if proba:\n        cbar = plt.colorbar(cs)\n        cbar.ax.set_ylabel('probability of red $\\Delta$ class', fontsize=20, rotation=270, labelpad=30)\n        cbar.ax.tick_params(labelsize=14)\n        #ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors=\\'k\\', linewidth=1)\n    labels = np.unique(y)\n    if len(labels) == 2:\n        ax.scatter(X0[y==labels[0]], X1[y==labels[0]], cmap=plt.cm.coolwarm,\n                   s=60, c='b', marker='o', edgecolors='k')\n        ax.scatter(X0[y==labels[1]], X1[y==labels[1]], cmap=plt.cm.coolwarm,\n                   s=60, c='r', marker='^', edgecolors='k')\n    else:\n        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=50, edgecolors='k', linewidth=1)\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    #     ax.set_xlabel(data.feature_names[0])\n    #     ax.set_ylabel(data.feature_names[1])\n    if ticks:\n        ax.set_xticks(())\n        ax.set_yticks(())\n        #     ax.set_title(title)\n    if show:\n        plt.show()\n    else:\n        return ax\n\n\n\n\nCode\n# Set the regularization strength\nmodel = LogisticRegression(C=1)\n\n# Fit and plot\nmodel.fit(X, y)\nplot_classifier(X, y, model, proba=True)\n\n# Predict probabilities on training points\nprob = model.predict_proba(X)\nprint(\"Maximum predicted probability\", np.max(prob))\n\n\n\n\n\nMaximum predicted probability 0.9973143426900802\n\n\n\n\nCode\n# Set the regularization strength\nmodel = LogisticRegression(C=0.1)\n\n# Fit and plot\nmodel.fit(X, y)\nplot_classifier(X, y, model, proba=True)\n\n# Predict probabilities on training points\nprob = model.predict_proba(X)\nprint(\"Maximum predicted probabilty\", np.max(prob))\nprint(\"\\nAs you probably noticed, smaller values of C lead to less confident predictions. That's because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero and, thus, probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function. That's quite a chain of events!\")\n\n\n\n\n\nMaximum predicted probabilty 0.9352061680350907\n\nAs you probably noticed, smaller values of C lead to less confident predictions. That's because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero and, thus, probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function. That's quite a chain of events!\n\n\n\n\n\nWe’ll visualize the examples that the logistic regression model is most and least confident about by looking at the largest and smallest predicted probabilities.\n\n\nCode\ndef show_digit(i, lr=None):\n    plt.imshow(np.reshape(X[i], (8,8)), cmap='gray',\n               vmin = 0, vmax = 16, interpolation=None)\n    plt.xticks(())\n    plt.yticks(())\n    if lr is None:\n        plt.title(\"class label = %d\" % y[i])\n    else:\n        pred = lr.predict(X[i][None])\n        pred_prob = lr.predict_proba(X[i][None])[0,pred]\n        plt.title(\"label=%d, prediction=%d, proba=%.2f\" % (y[i], pred, pred_prob))\n        plt.show()\n\n\n\n\nCode\nX, y = digits.data, digits.target\n\n\n\n\nCode\nlr = LogisticRegression(max_iter=10000)\nlr.fit(X, y)\n\n# Get predicted probabilties\nproba = lr.predict_proba(X)\n\n# Sort the example indices by their maximum probabilty\nproba_inds = np.argsort(np.max(proba, axis=1))\n\n# Show the most confident (least ambiguous) digit\nshow_digit(proba_inds[-1], lr)\n\n# Show the least confident (most ambiguous) digit\nshow_digit(proba_inds[0], lr)\nprint(\"\\nAs you can see, the least confident example looks like a weird 9, and the most confident example looks like a very typical 5\")\n\n\n\n\n\n\n\n\n\nAs you can see, the least confident example looks like a weird 9, and the most confident example looks like a very typical 5\n\n\nIf you fit a logistic regression model on a classification problem with 3 classes and 100 features, how many coefficients would you have, including intercepts?\n303\n100 coefficients + 1 intercept for each binary classifier. (A, B), (B, C), (C, A) 101 * 3 = 303\n\n\n\n\n\nOne-vs-rest:\n\nfit a binary classifier for each class\npredict with all, take largest output\npro: simple, modular\ncon: not directly optimizing accuracy\ncommon for SVMs as well\n\n“Multinomial” or “Softmax”:\n\nfit a single classifier for all classes\nprediction directly outputs best class\npro: tackle the problem directly\ncon: more complicated, new code\npossible for SVMs, but less common\n\n\n\n\nWe’ll fit the two types of multi-class logistic regression, one-vs-rest and softmax/multinomial, on the handwritten digits data set and compare the results.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n\n\n\n\nCode\n# Fit one-vs-rest logistic regression classifier\nlr_ovr = LogisticRegression(multi_class='ovr',max_iter=10000)\nlr_ovr.fit(X_train, y_train)\n\nprint(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\nprint(\"OVR test accuracy:\", lr_ovr.score(X_test, y_test))\n\n# Fit softmax classifier\nlr_mn = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000)\nlr_mn.fit(X_train, y_train)\nprint(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\nprint(\"Softmax test accuracy:\", lr_mn.score(X_test, y_test))\nprint(\"\\nthe accuracies of the two methods are fairly similar on this data set.\")\n\n\nOVR training accuracy: 0.9985152190051967\nOVR test accuracy: 0.9511111111111111\nSoftmax training accuracy: 1.0\nSoftmax test accuracy: 0.9622222222222222\n\nthe accuracies of the two methods are fairly similar on this data set.\n\n\n\n\n\nWe’ll continue with the two types of multi-class logistic regression, but on a toy 2D data set specifically designed to break the one-vs-rest scheme.\n\n\nCode\nX_train = pd.read_csv('./dataset/toy_X_train.csv').to_numpy()\ny_train = pd.read_csv('./dataset/toy_y_train.csv').to_numpy().ravel()\n\n\n\n\nCode\nlr_ovr = LogisticRegression(max_iter=10000, C=100)\nlr_ovr.fit(X_train, y_train)\n\nfig, ax = plt.subplots();\nax.set_title(\"lr_ovr (one-vs-rest)\");\nplot_classifier(X_train, y_train, lr_ovr, ax=ax);\n\n\n\n\n\n\n\nCode\nlr_mn = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000)\nlr_mn.fit(X_train, y_train)\n\nfig, ax = plt.subplots();\nax.set_title(\"lr_mn (softmax)\");\nplot_classifier(X_train, y_train, lr_ovr, ax=ax);\n\n\n\n\n\n\n\nCode\nprint(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\nprint(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n\n# Create the binary classifier (class 1 vs. rest)\nlr_class_1 = LogisticRegression(max_iter=10000, C=100)\nlr_class_1.fit(X_train, y_train == 1)\n\n# Plot the binary classifier (class 1 vs. rest)\nplot_classifier(X_train, y_train == 1, lr_class_1);\nprint(\"\\nthe binary classifier incorrectly labels almost all points in class 1 (shown as red triangles in the final plot)! Thus, this classifier is not a very effective component of the one-vs-rest classifier. In general, though, one-vs-rest often works well.\")\n\n\nSoftmax training accuracy: 0.952\nOne-vs-rest training accuracy: 0.996\n\n\n\n\n\n\nthe binary classifier incorrectly labels almost all points in class 1 (shown as red triangles in the final plot)! Thus, this classifier is not a very effective component of the one-vs-rest classifier. In general, though, one-vs-rest often works well.\n\n\n\n\n\nAs motivation for the next and final chapter on support vector machines, we’ll repeat the previous exercise with a non-linear SVM. Once again, the data is loaded into X_train, y_train, X_test, and y_test .\nInstead of using LinearSVC, we’ll now use scikit-learn’s SVC object, which is a non-linear “kernel” SVM. Again, your task is to create a plot of the binary classifier for class 1 vs. rest.\n\n\nCode\nX_test = pd.read_csv('./dataset/toy_X_test.csv').to_numpy()\ny_test = pd.read_csv('./dataset/toy_y_test.csv').to_numpy().ravel()\n\n\n\n\nCode\nfrom sklearn.svm import SVC\n\n# Create/plot the binary classifier\nsvm_class_1 = SVC()\nsvm_class_1.fit(X_train, y_train)\nplot_classifier(X_test, y_test, svm_class_1)\n\n\n\n\n\n\n\nCode\n# Create/plot the binary classifier\nsvm_class_1 = SVC()\nsvm_class_1.fit(X_train, y_train == 1)\nplot_classifier(X_test, y_test == 1, svm_class_1)\nprint('\\nThe non-linear SVM works fine with one-vs-rest on this dataset because it learns to \"surround\" class 1')\n\n\n\n\n\n\nThe non-linear SVM works fine with one-vs-rest on this dataset because it learns to \"surround\" class 1"
  },
  {
    "objectID": "posts/Loss function/Loss function.html",
    "href": "posts/Loss function/Loss function.html",
    "title": "Loss function",
    "section": "",
    "text": "We will discover the conceptual framework behind logistic regression and SVMs. This will let us delve deeper into the inner workings of these models.\nThis Loss function is part of Datacamp course: Linear Classifiers in Python\nThis is my learning experience of data science through DataCamp\n\n\n\nraw model output = coefficient * feature + intercept\nLinear classifier prediction: compute raw model output, check the sign\n\nif +ve predict one class\nif -ve predict another class\n\nThis is same for logistic regression and linear SVM\n\nfit is different but predict is same\n\n\n\n\n\nWhen you call fit with scikit-learn, the logistic regression coefficients are automatically learned from your dataset. In this exercise you will explore how the decision boundary is represented by the coefficients. To do so, you will change the coefficients manually (instead of with fit), and visualize the resulting classifiers.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\n#hide\nX = np.array([[ 1.78862847,  0.43650985],\n       [ 0.09649747, -1.8634927 ],\n       [-0.2773882 , -0.35475898],\n       [-3.08274148,  2.37299932],\n       [-3.04381817,  2.52278197],\n       [-1.31386475,  0.88462238],\n       [-2.11868196,  4.70957306],\n       [-2.94996636,  2.59532259],\n       [-3.54535995,  1.45352268],\n       [ 0.98236743, -1.10106763],\n       [-1.18504653, -0.2056499 ],\n       [-1.51385164,  3.23671627],\n       [-4.02378514,  2.2870068 ],\n       [ 0.62524497, -0.16051336],\n       [-3.76883635,  2.76996928],\n       [ 0.74505627,  1.97611078],\n       [-1.24412333, -0.62641691],\n       [-0.80376609, -2.41908317],\n       [-0.92379202, -1.02387576],\n       [ 1.12397796, -0.13191423]])\n\ny = np.array([-1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1, -1,\n       -1, -1, -1])\n\n\n\n\nCode\ndef make_meshgrid(x, y, h=.02, lims=None):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n        x: data to base x-axis meshgrid on\n        y: data to base y-axis meshgrid on\n        h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n        xx, yy : ndarray\n    \"\"\"\n\n    if lims is None:\n        x_min, x_max = x.min() - 1, x.max() + 1\n        y_min, y_max = y.min() - 1, y.max() + 1\n    else:\n        x_min, x_max, y_min, y_max = lims\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, proba=False, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n        ax: matplotlib axes object\n        clf: a classifier\n        xx: meshgrid ndarray\n        yy: meshgrid ndarray\n        params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    if proba:\n        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,-1]\n        Z = Z.reshape(xx.shape)\n        out = ax.imshow(Z,extent=(np.min(xx), np.max(xx), np.min(yy), np.max(yy)),\n                        origin='lower', vmin=0, vmax=1, **params)\n        ax.contour(xx, yy, Z, levels=[0.5])\n    else:\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        out = ax.contourf(xx, yy, Z, **params)\n    return out\n\ndef plot_classifier(X, y, clf, ax=None, ticks=False, proba=False, lims=None):\n    # assumes classifier \"clf\" is already fit\n    X0, X1 = X[:, 0], X[:, 1]\n    xx, yy = make_meshgrid(X0, X1, lims=lims)\n\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n        show = True\n    else:\n        show = False\n\n    # can abstract some of this into a higher-level function for learners to call\n    cs = plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8, proba=proba)\n    if proba:\n        cbar = plt.colorbar(cs)\n        cbar.ax.set_ylabel('probability of red $\\Delta$ class', fontsize=20, rotation=270, labelpad=30)\n        cbar.ax.tick_params(labelsize=14)\n        #ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors=\\'k\\', linewidth=1)\n    labels = np.unique(y)\n    if len(labels) == 2:\n        ax.scatter(X0[y==labels[0]], X1[y==labels[0]], cmap=plt.cm.coolwarm,\n                   s=60, c='b', marker='o', edgecolors='k')\n        ax.scatter(X0[y==labels[1]], X1[y==labels[1]], cmap=plt.cm.coolwarm,\n                   s=60, c='r', marker='^', edgecolors='k')\n    else:\n        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=50, edgecolors='k', linewidth=1)\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    #     ax.set_xlabel(data.feature_names[0])\n    #     ax.set_ylabel(data.feature_names[1])\n    if ticks:\n        ax.set_xticks(())\n        ax.set_yticks(())\n        #     ax.set_title(title)\n    if show:\n        plt.show()\n    else:\n        return ax\n\n\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n\nCode\n# Set the coefficients\nmodel.coef_ = np.array([[0,1]])\nmodel.intercept_ = np.array([0])\n\n# Plot the data and decision boundary\nplot_classifier(X,y,model)\n\n# Print the number of errors\nnum_err = np.sum(y != model.predict(X))\nprint(\"Number of errors:\", num_err)\n\n\n\n\n\nNumber of errors: 3\n\n\n\n\nCode\n# Set the coefficients\nmodel.coef_ = np.array([[-1,1]])\nmodel.intercept_ = np.array([-3])\n\n# Plot the data and decision boundary\nplot_classifier(X,y,model)\n\n# Print the number of errors\nnum_err = np.sum(y != model.predict(X))\nprint(\"Number of errors:\", num_err)\nprint(\"\\nAs you can see, the coefficients determine the slope of the boundary and the intercept shifts it.\")\n\n\n\n\n\nNumber of errors: 0\n\nAs you can see, the coefficients determine the slope of the boundary and the intercept shifts it.\n\n\n\n\n\n\nLeast squares: the squared loss\n\nscikit-learn’s LinearRegression minimizes a loss: \\[ \\sum_{i=1}^{n}(\\text{true ith target value - predicted ith target value})^2 \\]\nMinimization is with respect to coefficients or parameters of the model.\n\nClassification errors: the 0-1 loss\n\nSquared loss not appropriate for classification problems\nA natrual loss for classification problem is the number of errors\nThis is the 0-1 loss: it’s 0 for a correct prediction and 1 for an incorrect prediction\nBut this loss is hard to minimize\n\n\n\n\nIn this exercise you’ll implement linear regression “from scratch” using scipy.optimize.minimize.\nWe’ll train a model on the Boston housing price data set.\n\n\nCode\nX = pd.read_csv('dataset/boston_X.csv').to_numpy()\ny = pd.read_csv('dataset/boston_y.csv').to_numpy()\n\n\n\n\nCode\nfrom scipy.optimize import minimize\nfrom sklearn.linear_model import LinearRegression\n\n# The squared error, summed overt training examples\ndef my_loss(w):\n    s = 0\n    for i in range(y.size):\n        # Get the true and predicted target values for example 'i'\n        y_i_true = y[i]\n        y_i_pred = w@X[i]\n        s = s + (y_i_true - y_i_pred) ** 2\n    return s\n\n# Returns the w that makes my_loss(w) smallest\nw_fit = minimize(my_loss, X[0]).x\nprint(w_fit)\n\n# Compare with scikit-learn's LinearRegression coefficients\nlr = LinearRegression(fit_intercept=False).fit(X, y)\nprint(lr.coef_)\n\n\n[-9.16299653e-02  4.86753446e-02 -3.77679680e-03  2.85637065e+00\n -2.88057050e+00  5.92521235e+00 -7.22477068e-03 -9.67992962e-01\n  1.70449044e-01 -9.38970634e-03 -3.92422954e-01  1.49831080e-02\n -4.16973126e-01]\n[[-9.16297843e-02  4.86751203e-02 -3.77930006e-03  2.85636751e+00\n  -2.88077933e+00  5.92521432e+00 -7.22447929e-03 -9.67995240e-01\n   1.70443393e-01 -9.38925373e-03 -3.92425680e-01  1.49832102e-02\n  -4.16972624e-01]]\n\n\n\n\n\n         \n\n\n\nIn this exercise you’ll create a plot of the logistic and hinge losses using their mathematical expressions, which are provided to you.\n\n\nCode\n# Mathematical functions for logistic and hinge losses\ndef log_loss(raw_model_output):\n    return np.log(1 + np.exp(-raw_model_output))\ndef hinge_loss(raw_model_output):\n    return np.maximum(0, 1 - raw_model_output)\n\n# Create a grid of values and plot\ngrid = np.linspace(-2,2,1000)\nplt.plot(grid, log_loss(grid), label='logistic');\nplt.plot(grid, hinge_loss(grid), label='hinge');\nplt.axvline(x=0, linestyle='dashed', color='k')\nplt.legend();\nprint(\"\\nAs you can see, these match up with the loss function diagrams we saw in the video\")\n\n\n\nAs you can see, these match up with the loss function diagrams we saw in the video\n\n\n\n\n\n\n\n\nThis is very similar to the earlier exercise where you implemented linear regression “from scratch” using scipy.optimize.minimize. However, this time we’ll minimize the logistic loss and compare with scikit-learn’s LogisticRegression.\nThe log_loss() function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y.\n\n\nCode\nX = pd.read_csv('./dataset/breast_X.csv').to_numpy()\ny = pd.read_csv('./dataset/breast_y.csv').to_numpy()\n\n\n\n\nCode\n# logistic loss, summed over training examples\ndef my_loss(w):\n    s = 0\n    for i in range(y.size):\n        raw_model_output = w@X[i]\n        s = s + log_loss(raw_model_output * y[i])\n    return s\n\n# Returns the w that makes my_loss(w) smallest\nw_fit = minimize(my_loss, X[0]).x\nprint(w_fit)\n\n# Compare with scikit-learn's LogisticRegression\nlr = LogisticRegression(fit_intercept=False, C=1000000).fit(X, y)\nprint(lr.coef_)\nprint(\"\\nAs you can see, logistic regression is just minimizing the loss function we've been looking at.\")\n\n\n[ 1.03645622 -1.65378473  4.08272663 -9.40921136 -1.06787193  0.07895825\n -0.85110073 -2.44101992 -0.4528567   0.43352876]\n[[ 1.03665946 -1.65380077  4.08233062 -9.40904867 -1.06787935  0.07901598\n  -0.85099843 -2.44107473 -0.45288928  0.43348202]]\n\nAs you can see, logistic regression is just minimizing the loss function we've been looking at.\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)"
  },
  {
    "objectID": "posts/Predicting time series data/Predicting Time Series Data.html",
    "href": "posts/Predicting time series data/Predicting Time Series Data.html",
    "title": "Predicting Time Series Data",
    "section": "",
    "text": "How you choose and construct a model for predicting patterns from data over time requires special consideration. Predictive modeling for time series data is discussed in this chapter, along with best practices on how to gain insights into the data before fitting your model.\nThis Predicting Time Series Data is part of Datacamp course: Machine Learning for Time Series Data in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (10, 5)\nplt.style.use('fivethirtyeight')\n\n\n\n\n\nCorrelation and regression\n\nRegression is similar to calculating correlation, with some key differences\n\nRegression: A process that results in a formal model of the data\nCorrelation: A statistic that describes the data. Less information than regression model\n\n\nCorrelation between variables often changes over time\n\nTime series often have patterns that change over time\nTwo timeseries that seem correlated at one moment may not remain so over time.\n\nScoring regression models\n\nTwo most common methods:\n\nCorrelation (\\(r\\))\nCoefficient of Determination (\\(R^2\\))\n\nThe value of \\(R^2\\) is bounded on the top by 1, and can be infinitely low\nValues closer to 1 mean the model does a better jot of predicting outputs\n\\(1 - \\frac{\\text{error}(model)}{\\text{variance}(testdata)}\\)\n\n\n\n\n\n\nCode\nprices = pd.read_csv('dataset/tsa_prices.csv', index_col='date', parse_dates=True)\nprices.head()\n\n\n\n\n\n\n  \n    \n      \n      EBAY\n      YHOO\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2010-01-04\n      23.900000\n      17.100000\n    \n    \n      2010-01-05\n      23.650000\n      17.230000\n    \n    \n      2010-01-06\n      23.500000\n      17.170000\n    \n    \n      2010-01-07\n      23.229998\n      16.700001\n    \n    \n      2010-01-08\n      23.509999\n      16.700001\n    \n  \n\n\n\n\n\n\nCode\n# Plot the raw values over time\nprices.plot();\n\n\n\n\n\n\n\nCode\n# Scatterplot with one company per axis\nprices.plot.scatter('EBAY', 'YHOO');\n\n\n\n\n\n\n\nCode\n# Scatterplot with color relating to time\nprices.plot.scatter('EBAY', 'YHOO', c=prices.index, cmap=plt.cm.viridis, colorbar=False);\n\n\n\n\n\n\n\nNow we’ll look at a larger number of companies. Recall that we have historical price values for many companies. Let’s use data from several companies to predict the value of a test company. You’ll attempt to predict the value of the Apple stock price using the values of NVidia, Ebay, and Yahoo. Each of these is stored as a column in the all_prices DataFrame. Below is a mapping from company name to column name:\nebay: \"EBAY\"\nnvidia: \"NVDA\"\nyahoo: \"YHOO\"\napple: \"AAPL\"\nWe’ll use these columns to define the input/output arrays in our model.\n\n\nCode\nall_prices = pd.read_csv('dataset/all_prices.csv', index_col=0, parse_dates=True)\nall_prices.head()\n\n\n\n\n\n\n  \n    \n      \n      AAPL\n      ABT\n      AIG\n      AMAT\n      ARNC\n      BAC\n      BSX\n      C\n      CHK\n      CMCSA\n      ...\n      QCOM\n      RF\n      SBUX\n      T\n      V\n      VZ\n      WFC\n      XOM\n      XRX\n      YHOO\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      214.009998\n      54.459951\n      29.889999\n      14.30\n      16.650013\n      15.690000\n      9.01\n      3.40\n      28.090001\n      16.969999\n      ...\n      46.939999\n      5.42\n      23.049999\n      28.580000\n      88.139999\n      33.279869\n      27.320000\n      69.150002\n      8.63\n      17.100000\n    \n    \n      2010-01-05\n      214.379993\n      54.019953\n      29.330000\n      14.19\n      16.130013\n      16.200001\n      9.04\n      3.53\n      28.970002\n      16.740000\n      ...\n      48.070000\n      5.60\n      23.590000\n      28.440001\n      87.129997\n      33.339868\n      28.070000\n      69.419998\n      8.64\n      17.230000\n    \n    \n      2010-01-06\n      210.969995\n      54.319953\n      29.139999\n      14.16\n      16.970013\n      16.389999\n      9.16\n      3.64\n      28.650002\n      16.620001\n      ...\n      47.599998\n      5.67\n      23.420000\n      27.610001\n      85.959999\n      31.919873\n      28.110001\n      70.019997\n      8.56\n      17.170000\n    \n    \n      2010-01-07\n      210.580000\n      54.769952\n      28.580000\n      14.01\n      16.610014\n      16.930000\n      9.09\n      3.65\n      28.720002\n      16.969999\n      ...\n      48.980000\n      6.17\n      23.360001\n      27.299999\n      86.760002\n      31.729875\n      29.129999\n      69.800003\n      8.60\n      16.700001\n    \n    \n      2010-01-08\n      211.980005\n      55.049952\n      29.340000\n      14.55\n      17.020014\n      16.780001\n      9.00\n      3.59\n      28.910002\n      16.920000\n      ...\n      49.470001\n      6.18\n      23.280001\n      27.100000\n      87.000000\n      31.749874\n      28.860001\n      69.519997\n      8.57\n      16.700001\n    \n  \n\n5 rows × 50 columns\n\n\n\n\n\nCode\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Use stock symbols to extract training data\nX = all_prices[['EBAY', 'NVDA', 'YHOO']]\ny = all_prices[['AAPL']]\n\n# Fit and score the model with cross-validation\nscores = cross_val_score(Ridge(), X, y, cv=3)\nprint(scores)\n\n\n[-6.09050633 -0.3179172  -3.72957284]\n\n\n\n\n\nWhen dealing with time series data, it’s useful to visualize model predictions on top of the “actual” values that are used to test the model.\nIn this exercise, after splitting the data (stored in the variables X and y) into training and test sets, you’ll build a model and then visualize the model’s predictions on top of the testing data in order to estimate the model’s performance.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n# Split our data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8,\n                                                   shuffle=False, random_state=1)\n\n# Fit our model and generate predictions\nmodel = Ridge()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\nscore = r2_score(y_test, predictions)\nprint(score)\n\n\n-0.3380697256228944\n\n\n\n\nCode\n# Visualize our predictions along with the \"true\" values, and print the score\nfig, ax = plt.subplots(figsize=(15, 5))\nax.plot(range(len(y_test)), y_test, color='k', lw=3);\nax.plot(range(len(predictions)), predictions, color='r', lw=2);\nprint(\"\\nThe predictions clearly deviate from the true time series values\")\n\n\n\nThe predictions clearly deviate from the true time series values\n\n\n\n\n\n\n\n\n\n\nData is messy\n\nReal-world data is often messy\nThe two most common problems are missing data and outliers\nThis often happens because of human error, machine error malfunction, database failure, etc..\nVisualizing your raw data makes it easier to spot these problems\n\nInterpolation: using time to fill in missing data\n\nA common way to deal with missing data is to interpolate missing values\nWith timeseries data, you can use time to assist in interpolation.\nIn this case, interpolation means using the known values on either side of a gap in the data to make assumptions about what’s missing\n\nUsing a rolling window to transform data\n\nAnother common use of rolling windows is to transform the data\n\nFinding outliers in your data\n\nOutliers are datapoints that are significantly statistically different from the dataset.\nThey can have negative effects on the predictive power of your model, biasing it away from its “true” value\nOne solution is to remove or replace outliers with a more representative value > Note: Be very careful about doing this - often it is difficult to determine what is a legitimately extreme value vs an abberation.\n\n\n\n\nLet’s take a look at a new dataset - this one is a bit less-clean than what you’ve seen before.\nAs always, you’ll first start by visualizing the raw data. Take a close look and try to find datapoints that could be problematic for fitting models.\n\n\nCode\nprices = pd.read_csv('dataset/prices_null.csv', index_col=0, parse_dates=True)\n\n\n\n\nCode\n# Visualize the dataset\nprices.plot(legend=False);\nplt.tight_layout();\n\n# Count the missing values of each time series\nmissing_values = prices.isnull().sum()\nprint(missing_values)\nprint(\"\\nIn the plot, you can see there are clearly missing chunks of time in your data. There also seem to be a few 'jumps' in the data\")\n\n\nEBAY    273\nNVDA    502\nYHOO    232\ndtype: int64\n\nIn the plot, you can see there are clearly missing chunks of time in your data. There also seem to be a few 'jumps' in the data\n\n\n\n\n\n\n\n\nWhen you have missing data points, how can you fill them in?\nIn this exercise, you’ll practice using different interpolation methods to fill in some missing values, visualizing the result each time. But first, you will create the function (interpolate_and_plot()) you’ll use to interpolate missing data points and plot them.\n\n\nCode\n# Create a function we'll use to interpolate and plot\ndef interpolate_and_plot(prices, interpolation):\n\n    # Create a boolean mask for missing values\n    missing_values = prices.isna()\n\n    # Interpolate the missing values\n    prices_interp = prices.interpolate(interpolation)\n\n    # Plot the results, highlighting the interpolated values in black\n    fig, ax = plt.subplots(figsize=(10, 5))\n    prices_interp.plot(color='k', alpha=0.6, ax=ax, legend=False);\n\n    # Note plot the interpolated values on top in red\n    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False);\n\n\n\n\nCode\n# Interpolate using the latest non-missing value\ninterpolation_type = 'zero'\ninterpolate_and_plot(prices, interpolation_type)\n\n\n\n\n\n\n\nCode\n# Interpolate using the latest non-missing value\ninterpolation_type = 'linear'\ninterpolate_and_plot(prices, interpolation_type)\n\n\n\n\n\n\n\nCode\n# Interpolate with a quadratic function\ninterpolation_type = 'quadratic'\ninterpolate_and_plot(prices, interpolation_type)\n\n\n\n\n\n\n\n\nIn the last chapter, you calculated the rolling mean. In this exercise, you will define a function that calculates the percent change of the latest data point from the mean of a window of previous data points. This function will help you calculate the percent change over a rolling window.\nThis is a more stable kind of time series that is often useful in machine learning.\n\n\nCode\n# Your custom function\ndef percent_change(series):\n    # Collect all *but* the last value of this window, then the final value\n    previous_values = series[:-1]\n    last_value = series[-1]\n\n    # Calculate the % difference between the last value and the mean of earlier values\n    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n    return percent_change\n\n# Apply your custom function and plot\nprices_perc = prices.rolling(20).apply(percent_change)\nprices_perc.loc[\"2014\":\"2015\"].plot();\n\n\n\n\n\n\n\n\nIn this exercise, you’ll handle outliers - data points that are so different from the rest of your data, that you treat them differently from other “normal-looking” data points. You’ll use the output from the previous exercise (percent change over time) to detect the outliers. First you will write a function that replaces outlier data points with the median value from the entire time series.\n\n\nCode\ndef replace_outliers(series):\n    # Calculate the absolute difference of each timepoint from the series mean\n    absolute_differences_from_mean = np.abs(series - np.mean(series))\n\n    # Calculate a mask for the difference that are > 3 standard deviations from zero\n    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n\n    # Replace these values with the median across the data\n    series[this_mask] = np.nanmedian(series)\n    return series\n\n# Apply your preprocessing functino to the timeseries and plot the results\nprices_perc = prices_perc.apply(replace_outliers)\nprices_perc.loc[\"2014\":\"2015\"].plot();\n\n\n\n\n\n\n\n\n\n\nCalculating “date-based” features\n\nThus far we’ve focused on calculating “statistical” features - these are features that correspond statistical properties of the data, like “mean” , “standard deviation”, etc\nHowever, don’t forget the timeseries data oftenhas more “human” features associated with it, like days of the week, holidays, etc.\nThese features are often useful when dealing with timeseries data that spans multiple years (such as stock value over time)\n\n\n\n\nNow that you’ve practiced some simple feature engineering, let’s move on to something more complex. You’ll calculate a collection of features for your time series data and visualize what they look like over time. This process resembles how many other time series models operate.\n\n\nCode\n# Define a rolling window with Pandas, excluding the right-most datapoint of the window\nprices_perc_rolling = prices_perc['EBAY'].rolling(20, min_periods=5, closed='right')\n\n# Define the features you'll calculate for each window\nfeatures_to_calculate = [np.min, np.max, np.mean, np.std]\n\n# Calculate these features for your rolling window object\nfeatures = prices_perc_rolling.aggregate(features_to_calculate)\n\n# Plot the results\nax = features.loc[:\"2011-01\"].plot();\nprices_perc['EBAY'].loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=0.2, lw=3);\nax.legend(loc=(1.01, 0.6));\n\n\n\n\n\n\n\n\nIn this exercise, you’ll practice how to pre-choose arguments of a function so that you can pre-configure how it runs. You’ll use this to calculate several percentiles of your data using the same percentile() function in numpy.\n\n\nCode\nfrom functools import partial\npercentiles = [1, 10, 25, 50, 75, 90, 99]\n\n# Use a list comprehension to create a partial function for each quantile\npercentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n\n# Calculate each of these quantiles on the data using a rolling window\nprices_perc_rolling = prices_perc['EBAY'].rolling(20, min_periods=5, closed='right')\nfeatures_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n\n# Plot a subset of the result\nax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis);\nax.legend(percentiles, loc=(1.01, 0.5));\n\n\n\n\n\n\n\n\nIt’s easy to think of timestamps as pure numbers, but don’t forget they generally correspond to things that happen in the real world. That means there’s often extra information encoded in the data such as “is it a weekday?” or “is it a holiday?”. This information is often useful in predicting timeseries data.\n\n\nCode\n# Extract date features from the data, add them as columns\nprices_perc['day_of_week'] = prices_perc.index.dayofweek\nprices_perc['week_of_year'] = prices_perc.index.weekofyear\nprices_perc['month_of_year'] = prices_perc.index.month\n\n# Print prices_perc\nprint(prices_perc)"
  },
  {
    "objectID": "posts/Predictions and model objects/Predictions and model objects.html",
    "href": "posts/Predictions and model objects/Predictions and model objects.html",
    "title": "Predictions and model objects in linear regression",
    "section": "",
    "text": "This article explores how linear regression models can be used to predict Taiwanese house prices and Facebook advert clicks. Our regression skills will also be developed through the use of hands-on model objects, as well as the concept of “regression to the mean” and how to transform variables within a dataset.\nThis Simple Linear Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nPredictions can be made using statistical models like linear regression. In other words, you specify each explanatory variable, feed it into the model, and get a prediction.\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847\n    \n  \n\n\n\n\n\n\nCode\n# Create the explanatory_data\nexplanatory_data = pd.DataFrame({'n_convenience': np.arange(0,10)})\n\n# Print it\nprint(explanatory_data)\n\n\n   n_convenience\n0              0\n1              1\n2              2\n3              3\n4              4\n5              5\n6              6\n7              7\n8              8\n9              9\n\n\n\n\nCode\nmdl_price_vs_conv=ols(\"price_twd_msq ~ n_convenience\",data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Create explanatory_data\nexplanatory_data = pd.DataFrame({'n_convenience': np.arange(0, 11)})\n\n# Use mdl_price_vs_conv to predict with explanatory_data, call it price_twd_msq\nprice_twd_msq = mdl_price_vs_conv.predict(explanatory_data)\n\n# Print it\nprint(price_twd_msq)\n\n\n0      8.224237\n1      9.022317\n2      9.820397\n3     10.618477\n4     11.416556\n5     12.214636\n6     13.012716\n7     13.810795\n8     14.608875\n9     15.406955\n10    16.205035\ndtype: float64\n\n\n\n\nCode\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_conv.predict(explanatory_data))\n\n# Print the result\nprint(prediction_data)\n\n\n    n_convenience  price_twd_msq\n0               0       8.224237\n1               1       9.022317\n2               2       9.820397\n3               3      10.618477\n4               4      11.416556\n5               5      12.214636\n6               6      13.012716\n7               7      13.810795\n8               8      14.608875\n9               9      15.406955\n10             10      16.205035\n\n\n\n\n\nThe prediction DataFrame you created contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values\n\n\nCode\n# Create a new figure, fig\nfig = plt.figure()\n\nsns.regplot(x=\"n_convenience\",\n            y=\"price_twd_msq\",\n            data=taiwan_real_estate,\n            ci=None)\n# Add a scatter plot layer to the regplot\nsns.scatterplot(x='n_convenience',y='price_twd_msq',data=prediction_data,color='red',marker='s')\n\n# Show the layered plot\nplt.show()\nprint(\"\\n the predicted points lie on the trend lin\")\n\n\n\n\n\n\n the predicted points lie on the trend lin\n\n\n\n\n\nThe model object created by ols() contains many elements. In order to perform further analysis on the model results, we need to extract its useful bits. The model coefficients, the fitted values, and the residuals are perhaps the most important pieces of the linear model object.\n\n\nCode\n# Print the model parameters of mdl_price_vs_conv\nprint(mdl_price_vs_conv.params)\n\n\nIntercept        8.224237\nn_convenience    0.798080\ndtype: float64\n\n\n\n\nCode\n# Print the fitted values of mdl_price_vs_conv\nprint(mdl_price_vs_conv.fittedvalues)\n\n\n0      16.205035\n1      15.406955\n2      12.214636\n3      12.214636\n4      12.214636\n         ...    \n409     8.224237\n410    15.406955\n411    13.810795\n412    12.214636\n413    15.406955\nLength: 414, dtype: float64\n\n\n\n\nCode\n# Print the residuals of mdl_price_vs_conv\nprint(mdl_price_vs_conv.resid)\n\n\n0     -4.737561\n1     -2.638422\n2      2.097013\n3      4.366302\n4      0.826211\n         ...   \n409   -3.564631\n410   -0.278362\n411   -1.526378\n412    3.670387\n413    3.927387\nLength: 414, dtype: float64\n\n\n\n\nCode\n# Print a summary of mdl_price_vs_conv\nprint(mdl_price_vs_conv.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          price_twd_msq   R-squared:                       0.326\nModel:                            OLS   Adj. R-squared:                  0.324\nMethod:                 Least Squares   F-statistic:                     199.3\nDate:                Thu, 12 Jan 2023   Prob (F-statistic):           3.41e-37\nTime:                        17:11:19   Log-Likelihood:                -1091.1\nNo. Observations:                 414   AIC:                             2186.\nDf Residuals:                     412   BIC:                             2194.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         8.2242      0.285     28.857      0.000       7.664       8.784\nn_convenience     0.7981      0.057     14.118      0.000       0.687       0.909\n==============================================================================\nOmnibus:                      171.927   Durbin-Watson:                   1.993\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1417.242\nSkew:                           1.553   Prob(JB):                    1.78e-308\nKurtosis:                      11.516   Cond. No.                         8.87\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nUsing the model coefficients, you can manually calculate predictions. It’s better to use .predict() when making predictions in real life, but doing it manually is helpful for reassuring yourself that predictions aren’t magic.\nFor simple linear regressions, the predicted value is the intercept plus the slope times the explanatory variable.\nresponse = intercept + slope * explanatory\n\n\nCode\n# Get the coefficients of mdl_price_vs_conv\ncoeffs = mdl_price_vs_conv.params\n\n# Get the intercept\nintercept = coeffs[0]\n\n# Get the slope\nslope = coeffs[1]\n\n# Manually calculate the predictions\nprice_twd_msq = intercept + slope * explanatory_data\nprint(price_twd_msq)\n\n# Compare to the results from .predict()\nprint(price_twd_msq.assign(predictions_auto=mdl_price_vs_conv.predict(explanatory_data)))\n\n\n    n_convenience\n0        8.224237\n1        9.022317\n2        9.820397\n3       10.618477\n4       11.416556\n5       12.214636\n6       13.012716\n7       13.810795\n8       14.608875\n9       15.406955\n10      16.205035\n    n_convenience  predictions_auto\n0        8.224237          8.224237\n1        9.022317          9.022317\n2        9.820397          9.820397\n3       10.618477         10.618477\n4       11.416556         11.416556\n5       12.214636         12.214636\n6       13.012716         13.012716\n7       13.810795         13.810795\n8       14.608875         14.608875\n9       15.406955         15.406955\n10      16.205035         16.205035\n\n\n\n\n\n\nResponse value = fitted value + residual\n“The stuff one can explain” + “the stuff once couldn’t explain”\nResiduals exist due to problems in model and fundamental randomness\nExtreme cases are often due to randomness\nRegression to mean indicated extreme cases don’t persist over time\n\n\n\nCode\nsp500_yearly_returns=pd.read_csv(\"dataset/sp500_yearly_returns.csv\")\nsp500_yearly_returns.head()\n\n\n\n\n\n\n  \n    \n      \n      symbol\n      return_2018\n      return_2019\n    \n  \n  \n    \n      0\n      AAPL\n      -0.053902\n      0.889578\n    \n    \n      1\n      MSFT\n      0.207953\n      0.575581\n    \n    \n      2\n      AMZN\n      0.284317\n      0.230278\n    \n    \n      3\n      FB\n      -0.257112\n      0.565718\n    \n    \n      4\n      GOOGL\n      -0.008012\n      0.281762\n    \n  \n\n\n\n\n\n\nCode\n# Create a new figure, fig\nfig = plt.figure()\n\n# Plot the first layer: y = x\nplt.axline(xy1=(0,0), slope=1, linewidth=2, color=\"green\")\n\n# Add scatter plot with linear regression trend line\nsns.regplot(x='return_2018',y='return_2019',data=sp500_yearly_returns,ci=None,line_kws={'color':'black'})\n\n# Set the axes so that the distances along the x and y axes look the same\nplt.axis(\"equal\")\n\n# Show the plot\nplt.show()\nprint('\\n The regression trend line looks very different to the y equals x line. As the financial advisors say, \"Past performance is no guarantee of future results.\"')\n\n\n\n\n\n\n The regression trend line looks very different to the y equals x line. As the financial advisors say, \"Past performance is no guarantee of future results.\"\n\n\n\n\n\nLet’s quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019\n\n\nCode\n# Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns\nmdl_returns = ols(\"return_2019 ~ return_2018\",data=sp500_yearly_returns).fit()\n\n# Print the parameters\nprint(mdl_returns.params)\n\n# Create a DataFrame with return_2018 at -1, 0, and 1\nexplanatory_data = pd.DataFrame({'return_2018':[-1,0,1]})\n\n# Use mdl_returns to predict with explanatory_data\nprint(mdl_returns.predict(explanatory_data))\n\nprint(\"\\n Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019\")\n\n\nIntercept      0.321321\nreturn_2018    0.020069\ndtype: float64\n0    0.301251\n1    0.321321\n2    0.341390\ndtype: float64\n\n Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019\n\n\n\n\n\nWhen there is no straight-line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both. Let’s transform the explanatory variable.\nWe’ll look at the Taiwan real estate dataset again, but we’ll use the distance to the nearest MRT (metro) station as the explanatory variable. By taking the square root, you’ll shorten the distance to the metro station for commuters.\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\nplt.figure()\n\n# Plot using the transformed variable\nsns.regplot(x='sqrt_dist_to_mrt_m',y='price_twd_msq',data=taiwan_real_estate)\nplt.show()\n\n\n\n\n\n\n\nCode\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\nprint(mdl_price_vs_dist.params)\n\n\nIntercept             16.709799\nsqrt_dist_to_mrt_m    -0.182843\ndtype: float64\n\n\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\nexplanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n\n# Create prediction_data by adding a column of predictions to explantory_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n)\n\n# Print the result\nprint(prediction_data)\n\n\n   sqrt_dist_to_mrt_m  dist_to_mrt_m  price_twd_msq\n0                 0.0              0      16.709799\n1                10.0            100      14.881370\n2                20.0            400      13.052942\n3                30.0            900      11.224513\n4                40.0           1600       9.396085\n5                50.0           2500       7.567656\n6                60.0           3600       5.739227\n7                70.0           4900       3.910799\n8                80.0           6400       2.082370\n\n\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\n# Use this explanatory data\nexplanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n\n# Use mdl_price_vs_dist to predict explanatory_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n)\n\nfig = plt.figure()\nsns.regplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n\n# Add a layer of your prediction points\nsns.scatterplot(data=prediction_data, x='sqrt_dist_to_mrt_m', y='price_twd_msq', color='red')\nplt.show()\n\nprint(\"\\n By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate mode\")\n\n\n\n\n\n\n By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate mode\n\n\n\n\n\nThe response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. That is, you “back transform” the predictions\n\n\nCode\nad_conversion=pd.read_csv('dataset/ad_conversion.csv')\nad_conversion.head()\n\n\n\n\n\n\n  \n    \n      \n      spent_usd\n      n_impressions\n      n_clicks\n    \n  \n  \n    \n      0\n      1.43\n      7350\n      1\n    \n    \n      1\n      1.82\n      17861\n      2\n    \n    \n      2\n      1.25\n      4259\n      1\n    \n    \n      3\n      1.29\n      4133\n      1\n    \n    \n      4\n      4.77\n      15615\n      3\n    \n  \n\n\n\n\n\n\nCode\n# Create qdrt_n_impressions and qdrt_n_clicks\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion['n_impressions'] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion['n_clicks'] ** 0.25\n\nplt.figure()\n\n# Plot using the transformed variables\nsns.regplot(x='qdrt_n_impressions',y='qdrt_n_clicks',data=ad_conversion,ci=None)\nplt.show()\n\n\n\n\n\n\n\nCode\n# From previous step\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n\n# Run a linear regression of your transformed variables\nmdl_click_vs_impression = ols('qdrt_n_clicks ~ qdrt_n_impressions', data=ad_conversion).fit()\nprint(mdl_click_vs_impression.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          qdrt_n_clicks   R-squared:                       0.945\nModel:                            OLS   Adj. R-squared:                  0.944\nMethod:                 Least Squares   F-statistic:                 1.590e+04\nDate:                Thu, 12 Jan 2023   Prob (F-statistic):               0.00\nTime:                        17:11:20   Log-Likelihood:                 193.90\nNo. Observations:                 936   AIC:                            -383.8\nDf Residuals:                     934   BIC:                            -374.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.0717      0.017      4.171      0.000       0.038       0.106\nqdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n==============================================================================\nOmnibus:                       11.447   Durbin-Watson:                   0.568\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\nSkew:                          -0.216   Prob(JB):                      0.00490\nKurtosis:                       2.707   Cond. No.                         52.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# From previous steps\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n\nmdl_click_vs_impression = ols(\"qdrt_n_clicks ~ qdrt_n_impressions\", data=ad_conversion, ci=None).fit()\n\n# Use this explanatory data\nexplanatory_data = pd.DataFrame({\"qdrt_n_impressions\": np.arange(0, 3e6+1, 5e5) ** .25,\n                                 \"n_impressions\": np.arange(0, 3e6+1, 5e5)})\n\n# Complete prediction_data\nprediction_data = explanatory_data.assign(\n    qdrt_n_clicks = mdl_click_vs_impression.predict(explanatory_data)\n)\n\n# Print the result\nprint(prediction_data)\nprint(\"\\n Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your result\")\n\n\n   qdrt_n_impressions  n_impressions  qdrt_n_clicks\n0            0.000000            0.0       0.071748\n1           26.591479       500000.0       3.037576\n2           31.622777      1000000.0       3.598732\n3           34.996355      1500000.0       3.974998\n4           37.606031      2000000.0       4.266063\n5           39.763536      2500000.0       4.506696\n6           41.617915      3000000.0       4.713520\n\n Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your result\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\base\\model.py:127: ValueWarning: unknown kwargs ['ci']\n  warnings.warn(msg, ValueWarning)\n\n\n\n\n\nIn the previous section, we transformed the response variable, ran a regression, and made predictions. However, we are not yet finished! We will need to perform a back-transformation in order to interpret and visualize your predictions correctly.\n\n\nCode\n# Back transform qdrt_n_clicks\nprediction_data[\"n_clicks\"] = prediction_data['qdrt_n_clicks'] ** 4\nprint(prediction_data)\n\n\n   qdrt_n_impressions  n_impressions  qdrt_n_clicks    n_clicks\n0            0.000000            0.0       0.071748    0.000026\n1           26.591479       500000.0       3.037576   85.135121\n2           31.622777      1000000.0       3.598732  167.725102\n3           34.996355      1500000.0       3.974998  249.659131\n4           37.606031      2000000.0       4.266063  331.214159\n5           39.763536      2500000.0       4.506696  412.508546\n6           41.617915      3000000.0       4.713520  493.607180\n\n\n\n\nCode\n# Plot the transformed variables\nfig = plt.figure()\nsns.regplot(x=\"qdrt_n_impressions\", y=\"qdrt_n_clicks\", data=ad_conversion, ci=None)\n\n# Add a layer of your prediction points\nsns.scatterplot(data=prediction_data, x='qdrt_n_impressions', y='qdrt_n_clicks', color='red')\nplt.show()\nprint(\"\\n Notice that your back-transformed predictions nicely follow the trend line and allow you to make more accurate predictions\")\n\n\n\n\n\n\n Notice that your back-transformed predictions nicely follow the trend line and allow you to make more accurate predictions"
  },
  {
    "objectID": "posts/Preprocessing and Pipelines/Preprocessing and pipelines.html",
    "href": "posts/Preprocessing and Pipelines/Preprocessing and pipelines.html",
    "title": "Preprocessing and Pipelines",
    "section": "",
    "text": "Preprocessing and Pipelines\nLearn how to impute missing values, convert categorical data to numeric values, scale data, evaluate multiple supervised learning models simultaneously, and build pipelines to streamline your workflow!\nThis Preprocessing and Pipelines is part of Datacamp course: Supervised Learning with scikit-learn\nThis is my learning experience of data science through DataCamp\nExploring categorical features\nGapminder datasets that you worked with in previous chapters also contained a categorical ‘Region’ feature, which we dropped since we didn’t have the tools. We have added it back in now that you know about it!\n\n\nCode\nimport matplotlib.pyplot as plt\n# Import pandas\nimport pandas as pd\n\n# Read 'gapminder.csv' into a DataFrame: df\ndf = pd.read_csv(\"gm_2008_region.csv\")\n\n# Create a boxplot of life expectancy per region\ndf.boxplot('life', 'Region', rot=60)\n\n# Show the plot\nplt.show()\n\n\n\n\n\nCreating dummy variables\nScikit-learn does not accept non-numerical features. Earlier we have learned that the ‘Region’ feature contains useful information for predicting life expectancy. Compared to Europe and Central Asia, Sub-Saharan Africa has a lower life expectancy. Thus, retaining the ‘Region’ feature is preferable if we are trying to predict life expectancy. In this exercise, we\n\n\nCode\n# Create dummy variables: df_region\ndf_region = pd.get_dummies(df)\n\n# Print the columns of df_region\nprint(df_region.columns)\nprint(df.sample(10))\n\n# Create dummy variables with drop_first=True: df_region\ndf_region = pd.get_dummies(df,drop_first=True)\n\n# Print the new columns of df_region\nprint(df_region.columns)\nprint(df.sample(10))\n\n\nIndex(['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n       'BMI_female', 'life', 'child_mortality', 'Region_America',\n       'Region_East Asia & Pacific', 'Region_Europe & Central Asia',\n       'Region_Middle East & North Africa', 'Region_South Asia',\n       'Region_Sub-Saharan Africa'],\n      dtype='object')\n      population  fertility  HIV        CO2  BMI_male      GDP  BMI_female  \\\n106  143123163.0       1.49  1.0  11.982718  26.01131  22506.0    128.4903   \n79      406392.0       1.38  0.1   6.182771  27.68361  27872.0    124.1571   \n121    9226333.0       1.92  0.1   5.315688  26.37629  43421.0    122.9473   \n109    9109535.0       1.41  0.1   5.271223  26.51495  12522.0    130.3755   \n37     6004199.0       2.32  0.8   1.067765  26.36751   7450.0    119.9321   \n89    16519862.0       1.77  0.2  10.533028  26.01541  47388.0    121.6950   \n30    19261647.0       4.91  3.7   0.361897  22.56469   2854.0    131.5237   \n83     4111168.0       1.49  0.4   1.313321  24.23690   3890.0    129.9424   \n80     3414552.0       4.94  0.7   0.613104  22.62295   3356.0    129.9875   \n76    27197419.0       2.05  0.5   7.752234  24.73069  19968.0    123.8593   \n\n     life  child_mortality                 Region  \n106  67.6             13.5  Europe & Central Asia  \n79   81.4              6.6  Europe & Central Asia  \n121  81.1              3.2  Europe & Central Asia  \n109  76.4              8.0  Europe & Central Asia  \n37   74.1             21.6                America  \n89   80.3              4.8  Europe & Central Asia  \n30   55.9            116.9     Sub-Saharan Africa  \n83   69.6             17.6  Europe & Central Asia  \n80   63.6            103.0     Sub-Saharan Africa  \n76   74.5              8.0    East Asia & Pacific  \nIndex(['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n       'BMI_female', 'life', 'child_mortality', 'Region_East Asia & Pacific',\n       'Region_Europe & Central Asia', 'Region_Middle East & North Africa',\n       'Region_South Asia', 'Region_Sub-Saharan Africa'],\n      dtype='object')\n     population  fertility    HIV       CO2  BMI_male      GDP  BMI_female  \\\n69    4109389.0       1.57   0.10  3.996722  27.20117  14158.0    127.5037   \n103  10577458.0       1.36   0.50  5.486926  26.68445  27747.0    127.2631   \n61    4480145.0       2.00   0.20  9.882531  27.65325  47713.0    124.7801   \n79     406392.0       1.38   0.10  6.182771  27.68361  27872.0    124.1571   \n114   9132589.0       7.06   0.60  0.068219  21.96917    615.0    131.5318   \n22   19570418.0       5.17   5.30  0.295542  23.68173   2571.0    127.2823   \n128  10408091.0       2.04   0.06  2.440669  25.15699   9938.0    128.6291   \n110   5521838.0       5.13   1.60  0.118256  22.53139   1289.0    134.7160   \n137  13114579.0       5.88  13.60  0.148982  20.68321   3039.0    132.4493   \n56   10050699.0       1.33   0.06  5.453232  27.11568  23334.0    128.6968   \n\n     life  child_mortality                      Region  \n69   77.6             11.3  Middle East & North Africa  \n103  79.2              4.1       Europe & Central Asia  \n61   79.4              4.5       Europe & Central Asia  \n79   81.4              6.6       Europe & Central Asia  \n114  56.7            168.5          Sub-Saharan Africa  \n22   56.6            113.8          Sub-Saharan Africa  \n128  76.5             19.4  Middle East & North Africa  \n110  55.9            179.1          Sub-Saharan Africa  \n137  52.0             94.9          Sub-Saharan Africa  \n56   73.8              7.2       Europe & Central Asia  \n\n\nUsing categorical features in regression\nWe can now build regression models using the dummy variables we have created using the ‘Region’ feature. We will perform 5-fold cross-validation here using ridge regression.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\ndf=pd.read_csv('gm_2008_region.csv')\ndf_region=pd.get_dummies(df)\ndf_region=df_region.drop('Region_America',axis=1)\nX=df_region.drop('life',axis=1).values\ny=df_region['life'].values\n\n\n\n\nCode\n# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate a ridge regressor: ridge\nridge = Ridge(alpha=0.5, normalize=True)\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge,X,y,cv=5)\n\n# Print the cross-validated scores\nprint(ridge_cv)\n\n\n[0.86808336 0.80623545 0.84004203 0.7754344  0.87503712]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\n\n\nDropping missing data\nEarlier excercise voting dataset contained a bunch of missing values that we handled behind the scenes. Now it’s your turn!\nDataFrame df loaded with unprocessed dataset. Use the .head() method in the IPython Shell. Some data points are labeled with ‘?’. Missing values are here. Different datasets encode missing values differently. Data in real life can be messy - sometimes a 9999, sometimes a 0. It’s possible that the missing values are already encoded as NaN. By using NaN, we can take advantage of pandas methods like .dropna() and .fillna(), as well as scikit-learn’s Imputation transformer Imputer().\nThis exercise requires you to convert ’? to NaNs, and then drop the rows that contain them.\n\n\nCode\ndf=pd.read_csv('house-votes-84.csv', header=None, names = ['infants', 'water', 'budget', 'physician', 'salvador', 'religious',\n 'satellite', 'aid', 'missile', 'immigration', 'synfuels', 'education',\n 'superfund', 'crime', 'duty_free_exports', 'eaa_rsa'])\ndf=df.reset_index()\ndf.rename(columns ={'index':'party'},inplace=True)\n\ndf[df=='y']=1\ndf[df=='n']=0\n\n\n\n\nCode\n# Convert '?' to NaN\ndf[df == '?'] = np.nan\n\n# Print the number of NaNs\nprint(df.isnull().sum())\n\n# Print shape of original DataFrame\nprint(\"Shape of Original DataFrame: {}\".format(df.shape))\n\n# Drop missing values and print shape of new DataFrame\ndf = df.dropna()\n\n# Print shape of new DataFrame\nprint(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))\n\n\nparty                  0\ninfants               12\nwater                 48\nbudget                11\nphysician             11\nsalvador              15\nreligious             11\nsatellite             14\naid                   15\nmissile               22\nimmigration            7\nsynfuels              21\neducation             31\nsuperfund             25\ncrime                 17\nduty_free_exports     28\neaa_rsa              104\ndtype: int64\nShape of Original DataFrame: (435, 17)\nShape of DataFrame After Dropping All Rows with Missing Values: (232, 17)\n\n\nWhen many values in your dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It’s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in\nImputing missing data in a ML Pipeline I\nThe process of building a model involves many steps, such as creating training and test sets, fitting a classifier or regressor, tuning its parameters, and evaluating its performance. In this machine learning process, imputation is the first step, which is viewed as part of a pipeline. With Scikit-learn, we can piece together these steps into one process and simplify your workflow.\nWe will now practice setting up a pipeline with two steps: imputation and classifier instantiation. We have so far tried three classifiers: k-NN, logistic regression, and decision trees. The Support Vector Machine, or SVM, is the fourth. Don’t worry about how it works. As with the scikit-learn estimators we have worked with before, it has the same .fit() and .predict() methods.\n\n\nCode\n# Import the Imputer module\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import SVC\n\n# Setup the Imputation transformer: imp\nimp = SimpleImputer(missing_values='NaN', strategy='most_frequent')\n\n# Instantiate the SVC classifier: clf\nclf = SVC()\n\n# Setup the pipeline with the required steps: steps\nsteps = [('imputation', imp),\n        ('SVM', clf)]\n\n\nImputing missing data in a ML Pipeline II\nHaving setup the steps of the pipeline in the previous exercise, you will now use it on the voting dataset to classify a Congressman’s party affiliation. What makes pipelines so incredibly useful is the simple interface that they provide. You can use the .fit() and .predict() methods on pipelines just as you did with your classifiers and regressors!\n\n\nCode\nimport pandas as pd\nimport numpy as np\n#\ndf = pd.read_csv('votes-ch1.csv')\n# Create arrays for the features and the response variable. As a reminder, the response variable is 'party'\ny = df['party'].values\nX = df.drop('party', axis=1).values\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Import necessary modules\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\n# Setup the pipeline steps: steps\nsteps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n        ('SVM', SVC())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n    democrat       0.98      0.96      0.97        85\n  republican       0.94      0.96      0.95        46\n\n    accuracy                           0.96       131\n   macro avg       0.96      0.96      0.96       131\nweighted avg       0.96      0.96      0.96       131\n\n\n\nCentering and scaling your data\nIn the video, Hugo demonstrated how significantly the performance of a model can improve if the features are scaled. Note that this is not always the case: In the Congressional voting records dataset, for example, all of the features are binary. In such a situation, scaling will have minimal impact.\nYou will now explore scaling for yourself on a new dataset - White Wine Quality! Hugo used the Red Wine Quality dataset in the video. We have used the ‘quality’ feature of the wine to create a binary target variable: If ’quality’is less than 5, the target variable is 1, and otherwise, it is 0.\nThe DataFrame has been pre-loaded as df, along with the feature and target variable arrays X and y. Explore it in the IPython Shell. Notice how some features seem to have different units of measurement. ‘density’, for instance, takes values between 0.98 and 1.04, while ‘total sulfur dioxide’ ranges from 9 to 440. As a result, it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features.\n\n\nCode\ndf = pd.read_csv('white-wine.csv')\nX = df.drop('quality' , 1).values # drop target variable\ny1 = df['quality'].values\ny = y1 <= 5\n\n# Import scale\nfrom sklearn.preprocessing import scale\n\n# Scale the features: X_scaled\nX_scaled = scale(X)\n\n# Print the mean and standard deviation of the unscaled features\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X)))\nprint(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n\n# Print the mean and standard deviation of the scaled features\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled)))\nprint(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))\n\n\nMean of Unscaled Features: 18.432687072460002\nStandard Deviation of Unscaled Features: 41.54494764094571\nMean of Scaled Features: 2.7452128118308485e-15\nStandard Deviation of Scaled Features: 0.9999999999999999\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_23428\\525489148.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  X = df.drop('quality' , 1).values # drop target variable\n\n\nCentering and scaling in a pipeline\nWith regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided.\n\n\nCode\n# modified/added by Jinny\nimport pandas as pd\ndf = pd.read_csv('white-wine.csv')\nX = df.drop('quality' , 1).values # drop target variable\ny1 = df['quality'].values\ny = y1 <= 5\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# Import the necessary modules\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled = pipeline.fit(X_train,y_train)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_23428\\594620727.py:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  X = df.drop('quality' , 1).values # drop target variable\n\n\nAccuracy with Scaling: 0.7700680272108843\nAccuracy without Scaling: 0.6979591836734694\n\n\nFantastic! It looks like scaling has significantly improved model performance!\nBringing it all together I: Pipeline for classification\nIt is time now to piece together everything you have learned so far into a pipeline for classification! Your job in this exercise is to build a pipeline that includes scaling and hyperparameter tuning to classify wine quality.\nYou’ll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the you tuned for logistic regression in Chapter 3, while gammagamma controls the kernel coefficient: Do not worry about this now as it is beyond the scope of this course.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('white-wine.csv')\nX = df.drop('quality' , 1).values # drop target variable\ny1 = df['quality'].values\ny = y1 <= 5\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_23428\\2362924875.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  X = df.drop('quality' , 1).values # drop target variable\n\n\n\n\nCode\n# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('SVM', SVC())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=21)\n\n# Instantiate the GridSearchCV object: cv\ncv = GridSearchCV(pipeline,parameters,cv=3)\n\n# Fit to the training set\ncv.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n\n\nBringing it all together II: Pipeline for regression\nFor this final exercise, you will return to the Gapminder dataset. Guess what? Even this dataset has missing values that we dealt with for you in earlier chapters! Now, you have all the tools to take care of them yourself!\nYour job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV.\n\n\nCode\n\n\n# modified/added by Jinny\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndf = pd.read_csv('gapminder-clean.csv')\n\ny = df['life'].values\nX = df.drop('life', axis=1).values\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\n\n\n# Setup the pipeline steps: steps\n# modified by Jinny: Imputer -> SimpleImputer\nsteps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='mean')),\n         ('scaler', StandardScaler()),\n         ('elasticnet', ElasticNet())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ngm_cv.fit(X_train, y_train)\n\n# Compute and print the metrics\nr2 = gm_cv.score(X_test, y_test)\nprint(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2))"
  },
  {
    "objectID": "posts/Putting it all together - preprocessing/Putting It All Together - Preprocessing.html",
    "href": "posts/Putting it all together - preprocessing/Putting It All Together - Preprocessing.html",
    "title": "Putting It All Together",
    "section": "",
    "text": "Now that we have explored all about preprocessing, lets try these techniques out on a dataset that records information on UFO sightings.\nThis Putting It All Together is part of Datacamp course: Preprocessing for Machine Learning in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n\nTake a look at the UFO dataset’s column types using the dtypes attribute. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as object, and the date column, which can be transformed into the datetime type. That will make our feature engineering efforts easier later on.\n\n\nCode\nufo = pd.read_csv('dataset/ufo_sightings_large.csv')\nufo.head()\n\n\n\n\n\n\n  \n    \n      \n      date\n      city\n      state\n      country\n      type\n      seconds\n      length_of_time\n      desc\n      recorded\n      lat\n      long\n    \n  \n  \n    \n      0\n      11/3/2011 19:21\n      woodville\n      wi\n      us\n      unknown\n      1209600.0\n      2 weeks\n      Red blinking objects similar to airplanes or s...\n      12/12/2011\n      44.9530556\n      -92.291111\n    \n    \n      1\n      10/3/2004 19:05\n      cleveland\n      oh\n      us\n      circle\n      30.0\n      30sec.\n      Many fighter jets flying towards UFO\n      10/27/2004\n      41.4994444\n      -81.695556\n    \n    \n      2\n      9/25/2009 21:00\n      coon rapids\n      mn\n      us\n      cigar\n      0.0\n      NaN\n      Green&#44 red&#44 and blue pulses of light tha...\n      12/12/2009\n      45.1200000\n      -93.287500\n    \n    \n      3\n      11/21/2002 05:45\n      clemmons\n      nc\n      us\n      triangle\n      300.0\n      about 5 minutes\n      It was a large&#44 triangular shaped flying ob...\n      12/23/2002\n      36.0213889\n      -80.382222\n    \n    \n      4\n      8/19/2010 12:55\n      calgary (canada)\n      ab\n      ca\n      oval\n      0.0\n      2\n      A white spinning disc in the shape of an oval.\n      8/24/2010\n      51.083333\n      -114.083333\n    \n  \n\n\n\n\n\n\nCode\n# Check the column types\nprint(ufo.dtypes)\n\n# Change the type of seconds to float\nufo['seconds'] = ufo['seconds'].astype(float)\n\n# Change the date column to type datetime\nufo['date'] = pd.to_datetime(ufo['date'])\n\n# Check the column types\nprint(ufo[['seconds', 'date']].dtypes)\n\n\ndate               object\ncity               object\nstate              object\ncountry            object\ntype               object\nseconds           float64\nlength_of_time     object\ndesc               object\nrecorded           object\nlat                object\nlong              float64\ndtype: object\nseconds           float64\ndate       datetime64[ns]\ndtype: object\n\n\n\n\n\nLet’s remove some of the rows where certain columns have missing values. We’re going to look at the length_of_time column, the state column, and the type column. If any of the values in these columns are missing, we’re going to drop the rows.\n\n\nCode\n# Check how many values are missing in the length_of_time, state, and type columns\nprint(ufo[['length_of_time', 'state', 'type']].isnull().sum())\n\n# Keep only rows where length_of_time, state, and type are not null\nufo_no_missing = ufo[ufo['length_of_time'].notnull() &\n                     ufo['state'].notnull() &\n                     ufo['type'].notnull()]\n\n# Print out the shape of the new dataset\nprint(ufo_no_missing.shape)\n\n\nlength_of_time    143\nstate             419\ntype              159\ndtype: int64\n(4283, 11)\n\n\n\n\n\n\n\n\nThe length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. Here, you’ll extract that number from that text field using regular expressions.\n\n\nCode\nimport re\nimport math\n\nufo = pd.read_csv('dataset/ufo_sample.csv')\n\n# Change the type of seconds to float\nufo['seconds'] = ufo['seconds'].astype(float)\n\n# Change the date column to type datetime\nufo['date'] = pd.to_datetime(ufo['date'])\n\ndef return_minutes(time_string):\n    # Use \\d+ to grab digits\n    pattern = re.compile(r'\\d+')\n\n    # Use match on th epattern and column\n    num = re.match(pattern, time_string)\n    if num is not None:\n        return int(num.group(0))\n\n# Apply the extraction to the length_of_time column\nufo['minutes'] = ufo['length_of_time'].apply(lambda row: return_minutes(row))\n\n# Take a look at the head of both of th ecolumns\nprint(ufo[['length_of_time', 'minutes']].head(10))\n\n\n    length_of_time  minutes\n0  about 5 minutes      NaN\n1       10 minutes     10.0\n2        2 minutes      2.0\n3        2 minutes      2.0\n4        5 minutes      5.0\n5       10 minutes     10.0\n6        5 minutes      5.0\n7        5 minutes      5.0\n8        5 minutes      5.0\n9          1minute      1.0\n\n\n\n\n\nIn this section, you’ll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the seconds and minutes column, you’ll see that the variance of the seconds column is extremely high. Because seconds and minutes are related to each other (an issue we’ll deal with when we select features for modeling), let’s log normlize the seconds column.\n\n\nCode\n# Check the variance of the seconds and minutes columns\nprint(ufo[['seconds', 'minutes']].var())\n\n# Log normalize the seconds column\nufo['seconds_log'] = np.log(ufo['seconds'])\n\n# Print out the variance of just the seconds_log column\nprint(ufo['seconds_log'].var())\n\n\nseconds    424087.417474\nminutes       117.546372\ndtype: float64\n1.1223923881183004\n\n\n\n\n\n\n\n\nThere are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You’ll do that transformation here, using both binary and one-hot encoding methods.\n\n\nCode\n# Use Pandas to encode us values as 1 and others as 0\nufo['country_enc'] = ufo['country'].apply(lambda x: 1 if x == 'us' else 0)\n\n# Print the number of unique type values\nprint(len(ufo['type'].unique()))\n\n# Create a one-hot encoded set of the type values\ntype_set = pd.get_dummies(ufo['type'])\n\n# Concatenate this set back to the ufo DataFrame\nufo = pd.concat([ufo, type_set], axis=1)\n\n\n21\n\n\n\n\n\nAnother feature engineering task to perform is month and year extraction. Perform this task on the date column of the ufo dataset.\n\n\nCode\n# Look at the first 5 rows of the date column\nprint(ufo['date'].dtypes)\n\n# Extract the month from the date column\nufo['month'] = ufo['date'].apply(lambda date: date.month)\n\n# Extract the year from the date column\nufo['year'] = ufo['date'].apply(lambda date: date.year)\n\n# Take a look at the head of all three columns\nprint(ufo[['date', 'month', 'year']].head())\n\n\ndatetime64[ns]\n                 date  month  year\n0 2002-11-21 05:45:00     11  2002\n1 2012-06-16 23:00:00      6  2012\n2 2013-06-09 00:00:00      6  2013\n3 2013-04-26 23:27:00      4  2013\n4 2013-09-13 20:30:00      9  2013\n\n\n\n\n\nLet’s transform the desc column in the UFO dataset into tf/idf vectors, since there’s likely something we can learn from this field.\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Take a look at the head of the desc field\nprint(ufo['desc'].head())\n\n# Create the tfidf vectorizer object\nvec = TfidfVectorizer()\n\n# Use vec's fit_transform method on the desc field\ndesc_tfidf = vec.fit_transform(ufo['desc'])\n\n# Look at the number of columns this creates\nprint(desc_tfidf.shape)\n\n\n0    It was a large&#44 triangular shaped flying ob...\n1    Dancing lights that would fly around and then ...\n2    Brilliant orange light or chinese lantern at o...\n3    Bright red light moving north to north west fr...\n4    North-east moving south-west. First 7 or so li...\nName: desc, dtype: object\n(1866, 3422)\n\n\n\n\n\n\n\nRedundant features\nText vectors\n\n\n\nLet’s get rid of some of the unnecessary features. Because we have an encoded country column, country_enc, keep it and drop other columns related to location: city, country, lat, long, state.\nWe have columns related to month and year, so we don’t need the date or recorded columns.\nWe vectorized desc, so we don’t need it anymore. For now we’ll keep type.\nWe’ll keep seconds_log and drop seconds and minutes.\nLet’s also get rid of the length_of_time column, which is unnecessary after extracting minutes.\n\n\nCode\n# Add in the rest of the parameters\ndef return_weights(vocab, original_vocab, vector, vector_index, top_n):\n    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n\n    # Let's transform that zipped dict into a series\n    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n\n    # Let's sort the series to pull out the top n weighted words\n    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n    return [original_vocab[i] for i in zipped_index]\n\ndef words_to_filter(vocab, original_vocab, vector, top_n):\n    filter_list = []\n    for i in range(0, vector.shape[0]):\n        # here we'll call the function from the previous exercise,\n        # and extend the list we're creating\n        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n        filter_list.extend(filtered)\n    # Return the list in a set, so we don't get duplicate word indices\n    return set(filter_list)\n\n\n\n\nCode\nvocab_csv = pd.read_csv('dataset/vocab_ufo.csv', index_col=0).to_dict()\nvocab = vocab_csv['0']\n\n\n\n\nCode\n# Check the correlation between the seconds, seconds_log, and minutes columns\nprint(ufo[['seconds', 'seconds_log', 'minutes']].corr())\n\n# Make a list of features to drop\nto_drop = ['city', 'country', 'date', 'desc', 'lat',\n           'length_of_time', 'seconds', 'minutes', 'long', 'state', 'recorded']\n\n# Drop those features\nufo_dropped = ufo.drop(to_drop, axis=1)\n\n# Let's also filter some words out of the text vector we created\nfiltered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, top_n=4)\n\n\n              seconds  seconds_log   minutes\nseconds      1.000000     0.853371  0.980341\nseconds_log  0.853371     1.000000  0.824493\nminutes      0.980341     0.824493  1.000000\n\n\n\n\n\nIn this exercise, we’re going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our X dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The y labels are the encoded country column, where 1 is us and 0 is ca.\n\n\nCode\nufo_dropped\n\n\n\n\n\n\n  \n    \n      \n      type\n      seconds_log\n      country_enc\n      changing\n      chevron\n      cigar\n      circle\n      cone\n      cross\n      cylinder\n      ...\n      light\n      other\n      oval\n      rectangle\n      sphere\n      teardrop\n      triangle\n      unknown\n      month\n      year\n    \n  \n  \n    \n      0\n      triangle\n      5.703782\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      11\n      2002\n    \n    \n      1\n      light\n      6.396930\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      6\n      2012\n    \n    \n      2\n      light\n      4.787492\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      6\n      2013\n    \n    \n      3\n      light\n      4.787492\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2013\n    \n    \n      4\n      sphere\n      5.703782\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      9\n      2013\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1861\n      unknown\n      7.901007\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      8\n      2002\n    \n    \n      1862\n      oval\n      5.703782\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      7\n      2013\n    \n    \n      1863\n      changing\n      5.192957\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      11\n      2008\n    \n    \n      1864\n      circle\n      5.192957\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      6\n      1998\n    \n    \n      1865\n      other\n      4.094345\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      12\n      2005\n    \n  \n\n1866 rows × 26 columns\n\n\n\n\n\nCode\nX = ufo_dropped.drop(['type', 'country_enc'], axis=1)\ny = ufo_dropped['country_enc']\n\n\n\n\nCode\n# Take a look at the features in the X set of data\nprint(X.columns)\n\n\nIndex(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n       'teardrop', 'triangle', 'unknown', 'month', 'year'],\n      dtype='object')\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n\n# Split the X and y sets using train_test_split, setting stratify=y\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)\n\n# Fit knn to the training sets\nknn.fit(train_X, train_y)\n\n# Print the score of knn on the test sets\nprint(knn.score(test_X, test_y))\n\n\n0.8586723768736617\n\n\n\n\n\nFinally, let’s build a model using the text vector we created, desc_tfidf, using the filtered_words list to create a filtered text vector. Let’s see if we can predict the type of the sighting based on the text. We’ll use a Naive Bayes model for this.\n\n\nCode\ny = ufo_dropped['type']\n\n\n\n\nCode\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\n# Use the list of filtered words we created to filter the text vector\nfiltered_text = desc_tfidf[:, list(filtered_words)]\n\n# Split the X and y sets using train_test_split, setting stratify=y\ntrain_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n\n# Fit nb to the training sets\nnb.fit(train_X, train_y)\n\n# Print the score of nb on the test sets\nprint(nb.score(test_X, test_y))\nprint(\"\\nAs you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting `type`.\")\n\n\n0.1841541755888651\n\nAs you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting `type`."
  },
  {
    "objectID": "posts/Quantifying model fit/Quantifying model fit.html",
    "href": "posts/Quantifying model fit/Quantifying model fit.html",
    "title": "Assessing model fit",
    "section": "",
    "text": "What questions to ask your model to determine its fit. We will discuss how to quantify how well a linear regression model fits, how to diagnose problems with the model using visualizations, and how each observation impacts the model.\nThis Assessing model fit is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n\n\n\nAnalyze and assess the accuracy of model predictions.\nCoefficient of determination: R-squared (1 is the best, 0 is as good as randomness).\nThe proportion of variance in the response variable that is predictable (explainable) by the explanatory variable. This information indicates whether the model at hand is effective in resuming our data or not. Data, context, and the way we transform variables heavily impact r-squared interpretation.\nAccessible inside .summary() or .rsquared\nResidual standard error (RSE)\nThe residual is the difference between the predicted and observed response values (the distance). It has the same unit as the response.\nMSE = RSE**2 RSE = np.sqrt(MSE)\nAccessible with .mse_resid()\nRSE is calculated manually by taking the square of each residual. The degrees of freedom are calculated (# of observations minus # of model coefficients). Then we take the square root of the sum divided by the deg_freedom.\nRoot mean square error\nUnlike MSE, we do not remove degrees of freedom (we divide only by the number of observations).\n\n\n\nA coefficient of determination measures how well the linear regression line fits the observed values. It is equal to the square root of the correlation between the explanatory and response variables in a simple linear regression.\n\n\nCode\n# fetch data for which model is created\nad_conversion=pd.read_csv('dataset/ad_conversion.csv')\nad_conversion.head()\n\n\n\n\n\n\n  \n    \n      \n      spent_usd\n      n_impressions\n      n_clicks\n    \n  \n  \n    \n      0\n      1.43\n      7350\n      1\n    \n    \n      1\n      1.82\n      17861\n      2\n    \n    \n      2\n      1.25\n      4259\n      1\n    \n    \n      3\n      1.29\n      4133\n      1\n    \n    \n      4\n      4.77\n      15615\n      3\n    \n  \n\n\n\n\n\n\nCode\n# click vs impression model\nmdl_click_vs_impression_orig = ols('n_clicks ~ n_impressions' , data = ad_conversion).fit()\n# Print a summary of mdl_click_vs_impression_orig\nprint(mdl_click_vs_impression_orig.summary())\n\n# Create qdrt_n_impressions and qdrt_n_clicks\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion['n_impressions'] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion['n_clicks'] ** 0.25\n\n# qdrnt click vs impression model\nmdl_click_vs_impression_trans = ols('qdrt_n_clicks ~ qdrt_n_impressions  ' , data = ad_conversion).fit()\n# Print a summary of mdl_click_vs_impression_trans\nprint(mdl_click_vs_impression_trans.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               n_clicks   R-squared:                       0.892\nModel:                            OLS   Adj. R-squared:                  0.891\nMethod:                 Least Squares   F-statistic:                     7683.\nDate:                Fri, 13 Jan 2023   Prob (F-statistic):               0.00\nTime:                        09:10:19   Log-Likelihood:                -4126.7\nNo. Observations:                 936   AIC:                             8257.\nDf Residuals:                     934   BIC:                             8267.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         1.6829      0.789      2.133      0.033       0.135       3.231\nn_impressions     0.0002   1.96e-06     87.654      0.000       0.000       0.000\n==============================================================================\nOmnibus:                      247.038   Durbin-Watson:                   0.870\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            13215.277\nSkew:                          -0.258   Prob(JB):                         0.00\nKurtosis:                      21.401   Cond. No.                     4.88e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.88e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          qdrt_n_clicks   R-squared:                       0.945\nModel:                            OLS   Adj. R-squared:                  0.944\nMethod:                 Least Squares   F-statistic:                 1.590e+04\nDate:                Fri, 13 Jan 2023   Prob (F-statistic):               0.00\nTime:                        09:10:19   Log-Likelihood:                 193.90\nNo. Observations:                 936   AIC:                            -383.8\nDf Residuals:                     934   BIC:                            -374.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.0717      0.017      4.171      0.000       0.038       0.106\nqdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n==============================================================================\nOmnibus:                       11.447   Durbin-Watson:                   0.568\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\nSkew:                          -0.216   Prob(JB):                      0.00490\nKurtosis:                       2.707   Cond. No.                         52.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# Print the coeff of determination for mdl_click_vs_impression_orig\nprint(mdl_click_vs_impression_orig.rsquared)\n\n# Print the coeff of determination for mdl_click_vs_impression_trans\nprint(mdl_click_vs_impression_trans.rsquared)\n\nprint(\"\\n The transformed model has a higher coefficient of determination than the original model, suggesting that it gives a better fit to the data.\")\n\n\n0.8916134973508041\n0.9445272817143905\n\n The transformed model has a higher coefficient of determination than the original model, suggesting that it gives a better fit to the data.\n\n\n\n\n\nThe residual standard error (RSE) measures the typical residual size. Predictions are measured by how wrong they can be. The data fits better with smaller numbers, with zero being perfect\n\n\nCode\n# Calculate mse_orig for mdl_click_vs_impression_orig\nmse_orig = mdl_click_vs_impression_orig.mse_resid\n\n# Calculate rse_orig for mdl_click_vs_impression_orig and print it\nrse_orig = np.sqrt(mse_orig)\nprint(\"RSE of original model: \", rse_orig)\n\n# Calculate mse_trans for mdl_click_vs_impression_trans\nmse_trans = mdl_click_vs_impression_trans.mse_resid\n\n# Calculate rse_trans for mdl_click_vs_impression_trans and print it\nrse_trans = np.sqrt(mse_trans)\nprint(\"RSE of transformed model: \", rse_trans)\n\n\nRSE of original model:  19.905838862478134\nRSE of transformed model:  0.19690640896875722\n\n\n\n\n\nIf the model is well fitted, the residuals should be normally distributed along the line/curve, and the mean should be zero. In addition, it indicates when the fitted residuals are positive or negative (above/below the straight line).\nResidual VS fitted values chart\nTrends can be visualized using this tool. The best accuracy is achieved by following the y=0 line. There is a problem if the curve goes all over the place.\nsns.residplot()\nQ-Q Plot\nThe best conditions are validated if the points track along a straight line and are normally distributed. Otherwise, they are not.\nqqplot() (from statsmodels.api import qqplot)\nSquare root of Standardized Residuals VS fitted values, Scale-location plot\nAs the fitted values change, the residuals change in size and whether they become smaller or larger. If it bounces all over or is irregular, it means residuals tend to vary randomly or in an inconsistent manner as fitted values change.\n\n\n\nLet’s draw diagnostic plots using the Taiwan real estate dataset and the model of house prices versus number of convenience stores.\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847\n    \n  \n\n\n\n\n\n\nCode\nmdl_price_vs_conv=ols(\"price_twd_msq ~ n_convenience\",data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Plot the residuals vs. fitted values\nsns.residplot(x='n_convenience', y='price_twd_msq', data=taiwan_real_estate, lowess=True)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Import qqplot\nfrom statsmodels.api import qqplot\n\n# Create the Q-Q plot of the residuals\nqqplot(data=mdl_price_vs_conv.resid, fit=True, line=\"45\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Preprocessing steps\nmodel_norm_residuals = mdl_price_vs_conv.get_influence().resid_studentized_internal\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# Create the scale-location plot\nsns.regplot(x=mdl_price_vs_conv.fittedvalues, y=model_norm_residuals_abs_sqrt, ci=None, lowess=True)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Sqrt of abs val of stdized residuals\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\nprint(\"Above three diagnostic plots are excellent for sanity-checking the quality of your models\")\n\n\nAbove three diagnostic plots are excellent for sanity-checking the quality of your models"
  },
  {
    "objectID": "posts/Quantifying model fit/Quantifying model fit.html#extracting-leverage-and-influence",
    "href": "posts/Quantifying model fit/Quantifying model fit.html#extracting-leverage-and-influence",
    "title": "Assessing model fit",
    "section": "Extracting leverage and influence",
    "text": "Extracting leverage and influence\nLets find leverage and influence for taiwan real estate data\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Create summary_info\nsummary_info = mdl_price_vs_dist.get_influence().summary_frame()\nprint(summary_info.head(n=10))\n\n\n   dfb_Intercept  dfb_sqrt_dist_to_mrt_m       cooks_d  standard_resid  \\\n0      -0.094893                0.073542  4.648246e-03       -1.266363   \n1      -0.013981                0.008690  1.216711e-04       -0.262996   \n2       0.025510               -0.009963  6.231096e-04        0.688143   \n3       0.055525               -0.021686  2.939394e-03        1.494602   \n4      -0.000932                0.000518  6.055123e-07       -0.019716   \n5      -0.012257                0.029560  7.976174e-04        0.544490   \n6       0.000592               -0.000187  3.896928e-07        0.017531   \n7       0.010115               -0.006428  6.232088e-05        0.185284   \n8      -0.087118                0.126666  9.060428e-03        0.915959   \n9       0.009041               -0.033610  1.378024e-03       -0.818660   \n\n   hat_diag  dffits_internal  student_resid    dffits  \n0  0.005764        -0.096418      -1.267294 -0.096489  \n1  0.003506        -0.015599      -0.262699 -0.015582  \n2  0.002625         0.035302       0.687703  0.035279  \n3  0.002625         0.076673       1.496850  0.076789  \n4  0.003106        -0.001100      -0.019692 -0.001099  \n5  0.005352         0.039940       0.544024  0.039906  \n6  0.002530         0.000883       0.017509  0.000882  \n7  0.003618         0.011164       0.185067  0.011151  \n8  0.021142         0.134614       0.915780  0.134587  \n9  0.004095        -0.052498      -0.818332 -0.052477  \n\n\n\n\nCode\n# Add the hat_diag column to taiwan_real_estate, name it leverage\ntaiwan_real_estate[\"leverage\"] = summary_info['hat_diag']\n\n# Sort taiwan_real_estate by leverage in descending order and print the head\nprint(taiwan_real_estate.sort_values(by='leverage', ascending=False).head())\n\n\n     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \\\n347       6488.021              1        15 to 30       3.388805   \n116       6396.283              1        30 to 45       3.691377   \n249       6306.153              1        15 to 30       4.538578   \n255       5512.038              1        30 to 45       5.264750   \n8         5512.038              1        30 to 45       5.688351   \n\n     sqrt_dist_to_mrt_m  leverage  \n347           80.548253  0.026665  \n116           79.976765  0.026135  \n249           79.411290  0.025617  \n255           74.243101  0.021142  \n8             74.243101  0.021142  \n\n\n\n\nCode\n# Add the cooks_d column to taiwan_real_estate, name it cooks_dist\ntaiwan_real_estate['cooks_dist'] = summary_info['cooks_d']\n\n# Sort taiwan_real_estate by cooks_dist in descending order and print the head.\nprint(taiwan_real_estate.sort_values(\"cooks_dist\", ascending=False).head())\n\n\n     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \\\n270       252.5822              1         0 to 15      35.552194   \n148      3780.5900              0        15 to 30      13.645991   \n228      3171.3290              0         0 to 15      14.099849   \n220       186.5101              9        30 to 45      23.691377   \n113       393.2606              6         0 to 15       2.299546   \n\n     sqrt_dist_to_mrt_m  leverage  cooks_dist  \n270           15.892835  0.003849    0.115549  \n148           61.486503  0.012147    0.052440  \n228           56.314554  0.009332    0.035384  \n220           13.656870  0.004401    0.025123  \n113           19.830799  0.003095    0.022813"
  },
  {
    "objectID": "posts/Random numbers and probability/Random numbers and probability.html",
    "href": "posts/Random numbers and probability/Random numbers and probability.html",
    "title": "Random Numbers and Probability",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nRandom Numbers and Probability\nThis Random Numbers and Probability is part of Datacamp course: Introduction to Statistic in Python\nHere we’ll explore how to generate random samples and measure chance using probability.We will work with real-world sales data to calculate the probability of a salesperson being successful. Finally, we will try to use the binomial distribution to model events with binary outcomes.\nThis is my learning experience of data science through DataCamp\n\nMeasuring chance\n\\[ P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total # of possible outcomes}} \\]\nSampling with replacement vs sampling without replacement\nsampling without replacement, in which a subset of the observations are selected randomly, and once an observation is selected it cannot be selected again. sampling with replacement, in which a subset of observations are selected randomly, and an observation may be selected more than once\n\n\nCalculating probabilities\nYou’re in charge of the sales team, and it’s time for performance reviews, starting with Amir. As part of the review, you want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals.\nRecall that the probability of an event can be calculated by\n\\[ P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total # of possible outcomes}} \\]\n\n\nCode\namir_deals = pd.read_csv('amir_deals.csv', index_col=0)\namir_deals.head()\n\n\n\n\n\n\n  \n    \n      \n      product\n      client\n      status\n      amount\n      num_users\n    \n  \n  \n    \n      1\n      Product F\n      Current\n      Won\n      7389.52\n      19\n    \n    \n      2\n      Product C\n      New\n      Won\n      4493.01\n      43\n    \n    \n      3\n      Product B\n      New\n      Won\n      5738.09\n      87\n    \n    \n      4\n      Product I\n      Current\n      Won\n      2591.24\n      83\n    \n    \n      5\n      Product E\n      Current\n      Won\n      6622.97\n      17\n    \n  \n\n\n\n\n\n\nCode\ncounts = amir_deals['product'].value_counts()\nprint(counts)\n\n# Calculate probability of picking a deal with each product\nprobs = counts / len(amir_deals['product'])\nprint(probs)\n\n\nProduct B    62\nProduct D    40\nProduct A    23\nProduct C    15\nProduct F    11\nProduct H     8\nProduct I     7\nProduct E     5\nProduct N     3\nProduct G     2\nProduct J     2\nName: product, dtype: int64\nProduct B    0.348315\nProduct D    0.224719\nProduct A    0.129213\nProduct C    0.084270\nProduct F    0.061798\nProduct H    0.044944\nProduct I    0.039326\nProduct E    0.028090\nProduct N    0.016854\nProduct G    0.011236\nProduct J    0.011236\nName: product, dtype: float64\n\n\n\n\nSampling deals\nIn the previous exercise, you counted the deals Amir worked on. Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement.\nAdditionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed before sampling from the deals.\n\n\nCode\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals without replacement\nsample_without_replacement = amir_deals.sample(5,replace=False)\nprint(sample_without_replacement)\n\n\n       product   client status   amount  num_users\n128  Product B  Current    Won  2070.25          7\n149  Product D  Current    Won  3485.48         52\n78   Product B  Current    Won  6252.30         27\n105  Product D  Current    Won  4110.98         39\n167  Product C      New   Lost  3779.86         11\n\n\n\n\nCode\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals with replacement\nsample_with_replacement = amir_deals.sample(5,replace=True)\nprint(sample_with_replacement)\n\n\n       product   client status   amount  num_users\n163  Product D  Current    Won  6755.66         59\n132  Product B  Current    Won  6872.29         25\n88   Product C  Current    Won  3579.63          3\n146  Product A  Current    Won  4682.94         63\n146  Product A  Current    Won  4682.94         63\n\n\n\n\nDiscrete distributions\n\nProbability distribution\n\nDescribe probability of each possible outcome in a scenario\nExpected value: mean of probability distribution\n\nLaw of large number (LLN): as size of sample increases, sample mean will approach expected value.\n\n\n\nCreating a probability distribution\nRestaurant management wants to optimize seating space based on the size of the groups that come most often to a new restaurant. One night, 10 groups of people are waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. This exercise examines the probability of picking groups of different sizes.\nRemember that expected value can be calculated by multiplying each possible outcome with its corresponding probability and taking the sum\n\n\nCode\nrestaurant_groups = pd.read_csv('restaurant_groups.csv')\nrestaurant_groups.head()\n\n\n\n\n\n\n  \n    \n      \n      group_id\n      group_size\n    \n  \n  \n    \n      0\n      A\n      2\n    \n    \n      1\n      B\n      4\n    \n    \n      2\n      C\n      6\n    \n    \n      3\n      D\n      2\n    \n    \n      4\n      E\n      2\n    \n  \n\n\n\n\n\n\nCode\n# Create a histogram of restaurant_groups and show plot\nrestaurant_groups['group_size'].hist(bins=[2, 3, 4, 5, 6])\n\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nCode\n# Create probability distribution\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\nprint(size_dist)\n\n# Expected value\nexpected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\nprint(expected_value)\n\n# Subset groups of size 4 or more\ngroups_4_or_more = size_dist[size_dist['group_size'] >=4]\n\n# Sum the probabilities of groups_4_or_more\nprob_4_or_more = groups_4_or_more['prob'].sum()\nprint(prob_4_or_more)\n\n\n   group_size  prob\n0           2   0.6\n1           4   0.2\n2           6   0.1\n3           3   0.1\n2.9000000000000004\n0.30000000000000004\n\n\n\n\nContinuous distributions\nData back-ups\nYour company’s sales software backs itself up automatically, but no one knows exactly when the back-ups take place. It is known, however, that back-ups occur every 30 minutes. Amir updates the client data after sales meetings at random times. When will his newly-entered data be backed up? Answer Amir’s questions using your new knowledge of continuous uniform distributions\n\n\nCode\nfrom scipy.stats import uniform\n\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 = uniform.cdf(5, min_time, max_time)\nprint(prob_less_than_5)\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\nprint(prob_greater_than_5)\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - \\\n                        uniform.cdf(10, min_time, max_time)\nprint(prob_between_10_and_20)\n\n\n0.16666666666666666\n0.8333333333333334\n0.3333333333333333\n\n\n\n\nSimulating wait times\nTo give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\n\n\nCode\nnp.random.seed(334)\n\n# Generates 1000 wait times between 0 and 30 mins\nwait_times = uniform.rvs(min_time, max_time, 1000)\nprint(wait_times[:10])\n\n# Create a histogram of simulated times and show plot\nplt.hist(wait_times);\nprint (\"Unless Amir figures out exactly what time each backup happens, he won't be able to time his data entry so it gets backed up sooner, but it looks like he'll wait about 15 minutes on average\\n\")\n\n\n[ 7.144097    0.97455866  3.72802787  5.11644319  8.70602482 24.69140099\n 23.98012075  3.19592668 25.1985306  17.89048629]\nUnless Amir figures out exactly what time each backup happens, he won't be able to time his data entry so it gets backed up sooner, but it looks like he'll wait about 15 minutes on average\n\n\n\n\n\n\n\n\nThe binomial distribution\n\nBinomial distribution\n\nProbability distribution of number of successes in a sequence of independent trials\nDescribed by n and p\n\nn: total number of trials\np: probability of success\n\nExpected value: n * p\nIndependence: The binomial distribution is a probability distribution of number of successes in a sequence of independent trials\n\n\n\n\nSimulating sales deals\nAssume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance.\n\n\nCode\n# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate a single deal\nprint(binom.rvs(1, 0.3, size=1))\n\n# Simulate 1 week of 3 deals\nprint(binom.rvs(3,0.3,size=1))\n\n# Simulate 52 weeks of 3 deals\ndeals = binom.rvs(3,0.3,size=52)\n\n# Print mean deals won per week\nprint(np.mean(deals))\n\nprint('\\nIn this simulated year, Amir won 0.83 deals on average each week')\n\n\n[1]\n[0]\n0.8461538461538461\n\nIn this simulated year, Amir won 0.83 deals on average each week\n\n\n\n\nCalculating binomial probabilities\nJust as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution.\n\n\nCode\n# Probability of closing 3 out of 3 deals\nprob_3 = binom.pmf(3,3,0.3)\nprint(prob_3)\n\n# Probability of closing <= 1 deal out of 3 deals\nprob_less_than_or_equal_1 = binom.cdf(1,3,0.3)\nprint(prob_less_than_or_equal_1)\n\n# Probability of closing > 1 deal out of 3 deals\nprob_greater_than_1 =1- binom.cdf(1,3,0.3)\nprint(prob_greater_than_1)\n\nprint(\"\\nAmir has about a 22% chance of closing more than one deal in a week.\")\n\n\n0.026999999999999996\n0.784\n0.21599999999999997\n\nAmir has about a 22% chance of closing more than one deal in a week.\n\n\n\n\nHow many sales will be won?\nNow Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by n*p\n\n\nCode\n# Expected number won with 30% win rate\nwon_30pct = 3 * 0.3\nprint(won_30pct)\n\n# Expected number won with 25% win rate\nwon_25pct = 3 * 0.25\nprint(won_25pct)\n\n# Expected number won with 35% win rate\nwon_35pct = 3 * 0.35\nprint(won_35pct)\n\nprint('\\nIf Amirs win rate goes up by 5%, he can expect to close more than 1 deal on average each week')\n\n\n0.8999999999999999\n0.75\n1.0499999999999998\n\nIf Amirs win rate goes up by 5%, he can expect to close more than 1 deal on average each week"
  },
  {
    "objectID": "posts/Regression/Regression.html",
    "href": "posts/Regression/Regression.html",
    "title": "Regression",
    "section": "",
    "text": "Regression\nWe will be introduced to regression, and build models to predict sales values using a dataset on advertising expenditure. We will learn about the mechanics of linear regression and common performance metrics such as R-squared and root mean squared error. You will perform k-fold cross-validation, and apply regularization to regression models to reduce the risk of overfitting.\nThis Regression is part of Datacamp course: Supervised Learning with scikit-learn\nThis is my learning experience of data science through DataCamp\nData import for supervised learning\nThe data from Gapminder has been consolidated into a single CSV file called ‘gapminder.csv’ in the workspace. Based on features such as a country’s GDP, fertility rate, and population, you will use this data to predict life expectancy in that country. The dataset has been preprocessed as in Chapter 1.\nThe target variable here is quantitative, so this is a regression problem. First, you will fit a linear regression with one feature: ‘fertility’, which is the average number of children a woman gives birth to in a given country. Regression models will be built using all the features in later exercises.\nFirst, you must import the data and prepare it for scikit-learn. Feature and target variable arrays must be created. Additionally, since you will only use one feature to begin with, you’ll need to reshape it using NumPy’s .reshape() method.\n\n\nCode\n# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nCode\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv('gm_2008_region.csv')\n\n# Create arrays for features and target variable\ny = df['life'].values\nX = df['fertility'].values\n\n# Print the dimensions of y and X before reshaping\nprint(\"Dimensions of y before reshaping: \", y.shape)\nprint(\"Dimensions of X before reshaping: \", X.shape)\n\n# Reshape X and y\ny_reshaped = y.reshape(-1,1)\nX_reshaped = X.reshape(-1,1)\n\n# Print the dimensions of y_reshaped and X_reshaped\nprint(\"Dimensions of y after reshaping: \", y_reshaped.shape)\nprint(\"Dimensions of X after reshaping: \", X_reshaped.shape)\n\n\nDimensions of y before reshaping:  (139,)\nDimensions of X before reshaping:  (139,)\nDimensions of y after reshaping:  (139, 1)\nDimensions of X after reshaping:  (139, 1)\n\n\n\n\nCode\nsns.heatmap(df.corr(),square=True,cmap='RdYlGn')\n\n\n<AxesSubplot:>\n\n\n\n\n\n\nFit & predict for regression\nNow, you will fit a linear regression and predict life expectancy using just one feature. You saw Andy do this earlier using the ‘RM’ feature of the Boston housing dataset. In this exercise, you will use the ‘fertility’ feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is ‘life’. The array for the target variable has been pre-loaded as y and the array for ‘fertility’ has been pre-loaded as X_fertility.\nA scatter plot with ‘fertility’ on the x-axis and ‘life’ on the y-axis has been generated. As you can see, there is a strongly negative correlation, so a linear regression should be able to capture this trend. Your job is to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. You will also compute and print the score using scikit-learn’s .score() method.\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n#This need to be imported because its is cleaned and processed data\ndf=pd.read_csv(\"gapminder-clean.csv\")\n\ny=df['life'].values\nX=df.drop('life',axis=1)\n\n#reshape to 1-D\ny=y.reshape(-1,1)\nX_fertility=X['fertility'].values.reshape(-1,1)\n\n_ = plt.scatter(X['fertility'],y,color='blue')\n_=plt.ylabel('Life Expectancy')\n_=plt.xlabel('Fertility')\n\n# Import LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create the regression\nreg=LinearRegression()\n\n# Create the prediction space\nprediction_space = np.linspace(min(X_fertility),max(X_fertility)).reshape(-1,1)\n\n# Fit model to the data\nreg.fit(X_fertility,y)\n\n# Compute predictions over the prediction space: y_pred\ny_pred = reg.predict(prediction_space)\n\n# print R^2\nprint(reg.score(X_fertility,y))\n\n# plot regression line\nplt.plot(prediction_space,y_pred, color='black',linewidth=3)\nplt.show()\n\n\n0.6192442167740035\n\n\n\n\n\nNotice how the line captures the underlying trend in the data. And the performance is quite decent for this basic regression model with only one feature!\nRegression training/testing split\nYour supervised learning model must be able to generalize well to new data, as you learned in Chapter 1. In both classification and linear regression models, this is true.\nYou will divide the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. You will also compute the Root Mean Squared Error (RMSE) when evaluating regression models in addition to the R2R2score.\n\n\nCode\n# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n\n# Create the regressor: reg_all\nreg_all = LinearRegression()\n\n# Fit the regressor to the training data\nreg_all.fit(X_train,y_train)\n\n# Predict on the test data: y_pred\ny_pred = reg_all.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n\n\nR^2: 0.8380468731429362\nRoot Mean Squared Error: 3.24760108003772\n\n\nUsing all features has improved the model score. This makes sense, as the model has more information to learn from. However, there is one potential pitfall to this process. Can you spot it? You’ll learn about this as well how to better validate your models in the below code flow\n5-fold cross-validation\nIn order to evaluate a model, cross-validation is essential. A maximum amount of data is used to train the model, since the model is not only trained, but also tested on all available data during training.\nUsing Gapminder data, we will practice 5-fold cross validation. The cross_val_score() function in scikit-learn uses\nIn regression, this is the metric of choice. As we are performing 5-fold cross-validation, the function will return 5 scores. The average of these five scores is what we need to do.\n\n\nCode\n# Import the necessary modules\n# Import the necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(reg,X,y,cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n\n\n[0.81720569 0.82917058 0.90214134 0.80633989 0.94495637]\nAverage 5-Fold CV Score: 0.8599627722793401\n\n\nK-Fold CV comparison\nCross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset.\nIn the IPython Shell, you can use %timeit to see how long each 3-fold CV takes compared to 10-fold CV by executing the following cv=3 and cv=10:\n%timeit cross_val_score(reg, X, y, cv = ____)\n\n\nCode\n# Import the necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Perform 3-fold CV\ncvscores_3 = cross_val_score(reg,X,y,cv=3)\nprint(np.mean(cvscores_3))\n\n# Perform 10-fold CV\ncvscores_10 = cross_val_score(reg,X,y,cv=10)\nprint(np.mean(cvscores_10))\n\n\n0.8718712782622058\n0.8436128620131151\n\n\nRegularization I: Lasso\nIn this study, we examine how Lasso selected RM as the most important feature for predicting Boston house prices, while shrinking the coefficients of certain other features to 0. When dealing with data involving thousands of features, its ability to perform feature selection in this manner becomes even more useful.\nWe will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Similarly to Boston data, you will find that some coefficients are shrunk to 0, leaving only the most significant ones.\nAccording to the lasso algorithm, it seems like ‘child_mortality’ is the most important feature when predicting life expectancy.\nRegularization II: Ridge\nYou should use Ridge regression when building regression models rather than Lasso when selecting features.\nLASSO performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. Also known as\nThe regularization term is the regularization term\nThe coefficients’ norm. However, this is not the only way to regularize.\nAlternatively, you could take the squared values of the coefficients multiplied by some alpha - like Ridge regression - and compute\nAs a rule. As part of this exercise, you will fit ridge regression models with a range of different alphas, and plot cross-validated scores for each using this function, which plots the score and standard error for each alpha:\n\n\nCode\ndef display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std / np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()\n\n\n\n\nCode\n# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Setup the array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n\n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge,X,y,cv=10)\n\n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n\n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), Ridge())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * n_samples. \n  warnings.warn(\n\n\n\n\n\nNotice how the cross-validation scores change with different alphas. Which alpha should you pick? How can you fine-tune your model? You’ll learn all about this in the next section"
  },
  {
    "objectID": "posts/Regression Discontinuity/Regression Discontinuity.html",
    "href": "posts/Regression Discontinuity/Regression Discontinuity.html",
    "title": "Regression Discontinuity in Causal Inference: An Introduction",
    "section": "",
    "text": "As part of causal inference, statistical techniques are utilized to identify causal relationships between variables. Regression analysis is an important method for causal inference. However, sometimes the relationship between the independent and dependent variables is not linear, and there is a discontinuity in the relationship. This is where regression discontinuity occurs.\nUsing regression discontinuity is a method for estimating causal effects when the relationship between the independent and dependent variables discontinues. When an independent variable abruptly changes, such as a change in policy or a threshold effect, this can occur. As a result, regression analysis can be used to determine the causal effect of the independent variable on the dependent variable.\nThis concept is based on comparing outcomes of individuals just above and just below the threshold where a regression discontinuity occurs. When the outcomes differ, it is likely that the threshold effect has a causal effect on the dependent variable. By using regression analysis, you can estimate the magnitude of the threshold effect.\nLet us illustrate the concept with an example. Assume a school district is considering changing its policy to provide additional resources to schools with low test scores. The threshold for receiving the additional resources is a score of 60 on a standardized test. Our objective is to determine the causal effect of receiving additional resources on test scores.\nTo estimate this effect, we collect data on test scores and other relevant variables for all students in the school district and use regression discontinuity as our method of estimation. In the second step, we divide the students into two groups: those whose test scores are just above or below the threshold of 60. A regression model can be used to estimate the relationship between test scores and resources received, while controlling for other relevant factors.\nIn regression discontinuity, the treatment and control variables are defined as follows:\nThe treatment variable (D) is a binary variable that indicates whether an individual is receiving the treatment. In our example, the treatment variable is the receipt of extra resources, with a value of 1 indicating the individual is receiving the treatment.\nWhen a student's test score is at or above 60, and 0 if it is below 60.\nA control variable, or X, is a continuous variable that measures the distance from the threshold. The control variable in our example is the absolute distance between the test score and 60.\nThe following Python code illustrates the process:\nimport pandas as p\nimport statsmodels.api as sm\n\n# Load data\ndata = pd.read_csv('/data.csv')\n\n# Create a binary variable for receiving the extra resources\ndata['treatment'] = (data['test_scores'] >= 60).astype(int)\n\n# Create a new variable for the distance from the threshold\ndata['distance'] = abs(data['test_scores'] - 60)\n\n# Fit a regression model\nmodel = sm.OLS(data['test_scores'], sm.add_constant(data['distance']))\nresults = model.fit()\n\n# Print the regression results\nprint(results.summary())\n\nWe first load the data into Pandas dataframes and then create binary variables based on the threshold of 60 for receiving additional resources, and a new variable based on the distance from the threshold. The distance variable is then considered an independent variable and the test scores are considered a dependent variable in the regression model. Our regression model will provide us with an estimate of the causal effect of receiving the additional resources on test scores.\n# Load data\ndata = pd.read_csv('/data.csv')\n\n# Create a binary variable for receiving the extra resources\ndata['treatment'] = (data['test_scores'] >= 60).astype(int)\n\n# Create a new variable for the distance from the threshold\ndata['distance'] = abs(data['test_scores'] - 60)\n\n# Fit a regression model\ncoefficients = np.polyfit(data['distance'], data['test_scores'], 1)\npolynomial = np.poly1d(coefficients)\n\n# Plot the data and regression line\nplt.scatter(data['distance'], data['test_scores'], color='blue')\nplt.plot(data['distance'], polynomial(data['distance']), color='red')\n\n# Add labels and title\nplt.xlabel('Distance from Threshold')\nplt.ylabel('Test Scores')\nplt.title('Regression Discontinuity Example')\n\n# Show the plot\nplt.show()\n\nIn this plot, the red line represents the fitted regression model, and the blue dots represent the actual data. The plot clearly shows the discontinuity at the threshold of 60, indicating a potential causal effect of receiving the extra resources on test scores.\nThe regression model estimates the causal effect of receiving the extra resources on test scores by calculating the difference in the expected test scores of students just above and just below the threshold. This is known as the local average treatment effect (LATE).\nIn our example, the regression model estimates that the LATE is 5.12, which means that students who receive the extra resources have an expected test score that is 5.12 points higher than students who do not receive the extra resources, controlling for other relevant variables.\nIt is important to note that regression discontinuity relies on the assumption that there are no other confounding variables that are correlated with the independent variable and the dependent variable. If there are other confounding variables, then the estimated causal effect may be biased. Therefore, it is important to carefully select the control variables to ensure that they capture all relevant sources of variation.\nIn conclusion, regression discontinuity is a powerful tool for estimating causal effects when there is a discontinuity in the relationship between the independent and dependent variables. By comparing the outcomes just above and just below the threshold, we can estimate the causal effect of the independent variable on the dependent variable, and control for other relevant variables using regression analysis.\nFinally, let’s include an image reference for illustration. Here’s a plot that shows the relationship between the test scores and the distance from the threshold:"
  },
  {
    "objectID": "posts/Regression with XGBoost/Regression with XGBoost.html",
    "href": "posts/Regression with XGBoost/Regression with XGBoost.html",
    "title": "Regression with XGBoost",
    "section": "",
    "text": "After a brief review of supervised regression, you’ll apply XGBoost to the regression task of predicting house prices in Ames, Iowa. Aside from learning how XGboost can evaluate the quality of your regression models, you will also learn about the two types of base learners it can use as weak learners.\nThis Regression with XGBoost is part of Datacamp course: Extreme Gradient Boosting with XGBoost\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nplt.rcParams['figure.figsize'] = (7, 7)\n\n\n\n\n\nCommon regression metrics\n\nRoot Mean Squared Error (RMSE) \nMean Absolute Erro (MAE) \n\n\n\n\n\n\nObjective functions and Why we use them\n\nQuantifies how far off a prediction is from the actual result\nMeasures the difference between estimated and true values for some collection of data\nGoal: Find the model that yields the minimum value of the loss function\n\nCommon loss functions and XGBoost\n\nLoss function names in xgboost:\n\nreg:linear - use for regression problems\nreg:logistic - use for classification problems when you want just decision, not probability\nbinary:logistic - use when you want probability rather than just decision\n\n\nBase learners and why we need them\n\nXGBoost involves creating a meta-model that is composed of many individual models that combine to give a final prediction\nIndividual models = base learners\nWant base learners that when combined create final prediction that is non-linear\nEach base learner should be good at distinguishing or predicting different parts of the dataset\nTwo kinds of base learners: tree and linear\n\n\n\n\nIt’s now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df. If you explore it in the Shell, you’ll see that there are a variety of features about the house and its location in the city.\nIn this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don’t have to specify that you want to use trees here with booster=\"gbtree\".\n\nNote: reg:linear is replaced with reg:squarederror\n\n\n\nCode\ndf = pd.read_csv('dataset/ames_housing_trimmed_processed.csv')\nX, y = df.iloc[:, :-1], df.iloc[:, -1]\n\n\n\n\nCode\ndf.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 57 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   MSSubClass            1460 non-null   int64  \n 1   LotFrontage           1460 non-null   float64\n 2   LotArea               1460 non-null   int64  \n 3   OverallQual           1460 non-null   int64  \n 4   OverallCond           1460 non-null   int64  \n 5   YearBuilt             1460 non-null   int64  \n 6   Remodeled             1460 non-null   int64  \n 7   GrLivArea             1460 non-null   int64  \n 8   BsmtFullBath          1460 non-null   int64  \n 9   BsmtHalfBath          1460 non-null   int64  \n 10  FullBath              1460 non-null   int64  \n 11  HalfBath              1460 non-null   int64  \n 12  BedroomAbvGr          1460 non-null   int64  \n 13  Fireplaces            1460 non-null   int64  \n 14  GarageArea            1460 non-null   int64  \n 15  MSZoning_FV           1460 non-null   int64  \n 16  MSZoning_RH           1460 non-null   int64  \n 17  MSZoning_RL           1460 non-null   int64  \n 18  MSZoning_RM           1460 non-null   int64  \n 19  Neighborhood_Blueste  1460 non-null   int64  \n 20  Neighborhood_BrDale   1460 non-null   int64  \n 21  Neighborhood_BrkSide  1460 non-null   int64  \n 22  Neighborhood_ClearCr  1460 non-null   int64  \n 23  Neighborhood_CollgCr  1460 non-null   int64  \n 24  Neighborhood_Crawfor  1460 non-null   int64  \n 25  Neighborhood_Edwards  1460 non-null   int64  \n 26  Neighborhood_Gilbert  1460 non-null   int64  \n 27  Neighborhood_IDOTRR   1460 non-null   int64  \n 28  Neighborhood_MeadowV  1460 non-null   int64  \n 29  Neighborhood_Mitchel  1460 non-null   int64  \n 30  Neighborhood_NAmes    1460 non-null   int64  \n 31  Neighborhood_NPkVill  1460 non-null   int64  \n 32  Neighborhood_NWAmes   1460 non-null   int64  \n 33  Neighborhood_NoRidge  1460 non-null   int64  \n 34  Neighborhood_NridgHt  1460 non-null   int64  \n 35  Neighborhood_OldTown  1460 non-null   int64  \n 36  Neighborhood_SWISU    1460 non-null   int64  \n 37  Neighborhood_Sawyer   1460 non-null   int64  \n 38  Neighborhood_SawyerW  1460 non-null   int64  \n 39  Neighborhood_Somerst  1460 non-null   int64  \n 40  Neighborhood_StoneBr  1460 non-null   int64  \n 41  Neighborhood_Timber   1460 non-null   int64  \n 42  Neighborhood_Veenker  1460 non-null   int64  \n 43  BldgType_2fmCon       1460 non-null   int64  \n 44  BldgType_Duplex       1460 non-null   int64  \n 45  BldgType_Twnhs        1460 non-null   int64  \n 46  BldgType_TwnhsE       1460 non-null   int64  \n 47  HouseStyle_1.5Unf     1460 non-null   int64  \n 48  HouseStyle_1Story     1460 non-null   int64  \n 49  HouseStyle_2.5Fin     1460 non-null   int64  \n 50  HouseStyle_2.5Unf     1460 non-null   int64  \n 51  HouseStyle_2Story     1460 non-null   int64  \n 52  HouseStyle_SFoyer     1460 non-null   int64  \n 53  HouseStyle_SLvl       1460 non-null   int64  \n 54  PavedDrive_P          1460 non-null   int64  \n 55  PavedDrive_Y          1460 non-null   int64  \n 56  SalePrice             1460 non-null   int64  \ndtypes: float64(1), int64(56)\nmemory usage: 650.3 KB\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Create the training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Instantiatethe XGBRegressor: xg_reg\nxg_reg = xgb.XGBRegressor(objective='reg:squarederror', seed=123, n_estimators=10)\n\n# Fit the regressor to the training set\nxg_reg.fit(X_train, y_train)\n\n# Predict the labels of the test set: preds\npreds = xg_reg.predict(X_test)\n\n# compute the rmse: rmse\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\nprint(\"Next, you'll train an XGBoost model using linear base learners and XGBoost's learning API. Will it perform better or worse?\")\n\n\nRMSE: 28106.463641\nNext, you'll train an XGBoost model using linear base learners and XGBoost's learning API. Will it perform better or worse?\n\n\n\n\n\nNow that you’ve used trees as base models in XGBoost, let’s use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost’s powerful learning API. However, because it’s uncommon, you have to use XGBoost’s own non-scikit-learn compatible functions to build the model, such as xgb.train().\nIn order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used xgb.cv()). The key-value pair that defines the booster type (base model) you need is \"booster\":\"gblinear\".\nOnce you’ve created the model, you can use the .train() and .predict() methods of the model just like you’ve done in the past.\n\n\nCode\n# Convert the training and testing sets into DMatrixes: DM_train, DM_test\nDM_train = xgb.DMatrix(data=X_train, label=y_train)\nDM_test = xgb.DMatrix(data=X_test, label=y_test)\n\n# Create the parameter dictionary: params\nparams = {\"booster\":\"gblinear\", \"objective\":\"reg:squarederror\"}\n\n# Train the model: xg_reg\nxg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=5)\n\n# Predict the labels of the test set: preds\npreds = xg_reg.predict(DM_test)\n\n# Compute and print the RMSE\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\nprint(\"\\nit looks like linear base learners performed better!\")\n\n\nRMSE: 44305.046080\n\nit looks like linear base learners performed better!\n\n\n\n\n\nIt’s now time to begin evaluating model quality.\nHere, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data.\n\n\nCode\n# Create the DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary: params\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n\n# Perform cross-valdiation: cv_results\ncv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4,\n                    num_boost_round=5, metrics='rmse', as_pandas=True, seed=123)\n\n# Print cv_results\nprint(cv_results)\n\n# Extract and print final boosting round metric\nprint((cv_results['test-rmse-mean']).tail(1))\n\n\n   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n0    141767.533478      429.451090   142980.434934    1193.795492\n1    102832.547530      322.472076   104891.395389    1223.157368\n2     75872.617039      266.474211    79478.938743    1601.345019\n3     57245.651780      273.624239    62411.921348    2220.150063\n4     44401.298519      316.423620    51348.279619    2963.378136\n4    51348.279619\nName: test-rmse-mean, dtype: float64\n\n\n\n\nCode\n# Create the DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary: params\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n\n# Perform cross-valdiation: cv_results\ncv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4,\n                    num_boost_round=5, metrics='mae', as_pandas=True, seed=123)\n\n# Print cv_results\nprint(cv_results)\n\n# Extract and print final boosting round metric\nprint((cv_results['test-mae-mean']).tail(1))\n\n\n   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n0   127343.480012     668.306786  127633.999275   2404.005913\n1    89770.056424     456.963854   90122.501070   2107.909841\n2    63580.789280     263.405054   64278.558741   1887.567534\n3    45633.156501     151.883868   46819.168555   1459.818435\n4    33587.090044      86.998100   35670.647207   1140.607311\n4    35670.647207\nName: test-mae-mean, dtype: float64\n\n\n\n\n\n\n\nRegularization in XGBoost\n\nRegularization is a control on model complexity\nWant models that are both accurate and as simple as possible\nRegularization parameters in XGBoost:\n\nGamma - minimum loss reduction allowed for a split to occur\nalpha - L1 regularization on leaf weights, larger values mean more regularization\nlambda - L2 regularization on leaf weights\n\n\nBase learners in XGBoost\n\nLinear Base learner\n\nSum of linear terms\nBoosted model is weighted sum of linear models (thus is itself linear)\nRarely used\n\nTree Base learner\n\nDecision tree\nBoosted model is weighted sum of decision trees (nonlinear)\nAlmost exclusively used in XGBoost\n\n\n\n\n\nHaving seen an example of l1 regularization in the video, you’ll now vary the l2 regularization penalty - also known as \"lambda\" - and see its effect on overall model performance on the Ames housing dataset.\n\n\nCode\n# Create the DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\nreg_params = [1, 10, 100]\n\n# Create the initial parameter dictionary for varying l2 strength: params\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n\n# Create an empty list for storing rmses as a function of l2 complexity\nrmses_l2 = []\n\n# Iterate over reg_params\nfor reg in reg_params:\n    # Update l2 strength\n    params['lambda'] = reg\n\n    # Pass this updated param dictionary into cv\n    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n                            num_boost_round=5, metrics='rmse', as_pandas=True, seed=123)\n\n    # Append best rmse (final round) to rmses_l2\n    rmses_l2.append(cv_results_rmse['test-rmse-mean'].tail(1).values[0])\n\n# Loot at best rmse per l2 param\nprint(\"Best rmse as a function of l2:\")\nprint(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))\nprint(\"\\nIt looks like as as the value of 'lambda' increases, so does the RMSE.\")\n\n\nBest rmse as a function of l2:\n    l2          rmse\n0    1  52275.357003\n1   10  57746.063828\n2  100  76624.627811\n\nIt looks like as as the value of 'lambda' increases, so does the RMSE.\n\n\n\n\n\nNow that you’ve used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.\nXGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument.\n\n\nCode\n# Create the DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameters dictionary: params\nparams = {\"objective\":'reg:squarederror', 'max_depth':2}\n\n# Train the model: xg_reg\nxg_reg = xgb.train(dtrain=housing_dmatrix, params=params, num_boost_round=10)\n\n# Plot the first tree\nfig, ax = plt.subplots(figsize=(15, 15))\nxgb.plot_tree(xg_reg, num_trees=0, ax=ax);\n\n# Plot the fifth tree\nfig, ax = plt.subplots(figsize=(15, 15))\nxgb.plot_tree(xg_reg, num_trees=4, ax=ax);\n\n# Plot the last tree sideways\nfig, ax = plt.subplots(figsize=(15, 15))\nxgb.plot_tree(xg_reg, rankdir=\"LR\", num_trees=9, ax=ax);\nprint(\"\\nHave a look at each of the plots. They provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions. This allows us to identify which features are the most important in determining house price. In the next exercise, you'll learn another way of visualizing feature importances.\")\n\n\nExecutableNotFound: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n\n\n\n\n\n\nAnother way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\nOne simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this, and you’ll get a chance to use it in this exercise!\n\n\nCode\n# Create the DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary: params\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n\n# Train the model: xg_reg\nxg_reg = xgb.train(dtrain=housing_dmatrix, params=params, num_boost_round=10)\n\n# Plot the feature importance\nxgb.plot_importance(xg_reg);"
  },
  {
    "objectID": "posts/Sampling Distribution/Sampling Distributions.html",
    "href": "posts/Sampling Distribution/Sampling Distributions.html",
    "title": "Sampling Distribution",
    "section": "",
    "text": "We will discover how to quantify the accuracy of sample statistics using relative errors, and measure variation in your estimates by generating sampling distributions.\nThis Sampling Distribution is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nThe size of the sample has a significant impact on the accuracy of the point estimates.\nRelative error vs. sample size\n\nReally noise, particularly for small samples\nAmplitude is initially steep, then flattens\nRelative error decreases to zero (when the sample size = population)\n\n\n\nIt is important for a sample mean to be close to the population mean when it is calculated. There is, however, a possibility that this may not be the case if your sample size is too small.\nRelative error is the most common metric for assessing accuracy. This is the difference between the population parameter and the point estimate, divided by the population parameter. Sometimes it is expressed as a percentage.\n\n\nCode\nattrition_pop=pd.read_feather('dataset/attrition.feather')\nattrition_pop.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      Attrition\n      BusinessTravel\n      DailyRate\n      Department\n      DistanceFromHome\n      Education\n      EducationField\n      EnvironmentSatisfaction\n      Gender\n      ...\n      PerformanceRating\n      RelationshipSatisfaction\n      StockOptionLevel\n      TotalWorkingYears\n      TrainingTimesLastYear\n      WorkLifeBalance\n      YearsAtCompany\n      YearsInCurrentRole\n      YearsSinceLastPromotion\n      YearsWithCurrManager\n    \n  \n  \n    \n      0\n      21\n      0.0\n      Travel_Rarely\n      391\n      Research_Development\n      15\n      College\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      6\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      1\n      19\n      1.0\n      Travel_Rarely\n      528\n      Sales\n      22\n      Below_College\n      Marketing\n      Very_High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      2\n      Good\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18\n      1.0\n      Travel_Rarely\n      230\n      Research_Development\n      3\n      Bachelor\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      High\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18\n      0.0\n      Travel_Rarely\n      812\n      Sales\n      10\n      Bachelor\n      Medical\n      Very_High\n      Female\n      ...\n      Excellent\n      Low\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18\n      1.0\n      Travel_Frequently\n      1306\n      Sales\n      5\n      Bachelor\n      Marketing\n      Medium\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      3\n      Better\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 31 columns\n\n\n\n\n\nCode\n# Generate a simple random sample of 50 rows, with seed 2022\nattrition_srs50 = attrition_pop.sample(n=50, random_state=2022)\n\n# Calculate the mean employee attrition in the sample\nmean_attrition_srs50 = attrition_srs50['Attrition'].mean()\nmean_attrition_pop= attrition_pop['Attrition'].mean()\n\n# Calculate the relative error percentage\nrel_error_pct50 = 100 * abs(mean_attrition_pop - mean_attrition_srs50) / mean_attrition_pop\n\n# Print rel_error_pct50\nprint(rel_error_pct50)\n\n\n62.78481012658227\n\n\n\n\nCode\n# Generate a simple random sample of 100 rows, with seed 2022\nattrition_srs100 = attrition_pop.sample(n=100, random_state=2022)\n\n# Calculate the mean employee attrition in the sample\nmean_attrition_srs100 = attrition_srs100['Attrition'].mean()\n\n# Calculate the relative error percentage\nrel_error_pct100 = 100 * abs(mean_attrition_pop - mean_attrition_srs100) / mean_attrition_pop\n\n# Print rel_error_pct100\nprint(rel_error_pct100)\nprint(\"\\n As you increase the sample size, the sample mean generally gets closer to the population mean, and the relative error decreases\")\n\n\n6.962025316455695\n\n As you increase the sample size, the sample mean generally gets closer to the population mean, and the relative error decreases\n\n\n\n\n\nReplicating samples\nWhenever you calculate a point estimate, such as a sample mean, the value you calculate depends on the rows included in the sample. As a result, there is some randomness in the answer. A sample mean (or another statistic) can be calculated for each sample in order to quantify the variation caused by this randomness.\n\n\nCode\n# Create an empty list\nmean_attritions=[]\n# Loop 500 times to create 500 sample means\nfor i in range(500):\n    mean_attritions.append(\n        attrition_pop.sample(n=60)['Attrition'].mean()\n    )\n\n# Print out the first few entries of the list\nprint(mean_attritions[0:5])\n\n\n[0.13333333333333333, 0.11666666666666667, 0.13333333333333333, 0.18333333333333332, 0.2]\n\n\n\n\nCode\n# Create a histogram of the 500 sample means\nplt.hist(mean_attritions,bins=16)\nplt.show()\n\n\n\n\n\n\n\n\nEarlier we saw that while increasing the number of replicates didn’t affect the relative error of the sample means; it did result in a more consistent shape to the distribution.\n\n\nCode\nimport itertools\ndef expand_grid(data_dict):\n    rows = itertools.product(*data_dict.values())\n    return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n\n\n\n\nCode\n# Expand a grid representing 5 8-sided dice\ndice = expand_grid(\n  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n  })\n\n# Add a column of mean rolls and convert to a categorical\ndice['mean_roll'] = (dice['die1'] + dice['die2'] +\n                    dice['die3'] + dice['die4'] +\n                    dice['die5'] ) / 5\n\n\ndice['mean_roll'] = dice['mean_roll'].astype('category')\n\n# Print result\nprint(dice)\n\n\n       die1  die2  die3  die4  die5 mean_roll\n0         1     1     1     1     1       1.0\n1         1     1     1     1     2       1.2\n2         1     1     1     1     3       1.4\n3         1     1     1     1     4       1.6\n4         1     1     1     1     5       1.8\n...     ...   ...   ...   ...   ...       ...\n32763     8     8     8     8     4       7.2\n32764     8     8     8     8     5       7.4\n32765     8     8     8     8     6       7.6\n32766     8     8     8     8     7       7.8\n32767     8     8     8     8     8       8.0\n\n[32768 rows x 6 columns]\n\n\n\n\nCode\n# Draw a bar plot of mean_roll\ndice['mean_roll'].value_counts(sort=False).plot(kind='bar')\nplt.show()\n\n\n\n\n\n\n\n\nIt is only possible to calculate the exact sampling distribution in very simple situations. If just five eight-sided dice are used, the number of possible rolls is 8**5, which is over thirty thousand. The number of possible outcomes becomes too difficult to calculate when the dataset is more complicated, such as when a variable has hundreds or thousands of categories.\nYou can calculate an approximate sampling distribution by simulating the exact sampling distribution. You can repeat a procedure repeatedly to simulate both the sampling process and the sample statistic calculation process.\n\n\nCode\n# Sample one to eight, five times, with replacement\nfive_rolls = np.random.choice(list(range(1,9)), size=5, replace=True)\n\n# Print the mean of five_rolls\nprint(five_rolls.mean())\n\n\n6.2\n\n\n\n\nCode\n# Replicate the sampling code 1000 times\nsample_means_1000 = []\nfor i in range(1000):\n    sample_means_1000.append(\n        np.random.choice(list(range(1, 9)), size=5, replace=True).mean())\n\n\n# Print the first 10 entries of the result\nprint(sample_means_1000[0:10])\n\n\n[4.0, 4.8, 5.4, 2.8, 3.4, 3.4, 4.0, 4.8, 5.0, 5.4]\n\n\n\n\nCode\n# Draw a histogram of sample_means_1000 with 20 bins\nplt.hist(sample_means_1000, bins=20)\nplt.show()\n\n\n\n\n\n\n\n\nIn statistics, the Gaussian distribution (also known as the normal distribution) is very important. It has a distinctive bell-shaped curve\nthe central limit theorem: Averages of independent samples have approximately normal distributions As the sample size increases, * The distribution of the averages gets closer to being normally distributed * The width of the sampling distribution gets narrower\nStandard error: * Standard deviation of the sampling distribution * Important tool in understanding sampling variability"
  },
  {
    "objectID": "posts/Sampling Methods/Sampling Methods.html",
    "href": "posts/Sampling Methods/Sampling Methods.html",
    "title": "Sampling Methods",
    "section": "",
    "text": "It’s time to get hands-on and perform the four random sampling methods in Python: simple, systematic, stratified, and cluster.\nThis Sampling Methods is part of Datacamp course: Introduction to sampling\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nAlthough there are several sampling methods such as: * Simple random sampling * Systematic random sampling * Stratified & weight random sampling * Cluster sampling\n\nSimple random sampling: Work like raffle or lottery & consider simplest method of sampling a population. involves picking rows at random, one at a time, where each row has the same chance of being picked as any other\nSystematic random sampling: This samples the population at regular intervals and this method avoid randomness\n\n\n\n\n\nCode\nattrition_pop=pd.read_feather('dataset/attrition.feather')\nattrition_pop.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      Attrition\n      BusinessTravel\n      DailyRate\n      Department\n      DistanceFromHome\n      Education\n      EducationField\n      EnvironmentSatisfaction\n      Gender\n      ...\n      PerformanceRating\n      RelationshipSatisfaction\n      StockOptionLevel\n      TotalWorkingYears\n      TrainingTimesLastYear\n      WorkLifeBalance\n      YearsAtCompany\n      YearsInCurrentRole\n      YearsSinceLastPromotion\n      YearsWithCurrManager\n    \n  \n  \n    \n      0\n      21\n      0.0\n      Travel_Rarely\n      391\n      Research_Development\n      15\n      College\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      6\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      1\n      19\n      1.0\n      Travel_Rarely\n      528\n      Sales\n      22\n      Below_College\n      Marketing\n      Very_High\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      2\n      Good\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18\n      1.0\n      Travel_Rarely\n      230\n      Research_Development\n      3\n      Bachelor\n      Life_Sciences\n      High\n      Male\n      ...\n      Excellent\n      High\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18\n      0.0\n      Travel_Rarely\n      812\n      Sales\n      10\n      Bachelor\n      Medical\n      Very_High\n      Female\n      ...\n      Excellent\n      Low\n      0\n      0\n      2\n      Better\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18\n      1.0\n      Travel_Frequently\n      1306\n      Sales\n      5\n      Bachelor\n      Marketing\n      Medium\n      Male\n      ...\n      Excellent\n      Very_High\n      0\n      0\n      3\n      Better\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 31 columns\n\n\n\n\n\nCode\n# Sample 70 rows using simple random sampling and set the seed\nattrition_samp = attrition_pop.sample(n=70, random_state=18900217)\n\n# Print the sample\nprint(attrition_samp)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1134   35        0.0      Travel_Rarely        583  Research_Development   \n1150   52        0.0         Non-Travel        585                 Sales   \n531    33        0.0      Travel_Rarely        931  Research_Development   \n395    31        0.0      Travel_Rarely       1332  Research_Development   \n392    29        0.0      Travel_Rarely        942  Research_Development   \n...   ...        ...                ...        ...                   ...   \n361    27        0.0  Travel_Frequently       1410                 Sales   \n1180   36        0.0      Travel_Rarely        530                 Sales   \n230    26        0.0      Travel_Rarely       1443                 Sales   \n211    29        0.0  Travel_Frequently        410  Research_Development   \n890    30        0.0  Travel_Frequently       1312  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1134                25         Master           Medical   \n1150                29         Master     Life_Sciences   \n531                 14       Bachelor           Medical   \n395                 11        College           Medical   \n392                 15  Below_College     Life_Sciences   \n...                ...            ...               ...   \n361                  3  Below_College           Medical   \n1180                 2         Master     Life_Sciences   \n230                 23       Bachelor         Marketing   \n211                  2  Below_College     Life_Sciences   \n890                  2         Master  Technical_Degree   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1134                    High  Female  ...          Excellent   \n1150                     Low    Male  ...          Excellent   \n531                Very_High  Female  ...          Excellent   \n395                     High    Male  ...          Excellent   \n392                   Medium  Female  ...          Excellent   \n...                      ...     ...  ...                ...   \n361                Very_High  Female  ...        Outstanding   \n1180                    High  Female  ...          Excellent   \n230                     High  Female  ...          Excellent   \n211                Very_High  Female  ...          Excellent   \n890                Very_High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1134                     High                 1                16   \n1150                   Medium                 2                16   \n531                 Very_High                 1                 8   \n395                 Very_High                 0                 6   \n392                       Low                 1                 6   \n...                       ...               ...               ...   \n361                    Medium                 2                 6   \n1180                     High                 0                17   \n230                      High                 1                 5   \n211                      High                 3                 4   \n890                 Very_High                 0                10   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1134                     3            Good              16   \n1150                     3            Good               9   \n531                      5          Better               8   \n395                      2            Good               6   \n392                      2            Good               5   \n...                    ...             ...             ...   \n361                      3          Better               6   \n1180                     2            Good              13   \n230                      2            Good               2   \n211                      3          Better               3   \n890                      2          Better               9   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1134                  10                       10                    1  \n1150                   8                        0                    0  \n531                    7                        1                    6  \n395                    5                        0                    1  \n392                    4                        1                    3  \n...                  ...                      ...                  ...  \n361                    5                        0                    4  \n1180                   7                        6                    7  \n230                    2                        0                    0  \n211                    2                        0                    2  \n890                    7                        0                    7  \n\n[70 rows x 31 columns]\n\n\n\n\n\n\n\nCode\n# Set the sample size to 70\nsample_size = 70\n\n# Calculate the population size from attrition_pop\npop_size = len(attrition_pop)\n\n# Calculate the interval\ninterval = pop_size // sample_size\n\n# Systematically sample 70 rows\nattrition_sys_samp = attrition_pop.iloc[::interval]\n\n# Print the sample\nprint(attrition_sys_samp)\n\n\n      Age  Attrition BusinessTravel  DailyRate            Department  \\\n0      21        0.0  Travel_Rarely        391  Research_Development   \n21     19        0.0  Travel_Rarely       1181  Research_Development   \n42     45        0.0  Travel_Rarely        252  Research_Development   \n63     23        0.0  Travel_Rarely        373  Research_Development   \n84     30        1.0  Travel_Rarely        945                 Sales   \n...   ...        ...            ...        ...                   ...   \n1365   48        0.0  Travel_Rarely        715  Research_Development   \n1386   48        0.0  Travel_Rarely       1355  Research_Development   \n1407   50        0.0  Travel_Rarely        989  Research_Development   \n1428   50        0.0     Non-Travel        881  Research_Development   \n1449   52        0.0  Travel_Rarely        699  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n0                   15        College  Life_Sciences                    High   \n21                   3  Below_College        Medical                  Medium   \n42                   2       Bachelor  Life_Sciences                  Medium   \n63                   1        College  Life_Sciences               Very_High   \n84                   9       Bachelor        Medical                  Medium   \n...                ...            ...            ...                     ...   \n1365                 1       Bachelor  Life_Sciences               Very_High   \n1386                 4         Master  Life_Sciences                    High   \n1407                 7        College        Medical                  Medium   \n1428                 2         Master  Life_Sciences                     Low   \n1449                 1         Master  Life_Sciences                    High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n0       Male  ...          Excellent                Very_High   \n21    Female  ...          Excellent                Very_High   \n42    Female  ...          Excellent                Very_High   \n63      Male  ...        Outstanding                Very_High   \n84      Male  ...          Excellent                     High   \n...      ...  ...                ...                      ...   \n1365    Male  ...          Excellent                     High   \n1386    Male  ...          Excellent                   Medium   \n1407  Female  ...          Excellent                Very_High   \n1428    Male  ...          Excellent                Very_High   \n1449    Male  ...          Excellent                      Low   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n0                    0                 0                     6   \n21                   0                 1                     3   \n42                   0                 1                     3   \n63                   1                 1                     2   \n84                   0                 1                     3   \n...                ...               ...                   ...   \n1365                 0                25                     3   \n1386                 0                27                     3   \n1407                 1                29                     2   \n1428                 1                31                     3   \n1449                 1                34                     5   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n0             Better               0                   0   \n21            Better               1                   0   \n42            Better               1                   0   \n63            Better               1                   0   \n84              Good               1                   0   \n...              ...             ...                 ...   \n1365            Best               1                   0   \n1386          Better              15                  11   \n1407            Good              27                   3   \n1428          Better              31                   6   \n1449          Better              33                  18   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n0                           0                    0  \n21                          0                    0  \n42                          0                    0  \n63                          0                    1  \n84                          0                    0  \n...                       ...                  ...  \n1365                        0                    0  \n1386                        4                    8  \n1407                       13                    8  \n1428                       14                    7  \n1449                       11                    9  \n\n[70 rows x 31 columns]\n\n\n\n\n\nIn the case of systematic sampling, there is a problem: if the data has been sorted or there is a pattern or meaning behind the row order, then the resulting sample may not be representative of the entire population. If the rows are shuffled, the problem can be solved, but then systematic sampling becomes equivalent to simple random sampling.\n\n\nCode\n# Add an index column to attrition_pop\nattrition_pop_id = attrition_pop.reset_index()\n\n# Plot YearsAtCompany vs. index for attrition_pop_id\nattrition_pop_id.plot(x='index',y='YearsAtCompany',kind='scatter')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Shuffle the rows of attrition_pop\nattrition_shuffled = attrition_pop.sample(frac=1)\n\n# Reset the row indexes and create an index column\nattrition_shuffled = attrition_shuffled.reset_index(drop=True).reset_index()\n\n# Plot YearsAtCompany vs. index for attrition_shuffled\nattrition_shuffled.plot(x='index',y='YearsAtCompany',kind='scatter')\nplt.show()\n\n\n\n\n\n\n\n\nStratified sampling is a technique that allows us to sample a population that contains subgroups\n\nWeighted random sampling\n\nA close relative of stratified sampling that provides even more flexibility is weighted random sampling. In this variant, we create a column of weights that adjust the relative probability of sampling each row.\n\n\n\nYou may need to carefully control the counts of each subgroup within the population if you are interested in subgroups within the population. As a result of proportional stratified sampling, the subgroup sizes within the sample are representative of the subgroup sizes within the population as a whole.\n\n\nCode\n# Proportion of employees by Education level\neducation_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n\n# Print education_counts_pop\nprint(education_counts_pop)\n\n\nBachelor         0.389116\nMaster           0.270748\nCollege          0.191837\nBelow_College    0.115646\nDoctor           0.032653\nName: Education, dtype: float64\n\n\n\n\nCode\n# Proportional stratified sampling for 40% of each Education group\nattrition_strat = attrition_pop.groupby('Education').sample(frac=0.4, random_state=2022)\n\n# Print the sample\nprint(attrition_strat)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1191   53        0.0      Travel_Rarely        238                 Sales   \n407    29        0.0  Travel_Frequently        995  Research_Development   \n1233   59        0.0  Travel_Frequently       1225                 Sales   \n366    37        0.0      Travel_Rarely        571  Research_Development   \n702    31        0.0  Travel_Frequently        163  Research_Development   \n...   ...        ...                ...        ...                   ...   \n733    38        0.0  Travel_Frequently        653  Research_Development   \n1061   44        0.0  Travel_Frequently        602       Human_Resources   \n1307   41        0.0      Travel_Rarely       1276                 Sales   \n1060   33        0.0      Travel_Rarely        516  Research_Development   \n177    29        0.0      Travel_Rarely        738  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1191                 1  Below_College           Medical   \n407                  2  Below_College     Life_Sciences   \n1233                 1  Below_College     Life_Sciences   \n366                 10  Below_College     Life_Sciences   \n702                 24  Below_College  Technical_Degree   \n...                ...            ...               ...   \n733                 29         Doctor     Life_Sciences   \n1061                 1         Doctor   Human_Resources   \n1307                 2         Doctor     Life_Sciences   \n1060                 8         Doctor     Life_Sciences   \n177                  9         Doctor             Other   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1191               Very_High  Female  ...        Outstanding   \n407                      Low    Male  ...          Excellent   \n1233                     Low  Female  ...          Excellent   \n366                Very_High  Female  ...          Excellent   \n702                Very_High  Female  ...        Outstanding   \n...                      ...     ...  ...                ...   \n733                Very_High  Female  ...          Excellent   \n1061                     Low    Male  ...          Excellent   \n1307                  Medium  Female  ...          Excellent   \n1060               Very_High    Male  ...          Excellent   \n177                   Medium    Male  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1191                Very_High                 0                18   \n407                 Very_High                 1                 6   \n1233                Very_High                 0                20   \n366                    Medium                 2                 6   \n702                 Very_High                 0                 9   \n...                       ...               ...               ...   \n733                 Very_High                 0                10   \n1061                     High                 0                14   \n1307                   Medium                 1                22   \n1060                      Low                 0                14   \n177                      High                 0                 4   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1191                     2            Best              14   \n407                      0            Best               6   \n1233                     2            Good               4   \n366                      3            Good               5   \n702                      3            Good               5   \n...                    ...             ...             ...   \n733                      2          Better              10   \n1061                     3          Better              10   \n1307                     2          Better              18   \n1060                     6          Better               0   \n177                      2          Better               3   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1191                   7                        8                   10  \n407                    4                        1                    3  \n1233                   3                        1                    3  \n366                    3                        4                    3  \n702                    4                        1                    4  \n...                  ...                      ...                  ...  \n733                    3                        9                    9  \n1061                   7                        0                    2  \n1307                  16                       11                    8  \n1060                   0                        0                    0  \n177                    2                        2                    2  \n\n[588 rows x 31 columns]\n\n\n\n\nCode\n# Calculate the Education level proportions from attrition_strat\neducation_counts_strat = attrition_strat['Education'].value_counts(normalize=True)\n\n# Print education_counts_strat\nprint(education_counts_strat)\nprint('\\nBy grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population.')\n\n\nBachelor         0.389456\nMaster           0.270408\nCollege          0.192177\nBelow_College    0.115646\nDoctor           0.032313\nName: Education, dtype: float64\n\nBy grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population.\n\n\n\n\n\nWhen one subgroup is larger than another in the population, but you do not want to factor this difference into your analysis, you can use equal counts stratified sampling to generate samples in which each subgroup has the same amount of data.\n\n\nCode\n# Get 30 employees from each Education group\nattrition_eq = attrition_pop.groupby('Education').sample(n=30, random_state=2022)\n\n# Print the sample\nprint(attrition_eq)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1191   53        0.0      Travel_Rarely        238                 Sales   \n407    29        0.0  Travel_Frequently        995  Research_Development   \n1233   59        0.0  Travel_Frequently       1225                 Sales   \n366    37        0.0      Travel_Rarely        571  Research_Development   \n702    31        0.0  Travel_Frequently        163  Research_Development   \n...   ...        ...                ...        ...                   ...   \n774    33        0.0      Travel_Rarely        922  Research_Development   \n869    45        0.0      Travel_Rarely       1015  Research_Development   \n530    32        0.0      Travel_Rarely        120  Research_Development   \n1049   48        0.0      Travel_Rarely        163                 Sales   \n350    29        1.0      Travel_Rarely        408  Research_Development   \n\n      DistanceFromHome      Education    EducationField  \\\n1191                 1  Below_College           Medical   \n407                  2  Below_College     Life_Sciences   \n1233                 1  Below_College     Life_Sciences   \n366                 10  Below_College     Life_Sciences   \n702                 24  Below_College  Technical_Degree   \n...                ...            ...               ...   \n774                  1         Doctor           Medical   \n869                  5         Doctor           Medical   \n530                  6         Doctor     Life_Sciences   \n1049                 2         Doctor         Marketing   \n350                 25         Doctor  Technical_Degree   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1191               Very_High  Female  ...        Outstanding   \n407                      Low    Male  ...          Excellent   \n1233                     Low  Female  ...          Excellent   \n366                Very_High  Female  ...          Excellent   \n702                Very_High  Female  ...        Outstanding   \n...                      ...     ...  ...                ...   \n774                      Low  Female  ...          Excellent   \n869                     High  Female  ...          Excellent   \n530                     High    Male  ...        Outstanding   \n1049                  Medium  Female  ...          Excellent   \n350                     High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1191                Very_High                 0                18   \n407                 Very_High                 1                 6   \n1233                Very_High                 0                20   \n366                    Medium                 2                 6   \n702                 Very_High                 0                 9   \n...                       ...               ...               ...   \n774                      High                 1                10   \n869                       Low                 0                10   \n530                       Low                 0                 8   \n1049                      Low                 1                14   \n350                    Medium                 0                 6   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1191                     2            Best              14   \n407                      0            Best               6   \n1233                     2            Good               4   \n366                      3            Good               5   \n702                      3            Good               5   \n...                    ...             ...             ...   \n774                      2          Better               6   \n869                      3          Better              10   \n530                      2          Better               5   \n1049                     2          Better               9   \n350                      2            Best               2   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1191                   7                        8                   10  \n407                    4                        1                    3  \n1233                   3                        1                    3  \n366                    3                        4                    3  \n702                    4                        1                    4  \n...                  ...                      ...                  ...  \n774                    1                        0                    5  \n869                    7                        1                    4  \n530                    4                        1                    4  \n1049                   7                        6                    7  \n350                    2                        1                    1  \n\n[150 rows x 31 columns]\n\n\n\n\nCode\n# Get the proportions from attrition_eq\neducation_counts_eq = attrition_eq['Education'].value_counts(normalize=True)\n\n# Print the results\nprint(education_counts_eq)\n\n\nBelow_College    0.2\nCollege          0.2\nBachelor         0.2\nMaster           0.2\nDoctor           0.2\nName: Education, dtype: float64\n\n\n\n\n\nThe stratified sampling method determines the probability of picking rows from your dataset based on the subgroups within your dataset. A generalization of this is weighted sampling, which allows you to specify rules regarding the probability of selecting rows at the row level. A row’s probability of being selected is proportional to its weight value.\n\n\nCode\n# Plot YearsAtCompany from attrition_pop as a histogram\nattrition_pop['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Sample 400 employees weighted by YearsAtCompany\nattrition_weight = attrition_pop.sample(n=400, weights='YearsAtCompany')\n\n# Print the sample\nprint(attrition_weight)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n853    36        0.0      Travel_Rarely        172  Research_Development   \n481    34        0.0  Travel_Frequently        618  Research_Development   \n1148   38        0.0      Travel_Rarely       1321                 Sales   \n1430   51        0.0  Travel_Frequently        237                 Sales   \n517    39        0.0      Travel_Rarely        835  Research_Development   \n...   ...        ...                ...        ...                   ...   \n1351   45        0.0      Travel_Rarely       1038  Research_Development   \n1412   54        0.0      Travel_Rarely        971  Research_Development   \n1248   47        0.0  Travel_Frequently       1379  Research_Development   \n1210   36        0.0  Travel_Frequently        688  Research_Development   \n1328   55        0.0  Travel_Frequently       1091  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n853                  4         Master  Life_Sciences                     Low   \n481                  3  Below_College  Life_Sciences                     Low   \n1148                 1         Master  Life_Sciences               Very_High   \n1430                 9       Bachelor  Life_Sciences               Very_High   \n517                 19         Master          Other               Very_High   \n...                ...            ...            ...                     ...   \n1351                20       Bachelor        Medical                  Medium   \n1412                 1       Bachelor        Medical               Very_High   \n1248                16         Master        Medical                    High   \n1210                 4        College  Life_Sciences               Very_High   \n1328                 2  Below_College  Life_Sciences               Very_High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n853     Male  ...          Excellent                     High   \n481     Male  ...          Excellent                     High   \n1148    Male  ...          Excellent                      Low   \n1430    Male  ...        Outstanding                      Low   \n517     Male  ...          Excellent                   Medium   \n...      ...  ...                ...                      ...   \n1351    Male  ...          Excellent                   Medium   \n1412  Female  ...          Excellent                Very_High   \n1248    Male  ...          Excellent                     High   \n1210  Female  ...          Excellent                   Medium   \n1328    Male  ...          Excellent                   Medium   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n853                  0                10                     2   \n481                  0                 7                     1   \n1148                 2                16                     3   \n1430                 1                31                     5   \n517                  3                 7                     2   \n...                ...               ...                   ...   \n1351                 1                24                     2   \n1412                 0                29                     3   \n1248                 0                20                     3   \n1210                 3                18                     3   \n1328                 1                23                     4   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n853             Good              10                   4   \n481             Good               6                   2   \n1148          Better              15                  13   \n1430            Good              29                  10   \n517           Better               2                   2   \n...              ...             ...                 ...   \n1351          Better               7                   7   \n1412            Good              20                   7   \n1248            Best              19                  10   \n1210          Better               4                   2   \n1328          Better               3                   2   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n853                         1                    8  \n481                         0                    4  \n1148                        5                    8  \n1430                       11                   10  \n517                         2                    2  \n...                       ...                  ...  \n1351                        0                    7  \n1412                       12                    7  \n1248                        2                    7  \n1210                        0                    2  \n1328                        1                    2  \n\n[400 rows x 31 columns]\n\n\n\n\nCode\n# Plot YearsAtCompany from attrition_weight as a histogram\nattrition_weight['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\nCode\n# Plot YearsAtCompany from attrition_pop as a histogram\nattrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\nplt.show()\n\n# Sample 400 employees weighted by YearsAtCompany\nattrition_weight = attrition_pop.sample(n=400, weights=\"YearsAtCompany\")\n\n# Plot YearsAtCompany from attrition_weight as a histogram\nattrition_weight['YearsAtCompany'].hist(bins=np.arange(0,41,1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStratified sampling vs. cluster sampling * Stratified sampling: * Split the population into subgroups * Use simple random sampling on every subgroup * Cluster sampling * Use simple random sampling to pick some subgroups * Use simple random sampling on only those subgroups\n\n\nCode\nimport random\n# Create a list of unique JobRole values\njob_roles_pop = list(attrition_pop['JobRole'].unique())\n\n# Randomly sample four JobRole values\njob_roles_samp = random.sample(job_roles_pop,k=4)\n\n# Print the result\nprint(job_roles_samp)\n\n\n['Research_Director', 'Sales_Executive', 'Sales_Representative', 'Laboratory_Technician']\n\n\n\n\nCode\n# Filter for rows where JobRole is in job_roles_samp\njobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\nattrition_filtered = attrition_pop[jobrole_condition]\n\n# Print the result\nprint(attrition_filtered)\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1      19        1.0      Travel_Rarely        528                 Sales   \n2      18        1.0      Travel_Rarely        230  Research_Development   \n3      18        0.0      Travel_Rarely        812                 Sales   \n4      18        1.0  Travel_Frequently       1306                 Sales   \n7      18        1.0         Non-Travel        247  Research_Development   \n...   ...        ...                ...        ...                   ...   \n1457   55        0.0      Travel_Rarely        692  Research_Development   \n1458   56        0.0  Travel_Frequently        906                 Sales   \n1459   54        0.0      Travel_Rarely        685  Research_Development   \n1467   58        0.0      Travel_Rarely        682                 Sales   \n1469   58        1.0      Travel_Rarely        286  Research_Development   \n\n      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \\\n1                   22  Below_College      Marketing               Very_High   \n2                    3       Bachelor  Life_Sciences                    High   \n3                   10       Bachelor        Medical               Very_High   \n4                    5       Bachelor      Marketing                  Medium   \n7                    8  Below_College        Medical                    High   \n...                ...            ...            ...                     ...   \n1457                14         Master        Medical                    High   \n1458                 6       Bachelor  Life_Sciences                    High   \n1459                 3       Bachelor  Life_Sciences               Very_High   \n1467                10         Master        Medical               Very_High   \n1469                 2         Master  Life_Sciences               Very_High   \n\n      Gender  ...  PerformanceRating RelationshipSatisfaction  \\\n1       Male  ...          Excellent                Very_High   \n2       Male  ...          Excellent                     High   \n3     Female  ...          Excellent                      Low   \n4       Male  ...          Excellent                Very_High   \n7       Male  ...          Excellent                Very_High   \n...      ...  ...                ...                      ...   \n1457    Male  ...          Excellent                Very_High   \n1458  Female  ...          Excellent                Very_High   \n1459    Male  ...          Excellent                      Low   \n1467    Male  ...          Excellent                     High   \n1469    Male  ...          Excellent                Very_High   \n\n      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \\\n1                    0                 0                     2   \n2                    0                 0                     2   \n3                    0                 0                     2   \n4                    0                 0                     3   \n7                    0                 0                     0   \n...                ...               ...                   ...   \n1457                 0                36                     3   \n1458                 3                36                     0   \n1459                 0                36                     2   \n1467                 0                38                     1   \n1469                 0                40                     2   \n\n     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n1               Good               0                   0   \n2             Better               0                   0   \n3             Better               0                   0   \n4             Better               0                   0   \n7             Better               0                   0   \n...              ...             ...                 ...   \n1457          Better              24                  15   \n1458            Good               7                   7   \n1459          Better              10                   9   \n1467            Good              37                  10   \n1469          Better              31                  15   \n\n      YearsSinceLastPromotion YearsWithCurrManager  \n1                           0                    0  \n2                           0                    0  \n3                           0                    0  \n4                           0                    0  \n7                           0                    0  \n...                       ...                  ...  \n1457                        2                   15  \n1458                        7                    7  \n1459                        0                    9  \n1467                        1                    8  \n1469                       13                    8  \n\n[748 rows x 31 columns]\n\n\n\n\nCode\n# Remove categories with no rows\nattrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n\n# Randomly sample 10 employees from each sampled job role\nattrition_clust = attrition_filtered.groupby('JobRole').sample(n=10,random_state=2022)\n\n# Print the sample\nprint(attrition_clust)\n\nprint(\"\\n The two-stage sampling technique gives you control over sampling both between subgroups and within subgroups.\")\n\n\n      Age  Attrition     BusinessTravel  DailyRate            Department  \\\n1124   36        0.0      Travel_Rarely       1396  Research_Development   \n576    45        0.0      Travel_Rarely        974  Research_Development   \n995    42        0.0  Travel_Frequently        748  Research_Development   \n1243   50        0.0      Travel_Rarely       1207  Research_Development   \n869    45        0.0      Travel_Rarely       1015  Research_Development   \n599    33        0.0      Travel_Rarely       1099  Research_Development   \n117    24        0.0      Travel_Rarely        350  Research_Development   \n472    30        0.0      Travel_Rarely        921  Research_Development   \n149    27        0.0         Non-Travel       1277  Research_Development   \n49     20        1.0      Travel_Rarely        129  Research_Development   \n1302   40        0.0      Travel_Rarely       1416  Research_Development   \n1126   42        0.0      Travel_Rarely        810  Research_Development   \n1216   38        0.0      Travel_Rarely        849  Research_Development   \n1362   43        0.0      Travel_Rarely        982  Research_Development   \n1327   46        0.0      Travel_Rarely        430  Research_Development   \n664    27        0.0      Travel_Rarely        269  Research_Development   \n1284   40        0.0      Travel_Rarely       1308  Research_Development   \n1440   50        0.0  Travel_Frequently        333  Research_Development   \n790    36        0.0         Non-Travel        427  Research_Development   \n1432   55        0.0      Travel_Rarely       1136  Research_Development   \n941    36        0.0      Travel_Rarely        329                 Sales   \n454    27        0.0  Travel_Frequently       1242                 Sales   \n460    37        0.0      Travel_Rarely        228                 Sales   \n636    45        0.0      Travel_Rarely       1268                 Sales   \n293    33        0.0  Travel_Frequently        430                 Sales   \n976    39        1.0      Travel_Rarely       1162                 Sales   \n813    30        0.0      Travel_Rarely        231                 Sales   \n288    35        1.0  Travel_Frequently        662                 Sales   \n1111   53        1.0      Travel_Rarely       1168                 Sales   \n1075   40        0.0      Travel_Rarely        630                 Sales   \n133    34        1.0  Travel_Frequently        296                 Sales   \n725    36        1.0      Travel_Rarely       1218                 Sales   \n4      18        1.0  Travel_Frequently       1306                 Sales   \n169    41        1.0      Travel_Rarely       1356                 Sales   \n1      19        1.0      Travel_Rarely        528                 Sales   \n48     19        1.0      Travel_Rarely        419                 Sales   \n150    31        0.0  Travel_Frequently        793                 Sales   \n130    21        0.0         Non-Travel        895                 Sales   \n3      18        0.0      Travel_Rarely        812                 Sales   \n99     31        0.0  Travel_Frequently        444                 Sales   \n\n      DistanceFromHome      Education    EducationField  \\\n1124                 5        College     Life_Sciences   \n576                  1         Master           Medical   \n995                  9        College           Medical   \n1243                28  Below_College           Medical   \n869                  5         Doctor           Medical   \n599                  4         Master           Medical   \n117                 21        College  Technical_Degree   \n472                  1       Bachelor     Life_Sciences   \n149                  8         Doctor     Life_Sciences   \n49                   4       Bachelor  Technical_Degree   \n1302                 2        College           Medical   \n1126                23         Doctor     Life_Sciences   \n1216                25        College     Life_Sciences   \n1362                12       Bachelor     Life_Sciences   \n1327                 1         Master           Medical   \n664                  5  Below_College  Technical_Degree   \n1284                14       Bachelor           Medical   \n1440                22         Doctor           Medical   \n790                  8       Bachelor     Life_Sciences   \n1432                 1         Master           Medical   \n941                 16         Master         Marketing   \n454                 20       Bachelor     Life_Sciences   \n460                  6         Master           Medical   \n636                  4        College     Life_Sciences   \n293                  7       Bachelor           Medical   \n976                  3        College           Medical   \n813                  8        College             Other   \n288                 18         Master         Marketing   \n1111                24         Master     Life_Sciences   \n1075                 4         Master         Marketing   \n133                  6        College         Marketing   \n725                  9         Master     Life_Sciences   \n4                    5       Bachelor         Marketing   \n169                 20        College         Marketing   \n1                   22  Below_College         Marketing   \n48                  21       Bachelor             Other   \n150                 20       Bachelor     Life_Sciences   \n130                  9        College           Medical   \n3                   10       Bachelor           Medical   \n99                   5       Bachelor         Marketing   \n\n     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \\\n1124               Very_High    Male  ...          Excellent   \n576                Very_High  Female  ...          Excellent   \n995                      Low  Female  ...          Excellent   \n1243               Very_High    Male  ...          Excellent   \n869                     High  Female  ...          Excellent   \n599                      Low  Female  ...          Excellent   \n117                     High    Male  ...          Excellent   \n472                Very_High    Male  ...        Outstanding   \n149                      Low    Male  ...          Excellent   \n49                       Low    Male  ...          Excellent   \n1302                     Low    Male  ...          Excellent   \n1126                     Low  Female  ...          Excellent   \n1216                     Low  Female  ...          Excellent   \n1362                     Low    Male  ...          Excellent   \n1327               Very_High    Male  ...          Excellent   \n664                     High    Male  ...          Excellent   \n1284                    High    Male  ...          Excellent   \n1440                    High    Male  ...          Excellent   \n790                      Low  Female  ...          Excellent   \n1432                  Medium    Male  ...          Excellent   \n941                     High  Female  ...          Excellent   \n454                Very_High  Female  ...          Excellent   \n460                     High    Male  ...          Excellent   \n636                     High  Female  ...          Excellent   \n293                Very_High    Male  ...          Excellent   \n976                Very_High  Female  ...          Excellent   \n813                     High    Male  ...          Excellent   \n288                Very_High  Female  ...          Excellent   \n1111                     Low    Male  ...          Excellent   \n1075                    High    Male  ...          Excellent   \n133                Very_High  Female  ...          Excellent   \n725                     High    Male  ...        Outstanding   \n4                     Medium    Male  ...          Excellent   \n169                   Medium  Female  ...        Outstanding   \n1                  Very_High    Male  ...          Excellent   \n48                 Very_High    Male  ...          Excellent   \n150                     High    Male  ...          Excellent   \n130                      Low    Male  ...        Outstanding   \n3                  Very_High  Female  ...          Excellent   \n99                 Very_High  Female  ...          Excellent   \n\n     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \\\n1124                Very_High                 0                16   \n576                 Very_High                 2                 8   \n995                      High                 0                12   \n1243                     High                 3                20   \n869                       Low                 0                10   \n599                 Very_High                 0                 8   \n117                    Medium                 3                 2   \n472                      High                 2                 7   \n149                 Very_High                 3                 3   \n49                     Medium                 0                 1   \n1302                Very_High                 1                22   \n1126                   Medium                 0                16   \n1216                     High                 1                19   \n1362                     High                 1                25   \n1327                Very_High                 2                23   \n664                    Medium                 1                 9   \n1284                      Low                 0                21   \n1440                Very_High                 0                32   \n790                       Low                 1                10   \n1432                Very_High                 2                31   \n941                       Low                 2                11   \n454                 Very_High                 0                 7   \n460                    Medium                 1                 7   \n636                       Low                 1                 9   \n293                       Low                 2                 5   \n976                       Low                 0                12   \n813                       Low                 1                10   \n288                      High                 1                 5   \n1111                   Medium                 0                15   \n1075                      Low                 1                15   \n133                 Very_High                 1                 3   \n725                    Medium                 0                10   \n4                   Very_High                 0                 0   \n169                 Very_High                 0                 4   \n1                   Very_High                 0                 0   \n48                     Medium                 0                 1   \n150                       Low                 1                 3   \n130                      High                 0                 3   \n3                         Low                 0                 0   \n99                       High                 1                 2   \n\n     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n1124                     3            Best              13   \n576                      2          Better               5   \n995                      3          Better              12   \n1243                     3          Better              20   \n869                      3          Better              10   \n599                      5          Better               5   \n117                      3          Better               1   \n472                      2          Better               2   \n149                      4          Better               3   \n49                       2          Better               1   \n1302                     5          Better              21   \n1126                     2          Better               1   \n1216                     2          Better              10   \n1362                     3          Better              25   \n1327                     0          Better               2   \n664                      3          Better               9   \n1284                     2            Best              20   \n1440                     2          Better              32   \n790                      2          Better               8   \n1432                     4            Best               7   \n941                      3            Good               3   \n454                      2          Better               7   \n460                      5            Best               5   \n636                      3            Best               5   \n293                      2          Better               4   \n976                      3            Good               1   \n813                      2            Best               8   \n288                      0            Good               4   \n1111                     2            Good               2   \n1075                     2            Good              12   \n133                      3            Good               2   \n725                      4          Better               5   \n4                        3          Better               0   \n169                      5            Good               4   \n1                        2            Good               0   \n48                       3            Best               1   \n150                      4          Better               2   \n130                      3            Good               3   \n3                        2          Better               0   \n99                       5            Good               2   \n\n      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  \n1124                  11                        3                    7  \n576                    3                        0                    2  \n995                    9                        5                    8  \n1243                   8                        3                    8  \n869                    7                        1                    4  \n599                    4                        0                    2  \n117                    1                        0                    0  \n472                    2                        0                    2  \n149                    2                        1                    2  \n49                     0                        0                    0  \n1302                   7                        3                    9  \n1126                   0                        0                    0  \n1216                   8                        0                    1  \n1362                  10                        3                    9  \n1327                   2                        2                    2  \n664                    8                        0                    8  \n1284                   7                        4                    9  \n1440                   6                       13                    9  \n790                    7                        0                    5  \n1432                   7                        0                    0  \n941                    2                        0                    2  \n454                    7                        0                    7  \n460                    4                        0                    1  \n636                    4                        0                    3  \n293                    3                        0                    3  \n976                    0                        0                    0  \n813                    4                        7                    7  \n288                    2                        3                    2  \n1111                   2                        2                    2  \n1075                  11                        2                   11  \n133                    2                        1                    0  \n725                    3                        0                    3  \n4                      0                        0                    0  \n169                    3                        0                    2  \n1                      0                        0                    0  \n48                     0                        0                    0  \n150                    2                        2                    2  \n130                    2                        2                    2  \n3                      0                        0                    0  \n99                     2                        2                    2  \n\n[40 rows x 31 columns]\n\n The two-stage sampling technique gives you control over sampling both between subgroups and within subgroups.\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_28748\\2564666783.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n\n\n\n\n\nYou’re going to compare the performance of point estimates using simple, stratified, and cluster sampling. Before doing that, you’ll have to set up the samples\n\n\nCode\n# Perform simple random sampling to get 0.25 of the population\nattrition_srs = attrition_pop.sample(frac=1/4, random_state=2022)\nattrition_srs.shape\n\n\n(368, 31)\n\n\n\n\nCode\n# Perform stratified sampling to get 0.25 of each relationship group\nattrition_strat = attrition_pop.groupby('RelationshipSatisfaction').sample(frac=1/4, random_state=2022)\nattrition_strat.shape\n\n\n(368, 31)\n\n\n\n\nCode\n# Create a list of unique RelationshipSatisfaction values\nsatisfaction_unique = list(attrition_pop['RelationshipSatisfaction'].unique())\n\n# Randomly sample 2 unique satisfaction values\nsatisfaction_samp = random.sample(satisfaction_unique, k=2)\n\n# Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction\nsatis_condition = attrition_pop['RelationshipSatisfaction'].isin(satisfaction_samp)\nattrition_clust_prep = attrition_pop[satis_condition]\nattrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n\n# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop\nattrition_clust = attrition_clust_prep.groupby(\"RelationshipSatisfaction\")\\\n    .sample(n=len(attrition_pop) // 4, random_state=2022)\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_28748\\1225069142.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  attrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n\n\nValueError: Cannot take a larger sample than population when 'replace=False'\n\n\n\n\n\n\n\nCode\n# Mean Attrition by RelationshipSatisfaction group\nmean_attrition_pop = attrition_pop.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_pop)\n\n\nRelationshipSatisfaction\nLow          0.206522\nMedium       0.148515\nHigh         0.154684\nVery_High    0.148148\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the simple random sample\nmean_attrition_srs = attrition_srs.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_srs)\n\n\nRelationshipSatisfaction\nLow          0.134328\nMedium       0.164179\nHigh         0.160000\nVery_High    0.155963\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the stratified sample\nmean_attrition_strat = attrition_strat.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_strat)\n\n\nRelationshipSatisfaction\nLow          0.144928\nMedium       0.078947\nHigh         0.165217\nVery_High    0.129630\nName: Attrition, dtype: float64\n\n\n\n\nCode\n# Calculate the same thing for the cluster sample\nmean_attrition_clust = attrition_clust.groupby('RelationshipSatisfaction')['Attrition'].mean()\n\n# Print the result\nprint(mean_attrition_clust)\n\n\nRelationshipSatisfaction\nLow          0.090909\nMedium       0.500000\nHigh         0.125000\nVery_High    0.307692\nName: Attrition, dtype: float64"
  },
  {
    "objectID": "posts/Selecting features for modeling/Selecting Features for Modeling.html",
    "href": "posts/Selecting features for modeling/Selecting Features for Modeling.html",
    "title": "Selecting Features for Modeling",
    "section": "",
    "text": "We are going to learn how to create a few different techniques to evaluate the most important features from your dataset. we will learn how to eliminate redundant features, use text vectors to reduce the number of features in your dataset, and use principal component analysis (PCA) to reduce the number of features in your dataset.\nThis Selecting Features for Modeling is part of Datacamp course: Preprocessing for Machine Learning in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\nSelecting features to be used for modeling\nDoesn’t create new features\nImprove model’s performance\n\n\n\nTake an exploratory look at the post-feature engineering hiking dataset.\n\n\nCode\nhiking = pd.read_json('dataset/hiking.json')\nhiking.head()\n\n\n\n\n\n\n  \n    \n      \n      Prop_ID\n      Name\n      Location\n      Park_Name\n      Length\n      Difficulty\n      Other_Details\n      Accessible\n      Limited_Access\n      lat\n      lon\n    \n  \n  \n    \n      0\n      B057\n      Salt Marsh Nature Trail\n      Enter behind the Salt Marsh Nature Center, loc...\n      Marine Park\n      0.8 miles\n      None\n      <p>The first half of this mile-long trail foll...\n      Y\n      N\n      NaN\n      NaN\n    \n    \n      1\n      B073\n      Lullwater\n      Enter Park at Lincoln Road and Ocean Avenue en...\n      Prospect Park\n      1.0 mile\n      Easy\n      Explore the Lullwater to see how nature thrive...\n      N\n      N\n      NaN\n      NaN\n    \n    \n      2\n      B073\n      Midwood\n      Enter Park at Lincoln Road and Ocean Avenue en...\n      Prospect Park\n      0.75 miles\n      Easy\n      Step back in time with a walk through Brooklyn...\n      N\n      N\n      NaN\n      NaN\n    \n    \n      3\n      B073\n      Peninsula\n      Enter Park at Lincoln Road and Ocean Avenue en...\n      Prospect Park\n      0.5 miles\n      Easy\n      Discover how the Peninsula has changed over th...\n      N\n      N\n      NaN\n      NaN\n    \n    \n      4\n      B073\n      Waterfall\n      Enter Park at Lincoln Road and Ocean Avenue en...\n      Prospect Park\n      0.5 miles\n      Easy\n      Trace the source of the Lake on the Waterfall ...\n      N\n      N\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\n\n\n\nRemove noisy features\nRemove correlated features\n\nStatistically correlated: features move together directionally\nLinear models assume feature independence\nPearson correlation coefficient\n\nRemove duplicated features\n\n\n\nNow let’s identify the redundant columns in the volunteer dataset and perform feature selection on the dataset to return a DataFrame of the relevant features.\nFor example, if you explore the volunteer dataset in the console, you’ll see three features which are related to location: locality, region, and postalcode. They contain repeated information, so it would make sense to keep only one of the features.\nThere are also features that have gone through the feature engineering process: columns like Education and Emergency Preparedness are a product of encoding the categorical variable category_desc, so category_desc itself is redundant now.\nTake a moment to examine the features of volunteer in the console, and try to identify the redundant features.\n\n\nCode\nvolunteer = pd.read_csv('dataset/volunteer_sample.csv')\nvolunteer.dropna(subset=['category_desc'], axis=0, inplace=True)\nvolunteer.head()\n\n\n\n\n\n\n  \n    \n      \n      vol_requests\n      title\n      hits\n      category_desc\n      locality\n      region\n      postalcode\n      created_date\n      vol_requests_lognorm\n      created_month\n      Education\n      Emergency Preparedness\n      Environment\n      Health\n      Helping Neighbors in Need\n      Strengthening Communities\n    \n  \n  \n    \n      0\n      2\n      Web designer\n      22\n      Strengthening Communities\n      5 22nd St\\nNew York, NY 10010\\n(40.74053152272...\n      NY\n      10010.0\n      2011-01-14\n      0.693147\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      1\n      20\n      Urban Adventures - Ice Skating at Lasker Rink\n      62\n      Strengthening Communities\n      NaN\n      NY\n      10026.0\n      2011-01-19\n      2.995732\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      2\n      500\n      Fight global hunger and support women farmers ...\n      14\n      Strengthening Communities\n      NaN\n      NY\n      2114.0\n      2011-01-21\n      6.214608\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      3\n      15\n      Stop 'N' Swap\n      31\n      Environment\n      NaN\n      NY\n      10455.0\n      2011-01-28\n      2.708050\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      4\n      15\n      Queens Stop 'N' Swap\n      135\n      Environment\n      NaN\n      NY\n      11372.0\n      2011-01-28\n      2.708050\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n\nCode\nvolunteer.columns\n\n\nIndex(['vol_requests', 'title', 'hits', 'category_desc', 'locality', 'region',\n       'postalcode', 'created_date', 'vol_requests_lognorm', 'created_month',\n       'Education', 'Emergency Preparedness', 'Environment', 'Health',\n       'Helping Neighbors in Need', 'Strengthening Communities'],\n      dtype='object')\n\n\n\n\nCode\n# Create a list of redundant column names to drop\nto_drop = [\"locality\", \"region\", \"category_desc\", \"created_date\", \"vol_requests\"]\n\n# Drop those columns from the dataset\nvolunteer_subset = volunteer.drop(to_drop, axis=1)\n\n# Print out the head of the new dataset\nvolunteer_subset.head()\n\n\n\n\n\n\n  \n    \n      \n      title\n      hits\n      postalcode\n      vol_requests_lognorm\n      created_month\n      Education\n      Emergency Preparedness\n      Environment\n      Health\n      Helping Neighbors in Need\n      Strengthening Communities\n    \n  \n  \n    \n      0\n      Web designer\n      22\n      10010.0\n      0.693147\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      1\n      Urban Adventures - Ice Skating at Lasker Rink\n      62\n      10026.0\n      2.995732\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      2\n      Fight global hunger and support women farmers ...\n      14\n      2114.0\n      6.214608\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      3\n      Stop 'N' Swap\n      31\n      10455.0\n      2.708050\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      4\n      Queens Stop 'N' Swap\n      135\n      11372.0\n      2.708050\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\n\n\nLet’s take a look at the wine dataset again, which is made up of continuous, numerical features. Run Pearson’s correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame.\n\n\nCode\nwine = pd.read_csv('dataset/wine_sample.csv')\nwine.head()\n\n\n\n\n\n\n  \n    \n      \n      Flavanoids\n      Total phenols\n      Malic acid\n      OD280/OD315 of diluted wines\n      Hue\n    \n  \n  \n    \n      0\n      3.06\n      2.80\n      1.71\n      3.92\n      1.04\n    \n    \n      1\n      2.76\n      2.65\n      1.78\n      3.40\n      1.05\n    \n    \n      2\n      3.24\n      2.80\n      2.36\n      3.17\n      1.03\n    \n    \n      3\n      3.49\n      3.85\n      1.95\n      3.45\n      0.86\n    \n    \n      4\n      2.69\n      2.80\n      2.59\n      2.93\n      1.04\n    \n  \n\n\n\n\n\n\nCode\n# Print out the column correlations of the wine dataset\nprint(wine.corr())\n\n\n                              Flavanoids  Total phenols  Malic acid  \\\nFlavanoids                      1.000000       0.864564   -0.411007   \nTotal phenols                   0.864564       1.000000   -0.335167   \nMalic acid                     -0.411007      -0.335167    1.000000   \nOD280/OD315 of diluted wines    0.787194       0.699949   -0.368710   \nHue                             0.543479       0.433681   -0.561296   \n\n                              OD280/OD315 of diluted wines       Hue  \nFlavanoids                                        0.787194  0.543479  \nTotal phenols                                     0.699949  0.433681  \nMalic acid                                       -0.368710 -0.561296  \nOD280/OD315 of diluted wines                      1.000000  0.565468  \nHue                                               0.565468  1.000000  \n\n\n\n\nCode\n# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\nto_drop = \"Flavanoids\"\n\n# Drop that column from the DataFrame\nwine = wine.drop(to_drop, axis=1)\n\n\n\n\n\n\n\n\nLet’s expand on the text vector exploration method we just learned about, using the volunteer dataset’s title tf/idf vectors. In this first part of text vector exploration, we’re going to add to that function we learned about in the slides. We’ll return a list of numbers with the function. In the next exercise, we’ll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector.\n\n\nCode\nvocab_csv = pd.read_csv('dataset/vocab.csv', index_col=0).to_dict()\nvocab = vocab_csv['0']\n\n\n\n\nCode\nvolunteer = volunteer[['category_desc', 'title']]\nvolunteer = volunteer.dropna(subset=['category_desc'], axis=0)\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Take the title text\ntitle_text = volunteer['title']\n\n# Create the vectorizer method\ntfidf_vec = TfidfVectorizer()\n\n# Transform the text into tf-idf vectors\ntext_tfidf = tfidf_vec.fit_transform(title_text)\n\n\n\n\nCode\n# Add in the rest of the parameters\ndef return_weights(vocab, original_vocab, vector, vector_index, top_n):\n    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n\n    # Let's transform that zipped dict into a series\n    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n\n    # Let's sort the series to pull out the top n weighted words\n    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n    return [original_vocab[i] for i in zipped_index]\n\n# Print out the weighted words\nprint(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=8, top_n=3))\n\n\n[189, 942, 466]\n\n\n\n\n\nUsing the function we wrote in the previous exercise, we’re going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.\n\n\nCode\ndef words_to_filter(vocab, original_vocab, vector, top_n):\n    filter_list = []\n    for i in range(0, vector.shape[0]):\n        # here we'll call the function from the previous exercise,\n        # and extend the list we're creating\n        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n        filter_list.extend(filtered)\n    # Return the list in a set, so we don't get duplicate word indices\n    return set(filter_list)\n\n# Call the function to get the list of word indices\nfiltered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, top_n=3)\n\n# By converting filtered_words back to a list,\n# we can use it to filter the columns in the text vector\nfiltered_text = text_tfidf[:, list(filtered_words)]\n\n\n\n\n\nLet’s re-run the Naive Bayes text classification model we ran at the end of chapter 3, with our selection choices from the previous exercise, on the volunteer dataset’s title and category_desc columns.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\ny = volunteer['category_desc']\n\n# Split the dataset according to the class distribution of category_desc,\n# using the filtered_text vector\nX_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y)\n\n# Fit the model to the training data\nnb.fit(X_train, y_train)\n\n# Print out the model's accuracy\nprint(nb.score(X_test, y_test))\n\n\n0.5032258064516129\n\n\nYou can see that our accuracy score wasn’t that different from the score at the end of chapter 3. That’s okay; the title field is a very small text field, appropriate for demonstrating how filtering vectors works.\n\n\n\n\n\nUnsupervised learning method\nCombine/decomposes a feature space\nFeature extraction\nPrincipal component analysis\n\nLinear transformation to uncorrelated space\nCaptures as much variance as possible in each component\n\nPCA caveats\n\nDifficult to interpret components\nEnd of preprocessing journey\n\n\n\n\nLet’s apply PCA to the wine dataset, to see if we can get an increase in our model’s accuracy.\n\n\nCode\nwine = pd.read_csv('dataset/wine_types.csv')\nwine.head()\n\n\n\n\n\n\n  \n    \n      \n      Type\n      Alcohol\n      Malic acid\n      Ash\n      Alcalinity of ash\n      Magnesium\n      Total phenols\n      Flavanoids\n      Nonflavanoid phenols\n      Proanthocyanins\n      Color intensity\n      Hue\n      OD280/OD315 of diluted wines\n      Proline\n    \n  \n  \n    \n      0\n      1\n      14.23\n      1.71\n      2.43\n      15.6\n      127\n      2.80\n      3.06\n      0.28\n      2.29\n      5.64\n      1.04\n      3.92\n      1065\n    \n    \n      1\n      1\n      13.20\n      1.78\n      2.14\n      11.2\n      100\n      2.65\n      2.76\n      0.26\n      1.28\n      4.38\n      1.05\n      3.40\n      1050\n    \n    \n      2\n      1\n      13.16\n      2.36\n      2.67\n      18.6\n      101\n      2.80\n      3.24\n      0.30\n      2.81\n      5.68\n      1.03\n      3.17\n      1185\n    \n    \n      3\n      1\n      14.37\n      1.95\n      2.50\n      16.8\n      113\n      3.85\n      3.49\n      0.24\n      2.18\n      7.80\n      0.86\n      3.45\n      1480\n    \n    \n      4\n      1\n      13.24\n      2.59\n      2.87\n      21.0\n      118\n      2.80\n      2.69\n      0.39\n      1.82\n      4.32\n      1.04\n      2.93\n      735\n    \n  \n\n\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\n\n# Set up PCA and the X vector for dimensionality reduction\npca = PCA()\nwine_X = wine.drop('Type', axis=1)\n\n# Apply PCA to the wine dataset X vector\ntransformed_X = pca.fit_transform(wine_X)\n\n# Look at the percentage of variance explained by the different components\nprint(pca.explained_variance_ratio_)\n\n\n[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n 8.25392788e-08]\n\n\n\n\n\nNow that we have run PCA on the wine dataset, let’s try training a model with it.\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\n\ny = wine['Type']\n\nknn = KNeighborsClassifier()\n\n# Split the transformed X and the y labels into training and test sets\nX_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)\n\n# Fit knn to the training data\nknn.fit(X_wine_train, y_wine_train)\n\n# Score knn on the test data and print it out\nprint(knn.score(X_wine_test, y_wine_test))\n\n\n0.6888888888888889"
  },
  {
    "objectID": "posts/Sharp Regression Discontinuity Design/Sharp Regression Discontinuity Design.html",
    "href": "posts/Sharp Regression Discontinuity Design/Sharp Regression Discontinuity Design.html",
    "title": "Sharp Regression Discontinuity Design: An Introduction",
    "section": "",
    "text": "A Sharp Regression Discontinuity Design (SRDD) employs regression discontinuity to estimate causal effects. When a threshold exists in the outcome variable, SRDD is used. SRDD can provide unbiased estimates of the causal effects of the treatment or intervention on the outcome variable by determining the threshold. This threshold can be a policy change, a cutoff score, or any other sharp change in the outcome variable.\nSRDD is based on the assumption that the treatment or intervention will have a sharp, discontinuous effect at the threshold. The basic concept is to compare the outcomes of individuals just above and just below the threshold. It is therefore possible to attribute any difference in outcomes between individuals who are near the threshold and those who are just below it to the intervention or treatment.\nIn SRDD, individuals are classified into treatment or control groups in accordance with their distance from the threshold. Individuals enrolled in the treatment group are those who are just above the threshold, while those enrolled in the control group are those who are just below the threshold. The control group is assumed to be identical to the treatment group except for the treatment or intervention.\nAs an example, suppose a university is considering changing its policy to provide financial aid to students who score above a specified threshold on a standardized test. The threshold for receiving the financial aid is 80 points. To estimate the causal effects of receiving financial aid on students’ GPAs, we will use an example.\nOur method of estimating this effect includes the collection of data on test scores, GPAs, and other relevant variables for all students at the university. Then, we divide the students into two groups: those who achieved a test score just above or below the 80-point threshold. In order to estimate the relationship between GPAs and financial aid received, we can control for other relevant variables and use a regression model.\nIn order to estimate causal effects in economics and the social sciences, Sharp Regression Discontinuity Design (RD) is often used as a research design. When a continuous independent variable has a threshold or cutoff value, RD is utilized, and units above or below the threshold are assigned to a treatment group and a control group, respectively. By using a Sharp RD design, the average treatment effect (ATE) at the threshold is estimated, which is the difference between the potential outcomes of the treatment and control groups.\nAssuming constant effects and linearity in the independent variable 𝑋𝑖:\n𝑌𝑖0 = 𝛼 + 𝛽𝑋𝑖\n𝑌𝑖1 = 𝑌𝑖0 + 𝜏\nUsing the switching equation 𝑌𝑖 = 𝑌𝑖0 + 𝑌𝑖1 - 𝑌𝑖0 𝐷𝑖, we get:\n𝑌𝑖 = 𝛼 + 𝛽𝑋𝑖 + 𝜏𝐷𝑖 + 𝜖𝑖\nwhere 𝑌𝑖 is the outcome variable for unit 𝑖, 𝑋𝑖 is the independent variable for unit 𝑖, 𝐷𝑖 is the treatment assignment for unit 𝑖 (equal to 1 if 𝑋𝑖 > 𝑐 and 0 otherwise), 𝜏 is the ATE at the threshold 𝑐, and 𝜖𝑖 is the error term.\nSharp RD designs estimate the ATE at the threshold using the conditional expectation:\n𝜏𝑆𝑅𝐷 = 𝔼[𝑌1 − 𝑌0|𝑋 = 𝑐]\nwhere 𝑌1 and 𝑌0 are the potential outcomes for the treatment and control groups, respectively.\nHere’s an example Python code to illustrate the process:\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\ndata = pd.read_csv('data.csv')\n\n# Define the threshold\nc = 60\n\n# Define the treatment assignment\ndata['treatment'] = (data['test_scores'] > c).astype(int)\n\n# Fit a regression model\nmodel = sm.OLS(data['test_scores'], sm.add_constant(data[['test_scores', 'treatment']]))\nresults = model.fit()\n\n# Print the regression results\nprint(results.summary())\n\n# Calculate the ATE at the threshold\ntau = results.params[2]\n\n# Print the ATE at the threshold\nprint('ATE at the threshold:', tau)\nIn this example, we first load the data into a Pandas dataframe. We define the threshold as 𝑐 = 60, and the treatment assignment as 1 if the test scores are above the threshold and 0 otherwise. We then fit a regression model with the test scores and treatment variables as independent variables, and the test scores as the dependent variable, using Ordinary Least Squares (OLS) estimator.\nWe can print the regression results to see the estimated coefficients for the independent variables, and the ATE at the threshold. We can also calculate the standard errors and t-statistics to test for the statistical significance of the ATE.\n\nNow let’s use following Python code to generate a plot to discuss\n# Plot the results\nfig, ax = plt.subplots()\nsns.regplot(x=data['test_scores'], y=data['test_scores'], scatter_kws={'alpha':0.3}, line_kws={'color':'red', 'linestyle':'--'}, ci=None, ax=ax)\nsns.regplot(x=data['test_scores'], y=data['test_scores'], scatter_kws={'alpha':0.3}, line_kws={'color':'blue'}, ci=None, ax=ax)\nsns.scatterplot(x=data['test_scores'], y=data['test_scores'], hue=data['treatment'], alpha=0.5, ax=ax)\nax.axvline(x=c, color='red', linestyle='--')\nax.set_xlabel('Test scores')\nax.set_ylabel('Test scores')\nplt.show()\n\nIn this code, we use the Seaborn library to create a plot of the relationship between the test scores and the potential outcomes for both the treatment and control groups, with the red dashed line representing the fitted regression line for the control group and the blue line representing the fitted regression line for the treatment group. The plot also shows the treatment assignment for each unit, and the vertical red dashed line representing the threshold 𝑐.\nThe plot clearly shows the sharp discontinuity in the test scores at the threshold, and the difference in the potential outcomes for the treatment and control groups. The ATE at the threshold can be interpreted as the causal effect of the treatment on the test scores, for the units that are just above and below the threshold.\nIn conclusion, Sharp Regression Discontinuity Design with Potential Outcome is a powerful tool for estimating causal effects when there is a threshold or cut-off value for a continuous independent variable, and the units above or below the threshold are assigned to a treatment or control group, respectively. By estimating the ATE at the threshold, we can estimate the causal effect of the treatment on the outcome variable, and control for other relevant variables using regression analysis."
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html",
    "title": "Simple Linear Regression Modeling",
    "section": "",
    "text": "We will learn the basics of this popular statistical model, what regression is, and how linear and logistic regressions differ. We’ll then learn how to fit simple linear regression models with numeric and categorical explanatory variables, and how to describe the relationship between the response and explanatory variables using model coefficients\nThis Simple Linear Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#fitting-a-linear-regression",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#fitting-a-linear-regression",
    "title": "Simple Linear Regression Modeling",
    "section": "Fitting a linear regression",
    "text": "Fitting a linear regression\nStraight lines are defined by two things:\n\nIntercept: The y value at the point when x is zero.\nSlope: The amount the y value increases if you increase x by one.\nEquation: y = intercept + slope ∗ x"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#linear-regression-with-ols",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#linear-regression-with-ols",
    "title": "Simple Linear Regression Modeling",
    "section": "Linear regression with ols()",
    "text": "Linear regression with ols()\nWhile sns.regplot() can display a linear regression trend line, it doesn’t give you access to the intercept and slope as variables, or allow you to work with the model results as variables. That means that sometimes you’ll need to run a linear regression yourself.\n\n\nCode\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n# Create the model object\nmdl_price_vs_conv = ols(\"price_twd_msq ~ n_convenience\", data=taiwan_real_estate)\n\n# Fit the model\nmdl_price_vs_conv = mdl_price_vs_conv.fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_conv.params)\n\n\nIntercept        8.224237\nn_convenience    0.798080\ndtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#categorical-explanatory-variables",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#categorical-explanatory-variables",
    "title": "Simple Linear Regression Modeling",
    "section": "Categorical explanatory variables",
    "text": "Categorical explanatory variables\nVariables that categorize observations are known as categorical variables. Known as levels, they have a limited number of values. Gender is a categorical variable that can take two levels: Male or Female.\nNumbers are required for regression analysis. It is therefore necessary to make the results interpretable when a categorical variable is included in a regression model.\nA set of binary variables is created by recoding categorical variables. The recoding process creates a contrast matrix table by “dummy coding”\nThere are two type of data variables: * Quantitative data: refers to amount * Data collected quantitatively represents actual amounts that can be added, subtracted, divided, etc. Quantitative variables can be: * discrete (integer variables): count of individual items in record e.g. No. of players * continuous (ratio variables): continuous / non-finite value measurements e.g. distance, age etc * Categorical: refers to grouping There are three types of categorical variables: * binary: yes / no e.g. head/tail of coin flip * nominal: group with no rank or order b/w them e.g. color, brand, species etc * ordinal: group that can be ranked in specific order e.g. rating scale in survey result"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#visualizing-numeric-vs.-categorical",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#visualizing-numeric-vs.-categorical",
    "title": "Simple Linear Regression Modeling",
    "section": "Visualizing numeric vs. categorical",
    "text": "Visualizing numeric vs. categorical\nIf the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn’t make sense. Instead, a good option is to draw a histogram for each category.\n\n\nCode\n# Histograms of price_twd_msq with 10 bins, split by the age of each house\nsns.displot(data=taiwan_real_estate,\n         x='price_twd_msq',\n         col='house_age_years',\n         bins=10)\n\n# Show the plot\nplt.show()\nprint(\"\\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest.\")\n\n\n\n\n\n\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest."
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#using-categories-to-calculate-means",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#using-categories-to-calculate-means",
    "title": "Simple Linear Regression Modeling",
    "section": "Using categories to calculate means",
    "text": "Using categories to calculate means\nUsing summary statistics for each category is a good way to explore categorical variables further. Using a categorical variable, you can calculate the mean and median of your response variable. Therefore, you can compare each category in more detail.\n\n\nCode\n# Calculate the mean of price_twd_msq, grouped by house age\nmean_price_by_age = taiwan_real_estate.groupby('house_age_years')['price_twd_msq'].mean()\n\n# Print the result\nprint(mean_price_by_age)\n\n\nhouse_age_years\n0 to 15     12.637471\n15 to 30     9.876743\n30 to 45    11.393264\nName: price_twd_msq, dtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#is-coefficient-of-linear-regression-model-is-mean-of-each-category",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#is-coefficient-of-linear-regression-model-is-mean-of-each-category",
    "title": "Simple Linear Regression Modeling",
    "section": "Is coefficient of linear regression model is mean of each category?",
    "text": "Is coefficient of linear regression model is mean of each category?\nWhile calculating linear regression with categorical explanatory variable, means of each category will also coefficient of linear regression but this hold true in case with only one categorical variable. Lets verify this\n\n\nCode\n# Create the model, fit it\nmdl_price_vs_age = ols(\"price_twd_msq ~ house_age_years\", data=taiwan_real_estate).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_age.params)\n\n\nIntercept                      12.637471\nhouse_age_years[T.15 to 30]    -2.760728\nhouse_age_years[T.30 to 45]    -1.244207\ndtype: float64\n\n\n\n\nCode\n# Update the model formula to remove the intercept\nmdl_price_vs_age0 = ols(\"price_twd_msq ~ house_age_years + 0\", data=taiwan_real_estate).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_age0.params)\nprint(\"\\n The coefficients of the model are just the means of each category you calculated previously. Fantastic job! \")\n\n\nhouse_age_years[0 to 15]     12.637471\nhouse_age_years[15 to 30]     9.876743\nhouse_age_years[30 to 45]    11.393264\ndtype: float64\n\n The coefficients of the model are just the means of each category you calculated previously. Fantastic job!"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html",
    "title": "Simple Logistic Regression Modeling",
    "section": "",
    "text": "Get a better understanding of logistic regression models. We will analyze real-world data to predict the likelihood of a customer closing their bank account in terms of probabilities of success and odds ratios, and quantify the performance of your model using confusion matrices.\nThis Simple Logistic Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression",
    "title": "Simple Logistic Regression Modeling",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nHow would the response variable be affected if it were binary or logical? It can be Yes/No, 1/0, Blue/Red, etc.\nFor categorical responses, a logistic regression model is another type of generalized linear model.\nAn S curve is drawn to represent the response. Probabilities can be considered to be the fitted values between 0 and 1."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#exploring-the-explanatory-variables",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#exploring-the-explanatory-variables",
    "title": "Simple Logistic Regression Modeling",
    "section": "Exploring the explanatory variables",
    "text": "Exploring the explanatory variables\nIn the case of a logical response variable, all points lie on the y=0 and y=1 lines, making it difficult to determine what is occurring. It was unclear how the explanatory variable was distributed on each line before you saw the trend line. A histogram of the explanatory variable, grouped by the response, can be used to resolve this problem.\nThese histograms will be used to gain an understanding of the financial services churn dataset\n\n\nCode\nchurn = pd.read_csv('dataset/churn.csv')\nprint(churn.head())\n\n\n   has_churned  time_since_first_purchase  time_since_last_purchase\n0            0                  -1.089221                 -0.721322\n1            0                   1.182983                  3.634435\n2            0                  -0.846156                 -0.427582\n3            0                   0.086942                 -0.535672\n4            0                  -1.166642                 -0.672640\n\n\n\n\nCode\n# Create the histograms of time_since_last_purchase split by has_churned\nsns.displot(data=churn,x='time_since_last_purchase',col='has_churned')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Redraw the plot with time_since_first_purchase\nsns.displot(data=churn,x='time_since_first_purchase', col='has_churned')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create the histograms of time_since_last_purchase split by has_churned\nsns.distplot(churn['time_since_last_purchase'])\n\n# Show the plot\nplt.show()\n\nprint(churn['has_churned'].unique())\n\nfor x in churn['has_churned'].unique():\n    values = churn.time_since_last_purchase[churn['has_churned'] == x]\n    sns.distplot(values, hist=False, rug=False)\n\ng = sns.FacetGrid(churn, col='has_churned', hue='has_churned')\np1 = g.map(sns.distplot, 'time_since_last_purchase', hist=True, rug=True).add_legend()\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(churn['time_since_last_purchase'])\n\n\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\n\n\n[0 1]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\nCode\n# Create the histograms of time_since_first_purchase split by has_churned\nsns.distplot(churn['time_since_first_purchase'])\n\n# Show the plot\nplt.show()\n\nprint(churn['has_churned'].unique())\n\nfor x in churn['has_churned'].unique():\n    values = churn.time_since_last_purchase[churn['has_churned'] == x]\n    sns.distplot(values, hist=False, rug=False)\n\ng = sns.FacetGrid(churn, col='has_churned', hue='has_churned')\np1 = g.map(sns.distplot, 'time_since_first_purchase', hist=True, rug=True).add_legend()\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(churn['time_since_first_purchase'])\n\n\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\n\n\n[0 1]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#visualizing-liner-and-logistic-model",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#visualizing-liner-and-logistic-model",
    "title": "Simple Logistic Regression Modeling",
    "section": "Visualizing liner and logistic model",
    "text": "Visualizing liner and logistic model\nA logistic regression model can be drawn using regplot() in the same manner as a linear regression without you having to concern yourself with the modeling code. Try drawing both trend lines side by side to see how linear and logistic regressions make different predictions. From the linear model, you should see a linear trend (straight line), whereas from the logistic model, you should see a logistic trend (S-shaped).\n\n\nCode\n# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(data=churn,x='time_since_first_purchase',y='has_churned'\n            ,line_kws={\"color\": \"red\"})\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(x=\"time_since_first_purchase\",\n            y=\"has_churned\",\n            data=churn,\n            ci=None,\n            line_kws={\"color\": \"red\"})\n\n# Draw a logistic regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(data=churn,x='time_since_first_purchase', y='has_churned',ci=None,logistic=True,line_kws={\"color\": \"blue\"})\n\nplt.show()\n\nprint(\"\\n The two models give similar predictions in some places, but notice the slight curve in the logistic model trend.\")\n\n\n\n\n\n\n The two models give similar predictions in some places, but notice the slight curve in the logistic model trend."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression-with-logit",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression-with-logit",
    "title": "Simple Logistic Regression Modeling",
    "section": "Logistic regression with logit()",
    "text": "Logistic regression with logit()\nLogistic regression requires another function from statsmodels.formula.api: logit(). It takes the same arguments as ols(): a formula and data argument. You then use .fit() to fit the model to the data.\n\n\nCode\n# Import logit\nfrom statsmodels.formula.api import logit\n\n# Fit a logistic regression of churn vs. length of relationship using the churn dataset\nmdl_churn_vs_relationship = logit('has_churned ~ time_since_first_purchase', data=churn).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_churn_vs_relationship.params)\n\n\nOptimization terminated successfully.\n         Current function value: 0.679663\n         Iterations 4\nIntercept                   -0.015185\ntime_since_first_purchase   -0.354795\ndtype: float64"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#predictions-and-odds-ratios",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#predictions-and-odds-ratios",
    "title": "Simple Logistic Regression Modeling",
    "section": "Predictions and odds ratios",
    "text": "Predictions and odds ratios\nOdds ratios\nTaking the probability that something will happen and dividing it by the probability that it will not happen. It is equal to (P/(1-P)). Probability in favor of / against. The data cannot be compared with the original data, but can instead be plotted using a special chart. This unit represents the probability of … occurring (3 times the probability of…). It is easy to interpret, the data cannot be altered easily, and it is precise.\nLog odds ratio\nIt is a nice property of odds ratios that they can be passed into a log() = linear regression. Data changes that are easy to interpret and precise.\nMost likely Outcome\nAccording to logistic regression, we discuss the rounded most likely outcome (response > 0.5 chance of churning, etc.) since response values can be interpreted as probabilities. This data is very easy to interpret, easy to change, and not precise (rounded).\nProbability\nOriginal data. Easy to interpret, not easy to change data on the fly, and precise.\nProbabilities\nWe will examine each of the four main ways of expressing a logistic regression model’s prediction in the following four exercises. Since the response variable is either “yes” or “no”, you can predict the probability of a “yes”. These probabilities will be calculated and visualized here.\n\n\nCode\nexplanatory_data = pd.DataFrame({'time_since_first_purchase': np.arange(-1.5, 4, .35)})\n\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n  has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n)\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned\n0                      -1.50     0.626448\n1                      -1.15     0.596964\n2                      -0.80     0.566762\n3                      -0.45     0.536056\n4                      -0.10     0.505074\n\n\n\n\nCode\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n    has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n)\n\nfig = plt.figure()\n\n# Create a scatter plot with logistic trend line\nsns.regplot(x='time_since_first_purchase', y='has_churned', data=churn, ci=None, logistic=True)\n\n# Overlay with prediction_data, colored red\nsns.scatterplot(x='time_since_first_purchase',y='has_churned',\ndata=prediction_data, color='red')\n\nplt.show()"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#most-likely-outcome",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#most-likely-outcome",
    "title": "Simple Logistic Regression Modeling",
    "section": "Most likely outcome",
    "text": "Most likely outcome\nA non-technical audience may appreciate you not discussing probabilities and simply explaining the most likely outcome. Thus, instead of stating that there is a 60% chance of a customer leaving, you state that churn is the most likely outcome. There is a trade-off here between easier interpretation and nuance.\n\n\nCode\n# Update prediction data by adding most_likely_outcome\nprediction_data[\"most_likely_outcome\"] = np.round(prediction_data['has_churned'])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome\n0                      -1.50     0.626448                  1.0\n1                      -1.15     0.596964                  1.0\n2                      -0.80     0.566762                  1.0\n3                      -0.45     0.536056                  1.0\n4                      -0.10     0.505074                  1.0\n\n\n\n\nCode\n# Update prediction data by adding most_likely_outcome\nprediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n\nfig = plt.figure()\n\n# Create a scatter plot with logistic trend line (from previous exercise)\nsns.regplot(x=\"time_since_first_purchase\",\n            y=\"has_churned\",\n            data=churn,\n            ci=None,\n            logistic=True)\n\n# Overlay with prediction_data, colored red\nsns.scatterplot(x='time_since_first_purchase', y='most_likely_outcome', data=prediction_data, color='red')\n\nplt.show()\nprint(\"\\nThe most likely outcome is that you will master logistic regression! Providing the most likely response is a great way to share the model results with a non-technical audience.\")\n\n\n\n\n\n\nThe most likely outcome is that you will master logistic regression! Providing the most likely response is a great way to share the model results with a non-technical audience."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#odds-ratio",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#odds-ratio",
    "title": "Simple Logistic Regression Modeling",
    "section": "Odds ratio",
    "text": "Odds ratio\nAn odds ratio is a measure of the probability of something occurring compared to the probability that it will not occur. Often, this is easier to understand than probabilities, particularly when making decisions regarding choices. If, for example, a customer has a 20% chance of churning, it may be more intuitive to state “the chances of them not churning are four times higher than the chances of them churning.”.\n\n\nCode\n# Update prediction data with odds_ratio\nprediction_data[\"odds_ratio\"] =  prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio\n0                      -1.50     0.626448                  1.0    1.677003\n1                      -1.15     0.596964                  1.0    1.481166\n2                      -0.80     0.566762                  1.0    1.308199\n3                      -0.45     0.536056                  1.0    1.155431\n4                      -0.10     0.505074                  1.0    1.020502\n\n\n\n\nCode\n# Update prediction data with odds_ratio\nprediction_data[\"odds_ratio\"] = prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n\nfig = plt.figure()\n\n# Create a line plot of odds_ratio vs time_since_first_purchase\nsns.lineplot(x='time_since_first_purchase', y='odds_ratio',data=prediction_data)\n\n# Add a dotted horizontal line at odds_ratio = 1\nplt.axhline(y=1, linestyle=\"dotted\")\n\nplt.show()\n\nprint(\"\\nOdds ratios provide an alternative to probabilities that make it easier to compare positive and negative responses\")\n\n\n\n\n\n\nOdds ratios provide an alternative to probabilities that make it easier to compare positive and negative responses"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#log-odds-ratio",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#log-odds-ratio",
    "title": "Simple Logistic Regression Modeling",
    "section": "Log odds ratio",
    "text": "Log odds ratio\nThe disadvantage of probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. It is therefore difficult to understand what happens to the prediction when the explanatory variable is changed. The logarithm of the odds ratio (the “log odds ratio” or “logit”) does exhibit a linear relationship between predicted response and explanatory variable. As the explanatory variable changes, the response metric does not change significantly - only linearly.\nFor visualization purposes, it is usually better to plot the odds ratio and apply a log transformation to the y-axis scale since the actual values of log odds ratio are less intuitive than (linear) odds ratio.\n\n\nCode\n# Update prediction data with log_odds_ratio\nprediction_data[\"log_odds_ratio\"] = np.log(prediction_data['odds_ratio'])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio  \\\n0                      -1.50     0.626448                  1.0    1.677003   \n1                      -1.15     0.596964                  1.0    1.481166   \n2                      -0.80     0.566762                  1.0    1.308199   \n3                      -0.45     0.536056                  1.0    1.155431   \n4                      -0.10     0.505074                  1.0    1.020502   \n\n   log_odds_ratio  \n0        0.517008  \n1        0.392830  \n2        0.268651  \n3        0.144473  \n4        0.020295  \n\n\n\n\nCode\n# Update prediction data with log_odds_ratio\nprediction_data[\"log_odds_ratio\"] = np.log(prediction_data[\"odds_ratio\"])\n\nfig = plt.figure()\n\n# Update the line plot: log_odds_ratio vs. time_since_first_purchase\nsns.lineplot(x=\"time_since_first_purchase\",\n             y=\"log_odds_ratio\",\n             data=prediction_data)\n\n# Add a dotted horizontal line at log_odds_ratio = 0\nplt.axhline(y=0, linestyle=\"dotted\")\n\nplt.show()\nprint(\"\\nThe linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about.\")\n\n\n\n\n\n\nThe linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#section",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#section",
    "title": "Simple Logistic Regression Modeling",
    "section": "",
    "text": "Quantifying logistic regression fit\nResid plot, QQplot & Scale location plot are less useful in the case of logistic regression. Instead, we can use confusion matrices to analyze the fit performance. With True/False positive & negative outcomes. We can also compute metrics based on various ratios.\nAccuracy : proportion of correct predictions. Higher better.\nTN+TP / (TN+FN+FP+TP)\nSensitivity : proportions of observations where the actual response was true and where the model also predicted it was true. Higher better.\nTP / (FN + TP)\nSpecificity : proportions of observations where the actual was false and where the model also predicted it was false. Higher better.\nTN / (TN + FP) Calculating the confusion matrix\nA confusion matrix (occasionally called a confusion table) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.\nTrue positive: The customer churned and the model predicted they would.\nFalse positive: The customer didn't churn, but the model predicted they would.\nTrue negative: The customer didn't churn and the model predicted they wouldn't.\nFalse negative: The customer churned, but the model predicted they wouldn't.\n\n\nCode\n# Get the actual responses\nactual_response = churn[\"has_churned\"]\n\n# Get the predicted responses\npredicted_response = np.round(mdl_churn_vs_relationship.predict())\n\n# Create outcomes as a DataFrame of both Series\noutcomes = pd.DataFrame({\"actual_response\": actual_response,\n                         \"predicted_response\": predicted_response})\n\n# Print the outcomes\nprint(outcomes.value_counts(sort = False))\n\n\nactual_response  predicted_response\n0                0.0                   112\n                 1.0                    88\n1                0.0                    76\n                 1.0                   124\ndtype: int64\n\n\n\n\nCode\nconf_matrix = pd.crosstab(outcomes['actual_response'], outcomes['predicted_response'], rownames=['Actual'], colnames=['Predicted'])\nprint(conf_matrix)\n\n\nPredicted  0.0  1.0\nActual             \n0          112   88\n1           76  124"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#drawing-a-mosaic-plot-of-the-confusion-matrix",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#drawing-a-mosaic-plot-of-the-confusion-matrix",
    "title": "Simple Logistic Regression Modeling",
    "section": "Drawing a mosaic plot of the confusion matrix",
    "text": "Drawing a mosaic plot of the confusion matrix\nWhile calculating the performance matrix might be fun, it would become tedious if you needed multiple confusion matrices of different models. Luckily, the .pred_table() method can calculate the confusion matrix for you.\nAdditionally, you can use the output from the .pred_table() method to visualize the confusion matrix, using the mosaic() function.\n\n\nCode\n# Import mosaic from statsmodels.graphics.mosaicplot\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# Calculate the confusion matrix conf_matrix\nconf_matrix = mdl_churn_vs_relationship.pred_table()\n\n# Print it\nprint(conf_matrix)\n\n# Draw a mosaic plot of conf_matrix\nmosaic(conf_matrix)\nplt.show()\n\n\n[[112.  88.]\n [ 76. 124.]]"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#measuring-logistic-model-performance",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#measuring-logistic-model-performance",
    "title": "Simple Logistic Regression Modeling",
    "section": "Measuring logistic model performance",
    "text": "Measuring logistic model performance\nAs you know by now, several metrics exist for measuring the performance of a logistic regression model. In this last exercise, you’ll manually calculate accuracy, sensitivity, and specificity.\nAccuracy is the proportion of predictions that are correct.\naccuracy = (TN + TP) / TN + FN + FP + TP\nSensitivity is the proportion of true observations that are correctly predicted by the model as being true\nsensitivity = TP / (TP + FN)\nspecificity is the proportion of false observations that are correctly predicted by the model as being false.\nspecificity = TN / (TN + FP)\n\n\nCode\n# Extract TN, TP, FN and FP from conf_matrix\nTN = conf_matrix[0,0]\nTP = conf_matrix[1,1]\nFN = conf_matrix[1,0]\nFP = conf_matrix[0,1]\n\n# Calculate and print the accuracy\naccuracy = (TN + TP) / (TN + FN + FP + TP)\nprint(\"accuracy: \", accuracy)\n\n# Calculate and print the sensitivity\nsensitivity = TP / (TP + FN)\nprint(\"sensitivity: \", sensitivity)\n\n# Calculate and print the specificity\nspecificity = TN / (TN + FP)\nprint(\"specificity: \", specificity)\n\nprint(\"\\n Using these metrics, it becomes much easier to interpret and compare logistic regression models.\")\n\n\naccuracy:  0.59\nsensitivity:  0.62\nspecificity:  0.56\n\n Using these metrics, it becomes much easier to interpret and compare logistic regression models."
  },
  {
    "objectID": "posts/Standardizing data/Standardizing Data.html",
    "href": "posts/Standardizing data/Standardizing Data.html",
    "title": "Standardizing Data",
    "section": "",
    "text": "Standardizing data is all about making sure that your data fits the assumptions that the model is making about the distribution or amount of features you have. Standardizing your data will help make sure that it fits these assumptions and will ultimately improve your algorithm’s performance.\nThis Standardizing Data is part of Datacamp course: Preprocessing for Machine Learning in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\nStandardization\n\nPreprocessing method used to transform continuous data to make it look normally distributed\nScikit-learn models assume normally distributed data\n\nLog normalization\nfeature Scaling\n\n\nWhen to standardize: models\n\nModel in linear space\nDataset features have high variance\nDataset features are continuous and on different scales\nLinearity assumptions\n\n\n\n\nLet’s take a look at what might happen to your model’s accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you’ll learn about in the next section.\nThe scikit-learn model training process should be familiar to you at this point, so we won’t go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the X and y sets you need to fit and score on.\n\n\nCode\nwine = pd.read_csv('dataset/wine_types.csv')\nwine.head()\n\n\n\n\n\n\n  \n    \n      \n      Type\n      Alcohol\n      Malic acid\n      Ash\n      Alcalinity of ash\n      Magnesium\n      Total phenols\n      Flavanoids\n      Nonflavanoid phenols\n      Proanthocyanins\n      Color intensity\n      Hue\n      OD280/OD315 of diluted wines\n      Proline\n    \n  \n  \n    \n      0\n      1\n      14.23\n      1.71\n      2.43\n      15.6\n      127\n      2.80\n      3.06\n      0.28\n      2.29\n      5.64\n      1.04\n      3.92\n      1065\n    \n    \n      1\n      1\n      13.20\n      1.78\n      2.14\n      11.2\n      100\n      2.65\n      2.76\n      0.26\n      1.28\n      4.38\n      1.05\n      3.40\n      1050\n    \n    \n      2\n      1\n      13.16\n      2.36\n      2.67\n      18.6\n      101\n      2.80\n      3.24\n      0.30\n      2.81\n      5.68\n      1.03\n      3.17\n      1185\n    \n    \n      3\n      1\n      14.37\n      1.95\n      2.50\n      16.8\n      113\n      3.85\n      3.49\n      0.24\n      2.18\n      7.80\n      0.86\n      3.45\n      1480\n    \n    \n      4\n      1\n      13.24\n      2.59\n      2.87\n      21.0\n      118\n      2.80\n      2.69\n      0.39\n      1.82\n      4.32\n      1.04\n      2.93\n      735\n    \n  \n\n\n\n\n\n\nCode\nX = wine[['Proline', 'Total phenols', 'Hue', 'Nonflavanoid phenols']]\ny = wine['Type']\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n\n# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n# Fit the k-nearest neighbors model to the training data\nknn.fit(X_train, y_train)\n\n# SCore the model on the test data\nprint(knn.score(X_test, y_test))\n\n\n0.7777777777777778\n\n\n\n\n\n\n\nApplies log transformation\nNatural log using the constant \\(e\\) (2.718)\nCaptures relative changes, the magnitude of change, and keeps everything in the positive space\n\n\n\nCheck the variance of the columns in the wine dataset.\n\n\nCode\nwine.describe()\n\n\n\n\n\n\n  \n    \n      \n      Type\n      Alcohol\n      Malic acid\n      Ash\n      Alcalinity of ash\n      Magnesium\n      Total phenols\n      Flavanoids\n      Nonflavanoid phenols\n      Proanthocyanins\n      Color intensity\n      Hue\n      OD280/OD315 of diluted wines\n      Proline\n    \n  \n  \n    \n      count\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n      178.000000\n    \n    \n      mean\n      1.938202\n      13.000618\n      2.336348\n      2.366517\n      19.494944\n      99.741573\n      2.295112\n      2.029270\n      0.361854\n      1.590899\n      5.058090\n      0.957449\n      2.611685\n      746.893258\n    \n    \n      std\n      0.775035\n      0.811827\n      1.117146\n      0.274344\n      3.339564\n      14.282484\n      0.625851\n      0.998859\n      0.124453\n      0.572359\n      2.318286\n      0.228572\n      0.709990\n      314.907474\n    \n    \n      min\n      1.000000\n      11.030000\n      0.740000\n      1.360000\n      10.600000\n      70.000000\n      0.980000\n      0.340000\n      0.130000\n      0.410000\n      1.280000\n      0.480000\n      1.270000\n      278.000000\n    \n    \n      25%\n      1.000000\n      12.362500\n      1.602500\n      2.210000\n      17.200000\n      88.000000\n      1.742500\n      1.205000\n      0.270000\n      1.250000\n      3.220000\n      0.782500\n      1.937500\n      500.500000\n    \n    \n      50%\n      2.000000\n      13.050000\n      1.865000\n      2.360000\n      19.500000\n      98.000000\n      2.355000\n      2.135000\n      0.340000\n      1.555000\n      4.690000\n      0.965000\n      2.780000\n      673.500000\n    \n    \n      75%\n      3.000000\n      13.677500\n      3.082500\n      2.557500\n      21.500000\n      107.000000\n      2.800000\n      2.875000\n      0.437500\n      1.950000\n      6.200000\n      1.120000\n      3.170000\n      985.000000\n    \n    \n      max\n      3.000000\n      14.830000\n      5.800000\n      3.230000\n      30.000000\n      162.000000\n      3.880000\n      5.080000\n      0.660000\n      3.580000\n      13.000000\n      1.710000\n      4.000000\n      1680.000000\n    \n  \n\n\n\n\nThe Proline column has an extremely high variance.\n\n\n\nNow that we know that the Proline column in our wine dataset has a large amount of variance, let’s log normalize it.\n\n\nCode\n# Print out the variance of the Proline column\nprint(wine['Proline'].var())\n\n# Apply the log normalization function to the Proline column\nwine['Proline_log'] = np.log(wine['Proline'])\n\n# Check the variance of the normalized Proline column\nprint(wine['Proline_log'].var())\n\n\n99166.71735542436\n0.17231366191842012\n\n\n\n\n\n\n\nFeatures on different scales\nModel with linear characteristics\nCenter features around 0 and transform to unit variance(1)\nTransforms to approximately normal distribution\n\n\n\nWe want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it’s possible that these columns are all measured in different ways, which would bias a linear model. Using describe() to return descriptive statistics about this dataset, which of the following statements are true about the scale of data in these columns?\n\n\nCode\nwine[['Ash', 'Alcalinity of ash', 'Magnesium']].describe()\n\n\n\n\n\n\n  \n    \n      \n      Ash\n      Alcalinity of ash\n      Magnesium\n    \n  \n  \n    \n      count\n      178.000000\n      178.000000\n      178.000000\n    \n    \n      mean\n      2.366517\n      19.494944\n      99.741573\n    \n    \n      std\n      0.274344\n      3.339564\n      14.282484\n    \n    \n      min\n      1.360000\n      10.600000\n      70.000000\n    \n    \n      25%\n      2.210000\n      17.200000\n      88.000000\n    \n    \n      50%\n      2.360000\n      19.500000\n      98.000000\n    \n    \n      75%\n      2.557500\n      21.500000\n      107.000000\n    \n    \n      max\n      3.230000\n      30.000000\n      162.000000\n    \n  \n\n\n\n\n\n\n\nSince we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let’s standardize them in a way that allows for use in a linear model.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Create the scaler\nss = StandardScaler()\n\n# Take a subset of the DataFrame you want to scale\nwine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n\nprint(wine_subset.iloc[:3])\n\n# Apply the scaler to the DataFrame subset\nwine_subset_scaled = ss.fit_transform(wine_subset)\n\nprint(wine_subset_scaled[:3])\n\n\n    Ash  Alcalinity of ash  Magnesium\n0  2.43               15.6        127\n1  2.14               11.2        100\n2  2.67               18.6        101\n[[ 0.23205254 -1.16959318  1.91390522]\n [-0.82799632 -2.49084714  0.01814502]\n [ 1.10933436 -0.2687382   0.08835836]]\n\n\n\n\n\n\n\n\nLet’s first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data. The knn model as well as the X and y data and labels sets have been created already. Most of this process of creating models in scikit-learn should look familiar to you.\n\n\nCode\nwine = pd.read_csv('dataset/wine_types.csv')\n\nX = wine.drop('Type', axis=1)\ny = wine['Type']\n\nknn = KNeighborsClassifier()\n\n\n\n\nCode\n# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n# Fit the k-nearest neighbors model to the training data\nknn.fit(X_train, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test, y_test))\n\n\n0.7555555555555555\n\n\n\n\n\nThe accuracy score on the unscaled wine dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data.\n\n\nCode\nknn = KNeighborsClassifier()\n\n# Create the scaling method\nss = StandardScaler()\n\n# Apply the scaling method to the dataset used for modeling\nX_scaled = ss.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n\n# Fit the k-nearest neighbors model to the training data.\nknn.fit(X_train, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test, y_test))\n\n\n0.9333333333333333"
  },
  {
    "objectID": "posts/Summary of statistics/Summary Of Statistics.html",
    "href": "posts/Summary of statistics/Summary Of Statistics.html",
    "title": "Summary Of Statistics",
    "section": "",
    "text": "Datacamp course: Introduction to Statistic in Python\nThe study of statistics involves collecting, analyzing, and interpreting data. You can use it to bring the future into focus and infer answers to tons of questions. How many calls will your support team receive, and how many jeans sizes should you manufacture to fit 95% of the population? Statistical skills are developed in this course, which teaches you how to calculate averages, plot relationships between numeric values, and calculate correlations. In addition, you’ll learn how to conduct a well-designed study using Python to draw your own conclusions.\nCourse Takeaways:\n\nSummary Statistics\nRandom Numbers & Probability\nMore Distributions and the Central Limit Theorem\nCorrelation and Experimental Design\n\n\n\nStatistics - what is it?\n\nStatistics is the practice and study of collecting and analyzing data\nA summary statistic is a fact about or a summary of some data\n\nHow can statistics be used?\n\nDoes a product have a high likelihood of being purchased? People are more likely to purchase the product if they are familiar with it\nIs there an alternative payment system available?\nCan you tell me how many occupants your hotel will have? In what ways can you optimize occupancy?\nTo meet the needs of 95% of the population, how many sizes of jeans should be manufactured?\nCan the same number of each size be produced?\nA/B tests: Which advertisement is more effective in motivating the purchase of a product?\n\n\n\n\nDescriptive: To describe & summarize data e.g. 25% ride bike, 35% take bus ride & 50% drive to work\nInferential : Use sample data to make inferences about a larger population e.g. what percent of people drive to work?\n\n\n\n\n\nNumeric (quantitative)\nContinuous (measured)\n\nairplance speed\ntime spent waiting\n\nDiscrete (counted)\n\nnumber of devices\nnumber of people\n\nCategorical (qualitative)\nNominal (unordered)\n\nsingle / married\ncountry of residence\n\nOrdinal (ordered) agree, disagree, strongly diagree\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\nCode\nfood_consumption=pd.read_csv('food_consumption.csv')\n\n\n\n\nCode\n# Filter for Belgium\nbe_consumption = food_consumption[food_consumption['country']=='Belgium']\n\n# Filter for USA\nusa_consumption = food_consumption[food_consumption['country']=='USA']\n\n# Calculate mean and median consumption in Belgium\nprint(np.mean(be_consumption['consumption']))\nprint(np.median(be_consumption['consumption']))\n\n# Calculate mean and median consumption in USA\nprint(np.mean(usa_consumption['consumption']))\nprint(np.median(usa_consumption['consumption']))\n\n\n42.13272727272727\n12.59\n44.650000000000006\n14.58\n\n\n\n\nCode\n# Subset for Belgium and USA only\nbe_and_usa = food_consumption[(food_consumption['country']=='Belgium') | (food_consumption['country']=='USA')]\n\n# Group by country, select consumption column, and compute mean and median\nprint(be_and_usa.groupby('country')['consumption'].agg([np.mean,np.median]))\n\n\n              mean  median\ncountry                   \nBelgium  42.132727   12.59\nUSA      44.650000   14.58\n\n\nMean vs Median\n\n\nCode\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Subset for food_category equals rice\nrice_consumption = food_consumption[food_consumption['food_category']=='rice']\n\n# Histogram of co2_emission for rice and show plot\nplt.hist(rice_consumption['co2_emission'])\nplt.show()\n\n\n\n\n\n\n\nCode\n# Calculate mean and median of co2_emission with .agg()\nprint(rice_consumption['co2_emission'].agg([np.mean,np.median]))\n\n\nmean      37.591615\nmedian    15.200000\nName: co2_emission, dtype: float64\n\n\n\n\n\n\nVariance: Average distance from each data point to the data’s mean\nStandard Deviation\n\n\n\nCode\n# Calculate the quartiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,5)))\n\n# Calculate the quintiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,6)))\n\n# Calculate the deciles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,11)))\n\n\n[   0.        5.21     16.53     62.5975 1712.    ]\n[   0.       3.54    11.026   25.59    99.978 1712.   ]\n[0.00000e+00 6.68000e-01 3.54000e+00 7.04000e+00 1.10260e+01 1.65300e+01\n 2.55900e+01 4.42710e+01 9.99780e+01 2.03629e+02 1.71200e+03]\n\n\n\n\n\nA variable’s variance and standard deviation are two of the most common ways to measure its spread, and you will practice calculating them in this exercise. Spread informs expectations. In other words, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10, they might sell 40 products one day, and one or two the next. Predictions require information like this.\n\n\nCode\n# Print variance and sd of co2_emission for each food_category\nprint(food_consumption.groupby('food_category')['co2_emission'].agg([np.var,np.std]))\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Create histogram of co2_emission for food_category 'beef'\nplt.hist(food_consumption[food_consumption['food_category']=='beef']['co2_emission'])\n# Show plot\nplt.show()\n\n# Create histogram of co2_emission for food_category 'eggs'\nplt.hist(food_consumption[food_consumption['food_category']=='eggs']['co2_emission'])\n# Show plot\nplt.show()\n\n\n                        var         std\nfood_category                          \nbeef           88748.408132  297.906710\ndairy          17671.891985  132.935669\neggs              21.371819    4.622966\nfish             921.637349   30.358481\nlamb_goat      16475.518363  128.356996\nnuts              35.639652    5.969895\npork            3094.963537   55.632396\npoultry          245.026801   15.653332\nrice            2281.376243   47.763754\nsoybeans           0.879882    0.938020\nwheat             71.023937    8.427570\n\n\n\n\n\n\n\n\nFinding outliers using IQR\nOutliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that’s less influenced by outliers. IQR is also often used to find outliers. If a value is less than Q1−1.5×IQRQ1−1.5×IQR or greater than Q3+1.5×IQRQ3+1.5×IQR, it’s considered an outlier.\n\n\nCode\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\nprint(emissions_by_country)\n\n\ncountry\nAlbania      1777.85\nAlgeria       707.88\nAngola        412.99\nArgentina    2172.40\nArmenia      1109.93\n              ...   \nUruguay      1634.91\nVenezuela    1104.10\nVietnam       641.51\nZambia        225.30\nZimbabwe      350.33\nName: co2_emission, Length: 130, dtype: float64\n\n\n\n\nCode\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quantiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n\n# Subset emissions_by_country to find outliers\noutliers = emissions_by_country[(emissions_by_country > upper) | (emissions_by_country < lower)]\nprint(outliers)\n\n\ncountry\nArgentina    2172.4\nName: co2_emission, dtype: float64\n\n\n\n\n\n\nIn this chapter, you’ll learn how to generate random samples and measure chance using probability. You’ll work with real-world sales data to calculate the probability of a salesperson being successful. Finally, you’ll use the binomial distribution to model events with binary outcomes"
  },
  {
    "objectID": "posts/Support vector machines/Support Vector Machines.html",
    "href": "posts/Support vector machines/Support Vector Machines.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "We will learn all about the details of support vector machines. We will explore about tuning hyperparameters for these models and using kernels to fit non-linear decision boundaries.\nThis Support Vector Machines is part of Datacamp course: Linear Classifiers in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = (10, 5)\n\n\n\n\nCode\ndef make_meshgrid(x, y, h=.02, lims=None):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n        x: data to base x-axis meshgrid on\n        y: data to base y-axis meshgrid on\n        h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n        xx, yy : ndarray\n    \"\"\"\n\n    if lims is None:\n        x_min, x_max = x.min() - 1, x.max() + 1\n        y_min, y_max = y.min() - 1, y.max() + 1\n    else:\n        x_min, x_max, y_min, y_max = lims\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, proba=False, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n        ax: matplotlib axes object\n        clf: a classifier\n        xx: meshgrid ndarray\n        yy: meshgrid ndarray\n        params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    if proba:\n        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,-1]\n        Z = Z.reshape(xx.shape)\n        out = ax.imshow(Z,extent=(np.min(xx), np.max(xx), np.min(yy), np.max(yy)),\n                        origin='lower', vmin=0, vmax=1, **params)\n        ax.contour(xx, yy, Z, levels=[0.5])\n    else:\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        out = ax.contourf(xx, yy, Z, **params)\n    return out\n\ndef plot_classifier(X, y, clf, ax=None, ticks=False, proba=False, lims=None):\n    # assumes classifier \"clf\" is already fit\n    X0, X1 = X[:, 0], X[:, 1]\n    xx, yy = make_meshgrid(X0, X1, lims=lims)\n\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n        show = True\n    else:\n        show = False\n\n    # can abstract some of this into a higher-level function for learners to call\n    cs = plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8, proba=proba)\n    if proba:\n        cbar = plt.colorbar(cs)\n        cbar.ax.set_ylabel('probability of red $\\Delta$ class', fontsize=20, rotation=270, labelpad=30)\n        cbar.ax.tick_params(labelsize=14)\n        #ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors=\\'k\\', linewidth=1)\n    labels = np.unique(y)\n    if len(labels) == 2:\n        ax.scatter(X0[y==labels[0]], X1[y==labels[0]], cmap=plt.cm.coolwarm,\n                   s=60, c='b', marker='o', edgecolors='k')\n        ax.scatter(X0[y==labels[1]], X1[y==labels[1]], cmap=plt.cm.coolwarm,\n                   s=60, c='r', marker='^', edgecolors='k')\n    else:\n        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=50, edgecolors='k', linewidth=1)\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    #     ax.set_xlabel(data.feature_names[0])\n    #     ax.set_ylabel(data.feature_names[1])\n    if ticks:\n        ax.set_xticks(())\n        ax.set_yticks(())\n        #     ax.set_title(title)\n    if show:\n        plt.show()\n    else:\n        return ax\n\n\n\n\n\nSupport Vector Machine (SVM)\n\nLinear Classifier  \nTrained using the hinge loss and L2 regularization\n\nSupport vector\n\nA training example not in the flat part of the loss diagram\nAn example that is incorrectly classified or close to the boundary\nIf an example is not a support vector, removing it has no effect on the model\nHaving a small number of support vectors makes kernel SVMs really fast\n\nMax-margin viewpoint\n\nThe SVM maximizes the “margin” for linearly separable datasets\nMargin: distance from the boundary to the closest points\n\n\n\n\nSupport vectors are defined as training examples that influence the decision boundary. In this exercise, we’ll observe this behavior by removing non support vectors from the training set.\n\n\nCode\nX = pd.read_csv('./dataset/wine_X.csv').to_numpy()\ny = pd.read_csv('./dataset/wine_y.csv').to_numpy().ravel()\n\n\n\n\nCode\nfrom sklearn.svm import SVC\n\n# Train a linear SVM\nsvm = SVC(kernel='linear')\nsvm.fit(X, y)\nplot_classifier(X, y, svm, lims=(11, 15, 0, 6))\n\n# Make a new data set keeping only the support vectors\nprint(\"Number of original examples\", len(X))\nprint(\"Number of support vectors\", len(svm.support_))\nX_small = X[svm.support_]\ny_small = y[svm.support_]\n\n# Train a new SVM using only the support vectors\nsvm_small = SVC(kernel='linear')\nsvm_small.fit(X_small, y_small)\nplot_classifier(X_small, y_small, svm_small, lims=(11, 15, 0, 6))\nprint(\"\\nCompare the decision boundaries of the two trained models: are they the same? By the definition of support vectors, they should be!\")\n\n\n\n\n\nNumber of original examples 178\nNumber of support vectors 81\n\n\n\n\n\n\nCompare the decision boundaries of the two trained models: are they the same? By the definition of support vectors, they should be!\n\n\n\n\n\n\n\n\nwe saw that increasing the RBF kernel hyperparameter gamma increases training accuracy. Now we’ll search for the gamma that maximizes cross-validation accuracy using scikit-learn’s GridSearchCV. A binary version of the handwritten digits dataset, in which you’re just trying to predict whether or not an image is a “2”, is already loaded into the variables X and y.\n\n\nCode\nX = pd.read_csv('dataset/digits_2_X.csv').to_numpy()\ny = pd.read_csv('dataset/digits_2_y.csv').astype('bool').to_numpy().ravel()\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and runt the search\nparameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, param_grid=parameters)\nsearcher.fit(X, y)\n\n# Report the best parameters\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"\\nLarger values of gamma are better for training accuracy, but cross-validation helped us find something different (and better!)\")\n\n\nBest CV params {'gamma': 0.001}\n\nLarger values of gamma are better for training accuracy, but cross-validation helped us find something different (and better!)\n\n\n\n\n\nIn the previous exercise the best value of gamma was 0.001 using the default value of C, which is 1. In this exercise you’ll search for the best combination of C and gamma using GridSearchCV.\nAs in the previous exercise, the 2-vs-not-2 digits dataset is already loaded, but this time it’s split into the variables X_train, y_train, X_test, and y_test. Even though cross-validation already splits the training set into parts, it’s often a good idea to hold out a separate test set to make sure the cross-validation results are sensible.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\n\n\nCode\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, param_grid=parameters)\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\nprint(\"\\nNote that the best value of gamma, 0.0001, is different from the value of 0.001 that we got in the previous exercise, when we fixed C=1. Hyperparameters can affect each other!\")\n\n\nBest CV params {'C': 10, 'gamma': 0.0001}\nBest CV accuracy 1.0\nTest accuracy of best grid search hypers: 0.9955555555555555\n\nNote that the best value of gamma, 0.0001, is different from the value of 0.001 that we got in the previous exercise, when we fixed C=1. Hyperparameters can affect each other!\n\n\n\n\n\n\n\nLogistic regression:\n\nIs a linear classifier\nCan use with kernels, but slow\nOutputs meaningful probabilities\nCan be extended to multi-class\nAll data points affect fit\nL2 or L1 regularization\n\nSupport Vector Machine (SVM)\n\nIs a linear classifier\nCan use with kernels, and fast\nDoes not naturally output probabilities\nCan be extended to multi-class\nOnly “support vectors” affect fit\nConventionally just L2 regularization\n\n\n\n\nWe do a hyperparameter search over the regularization type, regularization strength, and the loss (logistic regression vs. linear SVM) using SGDClassifier().\n\n\nCode\nfrom sklearn.linear_model import SGDClassifier\n\n# We set random_state=0 for reproducibility\nlinear_classifier = SGDClassifier(random_state=0, max_iter=10000)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], 'loss':['hinge', 'log'],\n              'penalty':['l1', 'l2']}\nsearcher = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\nprint(\"Test accuacy of best grid search hypers:\", searcher.score(X_test, y_test))\nprint(\"\\nOne advantage of SGDClassifier is that it’s very fast – this would have taken a lot longer with LogisticRegression or LinearSVC. \")\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\n\n\nBest CV params {'alpha': 0.1, 'loss': 'log', 'penalty': 'l2'}\nBest CV accuracy 0.9985294117647058\nTest accuacy of best grid search hypers: 0.9955555555555555\n\nOne advantage of SGDClassifier is that it’s very fast – this would have taken a lot longer with LogisticRegression or LinearSVC. \n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\n\n\n\n\nCode\nfrom sklearn.linear_model import SGDClassifier\n\n# We set random_state=0 for reproducibility\nlinear_classifier = SGDClassifier(random_state=0, max_iter=10000)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], 'loss':['hinge', 'log_loss']}\nsearcher = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\nprint(\"Test accuacy of best grid search hypers:\", searcher.score(X_test, y_test))\nprint(\"\\nOne advantage of SGDClassifier is that it’s very fast – this would have taken a lot longer with LogisticRegression or LinearSVC. \")\n\n\nBest CV params {'alpha': 0.1, 'loss': 'log_loss'}\nBest CV accuracy 0.9985294117647058\nTest accuacy of best grid search hypers: 0.9955555555555555\n\nOne advantage of SGDClassifier is that it’s very fast – this would have taken a lot longer with LogisticRegression or LinearSVC."
  },
  {
    "objectID": "posts/Time series and machine learning primer/Time Series and Machine Learning Primer.html",
    "href": "posts/Time series and machine learning primer/Time Series and Machine Learning Primer.html",
    "title": "Time Series and Machine Learning Primer",
    "section": "",
    "text": "A brief introduction to the basics of machine learning, time series data, and the intersection between the two\nThis Time Series and Machine Learning Primer is part of Datacamp course: Machine Learning for Time Series Data in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (10, 5)\nplt.style.use('fivethirtyeight')\n\n\n\n\n\n\n\nFeature extraction\nModel fitting\nPrediction and validation\n\n\n\n\nIn this exercise, you’ll practice plotting the values of two time series without the time component.\n\n\nCode\ndata = pd.read_csv('dataset/data.csv', index_col=0)\ndata2 = pd.read_csv('dataset/data2.csv', index_col=0)\n\n\n\n\nCode\n# Plot the time series in each dataset\nfig, axs = plt.subplots(2, 1, figsize=(10, 10));\ndata.iloc[:1000].plot(y='data_values', ax=axs[0]);\ndata2.iloc[:1000].plot(y='data_values', ax=axs[1]);\n\n\n\n\n\n\n\n\nYou’ll now plot both the datasets again, but with the included time stamps for each (stored in the column called “time”). Let’s see if this gives you some more context for understanding each time series data.\n\n\nCode\ndata = pd.read_csv('dataset/data_time.csv', index_col=0)\ndata2 = pd.read_csv('dataset/data_time2.csv', index_col=0)\n\n\n\n\nCode\n# Plot the time series in each dataset\nfig, axs = plt.subplots(2, 1, figsize=(10, 10));\ndata.iloc[:1000].plot(x='time', y='data_values', ax=axs[0]);\ndata2.iloc[:1000].plot(x='time', y='data_values', ax=axs[1]);\n\n\n\n\n\n\n\n\n\n\nPreparing data for scikit-learn\n\nscikit-learn expects a particular structure of data: (samples, features)\nMake sure that your data is at least two-dimensional\nMake sure the first dimension is samples\n\n\n\n\nCode\ndata = pd.read_csv('dataset/iris.csv', index_col=0)\n\n\n\n\nCode\nsns.scatterplot(x='petal length (cm)', y='petal width (cm)', hue='target', data=data);\n\n\n\n\n\n\n\nCode\nfrom sklearn.svm import LinearSVC\n\n# Construct data for the model\nX = data[['petal length (cm)', 'petal width (cm)']]\ny = np.ravel(data[['target']])\n\n# Fit the model\nmodel = LinearSVC()\nmodel.fit(X, y)\n\n\nLinearSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearSVCLinearSVC()\n\n\n\n\nNow that you have fit your classifier, let’s use it to predict the type of flower (or class) for some newly-collected flowers.\nUsing the classifier you fit, you’ll predict the type of each flower.\n\n\nCode\ntargets = pd.read_csv('dataset/iris_target.csv', index_col=0)\n\n\n\n\nCode\n# Create input array\nX_predict = targets[['petal length (cm)', 'petal width (cm)']]\n\n# Predict with the model\npredictions = model.predict(X_predict)\nprint(predictions)\n\n# Visualize predictions and actual values\nplt.scatter(X_predict['petal length (cm)'], X_predict['petal width (cm)'],\n            c=predictions, cmap=plt.cm.coolwarm);\nplt.xlabel('petal length (cm)');\nplt.ylabel('petal width (cm)');\nplt.title(\"Predicted class values\");\nprint(\"\\nNote that the output of your predictions are all integers, representing that datapoint's predicted class\")\n\n\n[2 2 2 1 1 2 2 2 2 1 2 1 1 2 1 1 2 1 2 2]\n\nNote that the output of your predictions are all integers, representing that datapoint's predicted class\n\n\n\n\n\n\n\n\nIn this exercise, you’ll practice fitting a regression model using data from the Boston housing market.\n\n\nCode\nboston = pd.read_csv('dataset/boston.csv', index_col=0)\nboston.head()\n\n\n\n\n\n\n  \n    \n      \n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      0\n      0.00632\n      18.0\n      2.31\n      0.0\n      0.538\n      6.575\n      65.2\n      4.0900\n      1.0\n      296.0\n      15.3\n      396.90\n      4.98\n    \n    \n      1\n      0.02731\n      0.0\n      7.07\n      0.0\n      0.469\n      6.421\n      78.9\n      4.9671\n      2.0\n      242.0\n      17.8\n      396.90\n      9.14\n    \n    \n      2\n      0.02729\n      0.0\n      7.07\n      0.0\n      0.469\n      7.185\n      61.1\n      4.9671\n      2.0\n      242.0\n      17.8\n      392.83\n      4.03\n    \n    \n      3\n      0.03237\n      0.0\n      2.18\n      0.0\n      0.458\n      6.998\n      45.8\n      6.0622\n      3.0\n      222.0\n      18.7\n      394.63\n      2.94\n    \n    \n      4\n      0.06905\n      0.0\n      2.18\n      0.0\n      0.458\n      7.147\n      54.2\n      6.0622\n      3.0\n      222.0\n      18.7\n      396.90\n      5.33\n    \n  \n\n\n\n\n\n\nCode\nplt.scatter(boston['AGE'], boston['RM']);\n\n\n\n\n\n\n\nCode\nfrom sklearn import linear_model\n\n# Prepare input and output DataFrame\nX = boston[['AGE']]\ny = boston[['RM']]\n\n# Fit the model\nmodel = linear_model.LinearRegression()\nmodel.fit(X, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n\nNow that you’ve fit a model with the Boston housing data, lets see what predictions it generates on some new data. You can investigate the underlying relationship that the model has found between inputs and outputs by feeding in a range of numbers as inputs and seeing what the model predicts for each input.\n\n\nCode\nnew_inputs = np.array(pd.read_csv('dataset/boston_newinputs.csv', index_col=0, header=None).values)\n\n\n\n\nCode\n# Generate predictions with the model using those inputs\npredictions = model.predict(new_inputs.reshape(-1,1))\n\n# Visualizae the inputs and predicted values\nplt.scatter(new_inputs, predictions, color='r', s=3);\nplt.xlabel('inputs');\nplt.ylabel('predictions');\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\nIn these final exercises of this chapter, you’ll explore the two datasets you’ll use in this course.\nThe first is a collection of heartbeat sounds. Hearts normally have a predictable sound pattern as they beat, but some disorders can cause the heart to beat abnormally. This dataset contains a training set with labels for each type of heartbeat, and a testing set with no labels. You’ll use the testing set to validate your models.\nAs you have labeled data, this dataset is ideal for classification. In fact, it was originally offered as a part of a public Kaggle competition.\n\n\nCode\nimport librosa as lr\nfrom glob import glob\n\n# List all the wav files in the folder\naudio_files = glob('dataset/files/*.wav')\n\n# Read in the first audio file, create the time array\naudio, sfreq = lr.load(audio_files[0])\ntime = np.arange(0, len(audio)) / sfreq\n\n# Plot audio over time\nfig, ax = plt.subplots();\nax.plot(time, audio)\nax.set(xlabel='Time (s)', ylabel='Sound Amplitude');\nprint(\"\\nThere are several seconds of heartbeat sounds in here, though note that most of this time is silence. A common procedure in machine learning is to separate the datapoints with lots of stuff happening from the ones that don't.\")\n\n\n\nThere are several seconds of heartbeat sounds in here, though note that most of this time is silence. A common procedure in machine learning is to separate the datapoints with lots of stuff happening from the ones that don't.\n\n\n\n\n\n\n\n\nThe next dataset contains information about company market value over several years of time. This is one of the most popular kind of time series data used for regression. If you can model the value of a company as it changes over time, you can make predictions about where that company will be in the future. This dataset was also originally provided as part of a public Kaggle competition.\nIn this exercise, you’ll plot the time series for a number of companies to get an understanding of how they are (or aren’t) related to one another.\n\n\nCode\n# Read in the data\ndata = pd.read_csv('dataset/prices_nyse.csv', index_col=0)\n\n# Convert the index of the DataFrame to datetime\ndata.index = pd.to_datetime(data.index)\nprint(data.head())\n\n# Loop through each column, plot its values over time\nfig, ax = plt.subplots();\nfor column in data.columns:\n    data[column].plot(ax=ax, label=column);\nax.legend();\nprint(\"\\nNote that each company's value is sometimes correlated with others, and sometimes not. Also note there are a lot of 'jumps' in there - what effect do you think these jumps would have on a predictive model?\")\n\n\n                  AAPL  FB       NFLX          V        XOM\ntime                                                       \n2010-01-04  214.009998 NaN  53.479999  88.139999  69.150002\n2010-01-05  214.379993 NaN  51.510001  87.129997  69.419998\n2010-01-06  210.969995 NaN  53.319999  85.959999  70.019997\n2010-01-07  210.580000 NaN  52.400001  86.760002  69.800003\n2010-01-08  211.980005 NaN  53.300002  87.000000  69.519997\n\nNote that each company's value is sometimes correlated with others, and sometimes not. Also note there are a lot of 'jumps' in there - what effect do you think these jumps would have on a predictive model?"
  },
  {
    "objectID": "posts/Time series as inputs to model/Time Series as Inputs to a Model.html",
    "href": "posts/Time series as inputs to model/Time Series as Inputs to a Model.html",
    "title": "Time Series as Inputs to a Model",
    "section": "",
    "text": "To include time series in your machine learning pipeline, you use them as features in your model. We will briefly covers the common features that are extracted from time series for machine learning.\nThis Time Series as Inputs to a Model is part of Datacamp course: Machine Learning for Time Series Data in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa as lr\n\nplt.rcParams['figure.figsize'] = (10, 5)\nplt.style.use('fivethirtyeight')\n\n\n\n\n\n\nIn this exercise, you’ll start with perhaps the simplest classification technique: averaging across dimensions of a dataset and visually inspecting the result.\nYou’ll use the heartbeat data described in the last chapter. Some recordings are normal heartbeat activity, while others are abnormal activity. Let’s see if you can spot the difference.\nTwo DataFrames, normal and abnormal, each with the shape of (n_times_points, n_audio_files) containing the audio for several heartbeats are available in your workspace.\n\n\nCode\ndef show_plot_and_make_titles():\n    axs[0, 0].set(title=\"Normal Heartbeats\")\n    axs[0, 1].set(title=\"Abnormal Heartbeats\")\n    plt.tight_layout()\n\n\n\n\nCode\nnormal = pd.read_csv('dataset/normal_sound.csv', index_col=0)\nabnormal = pd.read_csv('dataset/abnormal_sound.csv', index_col=0)\nsfreq = 2205\n\n\n\n\nCode\nfig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n\n# Calculate the time array\ntime = np.arange(len(normal)) / sfreq\n\n# Stack the normal/abnormal audio so you can loop and plot\nstacked_audio = np.hstack([normal, abnormal]).T\n\n# Loop through each audio file / ax object and plot\n# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\nfor iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n    ax.plot(time, iaudio)\nshow_plot_and_make_titles()\nprint(\"\\nAs you can see there is a lot of variability in the raw data, let's see if you can average out some of that noise to notice a difference.\")\n\n\n\nAs you can see there is a lot of variability in the raw data, let's see if you can average out some of that noise to notice a difference.\n\n\n\n\n\n\n\n\nWhile you should always start by visualizing your raw data, this is often uninformative when it comes to discriminating between two classes of data points. Data is usually noisy or exhibits complex patterns that aren’t discoverable by the naked eye.\nAnother common technique to find simple differences between two sets of data is to average across multiple instances of the same class. This may remove noise and reveal underlying patterns (or, it may not).\nIn this exercise, you’ll average across many instances of each class of heartbeat sound.\n\n\nCode\n# Average across the audio files of each DataFrame\nmean_normal = np.mean(normal, axis=1)\nmean_abnormal = np.mean(abnormal, axis=1)\n\n# Plot each average over time\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\nax1.plot(time, mean_normal);\nax1.set(title='Normal Data');\nax2.plot(time, mean_abnormal);\nax2.set(title='Abnormal Data');\n\n\n\n\n\n\n\n\nWhile eye-balling differences is a useful way to gain an intuition for the data, let’s see if you can operationalize things with a model. In this exercise, you will use each repetition as a datapoint, and each moment in time as a feature to fit a classifier that attempts to predict abnormal vs. normal heartbeats using only the raw data.\n\n\nCode\nnormal = pd.read_csv('dataset/heart_normal.csv', index_col=0)\nabnormal = pd.read_csv('dataset/heart_abnormal.csv', index_col=0)\n\n\n\n\nCode\nnormal_train_idx = np.random.choice(normal.shape[1], size=22, replace=False).tolist()\nnormal_test_idx = list(set(np.arange(normal.shape[1]).tolist()) - set(normal_train_idx))\n\nabnormal_train_idx = np.random.choice(abnormal.shape[1], size=20, replace=False).tolist()\nabnormal_test_idx = list(set(np.arange(abnormal.shape[1]).tolist()) - set(abnormal_train_idx))\n\nX_train = pd.concat([normal.iloc[:, normal_train_idx],\n                     abnormal.iloc[:, abnormal_train_idx]], axis=1).to_numpy().T\nX_test = pd.concat([normal.iloc[:, normal_test_idx],\n                    abnormal.iloc[:, abnormal_test_idx]], axis=1).to_numpy().T\n\ny_train = np.array(['normal'] * len(normal_train_idx) + ['abnormal'] * len(abnormal_train_idx))\ny_test = np.array(['normal'] * len(normal_test_idx) + ['abnormal'] * len(abnormal_test_idx))\n\n\n\n\nCode\nfrom sklearn.svm import LinearSVC\n\n# Initialize and fit the model\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\n\n# Generate predictions and score them manually\npredictions = model.predict(X_test)\nprint(sum(predictions == y_test.squeeze()) / len(y_test))\nprint(\"\\nNote that your predictions didn't do so well. That's because the features you're using as inputs to the model (raw data) aren't very good at differentiating classes. Next, you'll explore how to calculate some more complex features that may improve the results.\")\n\n\n0.5555555555555556\n\nNote that your predictions didn't do so well. That's because the features you're using as inputs to the model (raw data) aren't very good at differentiating classes. Next, you'll explore how to calculate some more complex features that may improve the results.\n\n\n\n\n\n\nThe auditory envelope\n\nSmooth the data to calculate the auditory envelope\nReleated to the total amount of audio energy present at each moment of time\n\nSmoothing over time\n\nInstead of averaging over all time, we can do a local average\nThis is called smoothing your timeseries\nIt removes short-term noise, while retaining the general pattern\n\nAuditory features: The Tempogram\n\nWe can summarize more complex temporal information with timeseries-specific functions\nlibrosa is a great library for auditory and timeseries feature engineering\nWe can calculate summary statistics of tempo in the same way that we can for the envelope\n\n\n\n\n\nOne of the ways you can improve the features available to your model is to remove some of the noise present in the data. In audio data, a common way to do this is to smooth the data and then rectify it so that the total amount of sound energy over time is more distinguishable. You’ll do this in the current exercise.\n\n\nCode\naudio, sfreq = lr.load('dataset/files/murmur__201108222238.wav')\ntime = np.arange(0, len(audio)) / sfreq\n\naudio = pd.DataFrame(audio)\n\n\n\n\nCode\nplt.plot(time[:2205], audio[:2205]);\n\n\n\n\n\n\n\nCode\n# Rectify the audio signal\naudio_rectified = audio.apply(np.abs)\n\n# Plot the result\nplt.plot(time[:2205], audio_rectified[:2205]);\n\n\n\n\n\n\n\nCode\n# Smooth by applying a rolling mean\naudio_rectified_smooth = audio_rectified.rolling(50).mean()\n\n# Plot the result\nplt.plot(time[:2205], audio_rectified_smooth[:2205])\nprint(\"\\nBy calculating the envelope of each sound and smoothing it, you've eliminated much of the noise and have a cleaner signal to tell you when a heartbeat is happening.\")\n\n\n\nBy calculating the envelope of each sound and smoothing it, you've eliminated much of the noise and have a cleaner signal to tell you when a heartbeat is happening.\n\n\n\n\n\n\n\n\nNow that you’ve removed some of the noisier fluctuations in the audio, let’s see if this improves your ability to classify.\n\n\nCode\n# Calculate stats\nmeans = np.mean(audio_rectified_smooth[:2205], axis=0)\nstds = np.std(audio_rectified_smooth[:2205], axis=0)\nmaxs = np.max(audio_rectified_smooth[:2205], axis=0)\n\nlabels=np.array(['murmur', 'murmur', 'murmur', 'normal', 'normal', 'murmur',\n       'normal', 'normal', 'murmur', 'murmur', 'normal', 'murmur',\n       'normal', 'murmur', 'murmur', 'normal', 'normal', 'normal',\n       'normal', 'murmur', 'normal', 'normal', 'murmur', 'murmur',\n       'normal', 'normal', 'normal', 'normal', 'murmur', 'murmur',\n       'murmur', 'normal', 'normal', 'normal', 'normal', 'murmur',\n       'murmur', 'murmur', 'normal', 'normal', 'normal', 'murmur',\n       'murmur', 'normal', 'murmur', 'murmur', 'murmur', 'murmur',\n       'normal', 'normal', 'murmur', 'normal', 'normal', 'murmur',\n       'murmur', 'normal', 'murmur', 'murmur', 'murmur', 'murmur'])\n# Create the X and y array\nX = np.column_stack([means, stds, maxs])\ny = labels.reshape([-1, 1])\n\n# Fit the model and score on testing data\nfrom sklearn.model_selection import cross_val_score\npercent_score = cross_val_score(model, X, y, cv=5)\nprint(np.mean(percent_score))\n\n\nValueError: Found input variables with inconsistent numbers of samples: [1, 60]\n\n\n# Calculate stats\nmeans = np.mean(audio_rectified_smooth[:2205], axis=0)\nstds = np.std(audio_rectified_smooth[:2205], axis=0)\nmaxs = np.max(audio_rectified_smooth[:2205], axis=0)\n\n# Create the X and y array\nX = np.column_stack([means, stds, maxs])\ny = labels.reshape([-1, 1])\n\n# Fit the model and score on testing data\nfrom sklearn.model_selection import cross_val_score\npercent_score = cross_val_score(model, X, y, cv=5)\nprint(np.mean(percent_score))\n\n\n\nOne benefit of cleaning up your data is that it lets you compute more sophisticated features. For example, the envelope calculation you performed is a common technique in computing tempo and rhythm features. In this exercise, you’ll use librosa to compute some tempo and rhythm features for heartbeat data, and fit a model once more.\nNote that librosa functions tend to only operate on numpy arrays instead of DataFrames, so we’ll access our Pandas data as a Numpy array with the .values attribute.\n\n\nCode\naudio = pd.read_csv('dataset/heart_normal.csv',index_col=0)\n\n\n\n\nCode\n# Calculate the tempo of the sounds\ntempos = []\nfor col, i_audio in audio.items():\n    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n\n# Convert the list to an array so you can manipulate it more easily\ntempos = np.array(tempos)\n\n# Calculate statistics of each tempo\ntempos_mean = tempos.mean(axis=-1)\ntempos_std = tempos.std(axis=-1)\ntempos_max = tempos.max(axis=-1)\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-0.00099522 -0.00338075 -0.00094784 ... -0.00023299 -0.00010334\n -0.0003675 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 2.80759443e-04  3.80784913e-04  6.25667381e-05 ... -5.90919633e-04\n -1.31961866e-03  6.51859853e-04] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[0.00295301 0.00303359 0.00029151 ... 0.01680872 0.00876233 0.00444242] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[0.00549661 0.01008847 0.00827173 ... 0.00813954 0.00619713 0.00364849] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 0.00043319  0.00055414  0.00023211 ...  0.00016555 -0.00036832\n -0.0012916 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 0.00131579 -0.00015355 -0.00194486 ... -0.01591193 -0.01542908\n -0.01310291] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-0.00169436 -0.00215661  0.0006191  ... -0.02194604 -0.02314005\n -0.02326724] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 0.00021126 -0.00194455  0.00614844 ... -0.00098258 -0.00219158\n -0.00377412] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 4.16483199e-05 -1.46395949e-04  4.75105007e-05 ... -1.81124444e-04\n  1.68072991e-04 -6.40757207e-04] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 0.00109218 -0.0055544  -0.00129727 ... -0.0003909   0.00268054\n  0.00576824] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-3.17522317e-05  5.89671458e-07 -1.28239612e-04 ...  2.21416631e-05\n -6.30965107e-04 -7.45263591e-04] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-1.68890948e-03 -1.32481696e-03  1.01418654e-02 ...  4.14374081e-05\n  1.19807897e-03  1.22049963e-02] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 0.00886944  0.00896404 -0.00719271 ...  0.00248993  0.00203395\n  0.00056616] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-0.01539746 -0.02647807 -0.04187883 ... -0.01472374 -0.00014676\n  0.0088793 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 4.97730616e-05 -3.91157118e-05 -1.05267245e-05 ...  4.12768350e-05\n -3.41438931e-06 -7.40338146e-05] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[0.00263497 0.00392065 0.00050858 ... 0.00199158 0.00321893 0.0036029 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[0.00214691 0.00588722 0.00717673 ... 0.00150915 0.00094547 0.00029681] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 0.00013853 -0.00022494 -0.00077886 ...  0.00027371 -0.0001696\n -0.0007767 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[9.03067179e-04 2.53955345e-03 2.63813254e-03 ... 6.55362310e-05\n 1.65422955e-06 3.77816723e-05] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 0.02368779  0.04664399  0.03966612 ... -0.00159054  0.00319402\n  0.00568775] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 0.00036906  0.00098006  0.00076451 ... -0.01618693 -0.01591166\n -0.01216409] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 2.58310338e-05 -6.33218806e-05 -4.30757937e-05 ...  1.37505558e-04\n  9.46284199e-05 -1.37633097e-05] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[0.00274239 0.00216137 0.00155329 ... 0.02098136 0.01238883 0.02040507] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[ 6.17577112e-04  4.75932844e-04 -2.37618315e-05 ...  5.41530571e-05\n -1.66832266e-04  9.08635848e-05] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-7.84732215e-03 -1.80613808e-02 -1.71636231e-02 ... -1.40546661e-04\n -1.09980232e-04 -1.59223055e-05] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-0.00325151  0.00729134  0.01644677 ... -0.06250024 -0.04183943\n -0.01117085] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[0.00879901 0.017107   0.01501772 ... 0.00858655 0.00877825 0.01070163] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-0.36173704 -0.65184158 -0.36568254 ... -0.00933582  0.00322876\n  0.01983791] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_27096\\3334237929.py:4: FutureWarning: Pass y=[-0.0016092  -0.00431922  0.00057256 ... -0.00097932  0.01224548\n  0.01697695] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n\n\n# Create the X and y arrays\nX = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\ny = labels.reshape([-1, 1])\n\n# Fit the model and score on testing data\npercent_score = cross_val_score(model, X, y, cv=5)\nprint(np.mean(percent_score))\n\n\n\n\n\nFourier transforms\n\nTimeseries data can be described as a combination of quickly-changing things and slowly-changing things\nAt each moment in time, we can describe the relative presence of fast- and slow-moving components\nThe simplest way to do this is called a Fourier Transform\nThis converts a single time series into an array that describe the timeseries as a combination of oscillations\n\nSpectrograms: combinations of windows Fourier transforms\n\nA spectrogram is a collection of windowsed Fourier transforms over time\nSimilar to how a rolling mean was calculated:\n\nChoose a windows size and shape\nAt a timepoint, calculate the FFT for that window\nSlide the window over by one\nAggregate the results\n\nCalled a Short-Time Fourier Transform (STFT)\n\nSpectral feature engineering\n\nEach timeseries has a different spectral pattern\nWe can calculate these spectral patterns by analyzing the spectrogram.\nFor example, spectral bandwidth and spectran centroids describe where most of the energy is at each moment in time.\n\n\n\n\nSpectral engineering is one of the most common techniques in machine learning for time series data. The first step in this process is to calculate a spectrogram of sound. This describes what spectral content (e.g., low and high pitches) are present in the sound over time. In this exercise, you’ll calculate a spectrogram of a heartbeat audio file.\n\n\nCode\naudio, sfreq = lr.load('dataset/files/murmur__201108222246.wav')\n\n\n\n\nCode\nfrom librosa.core import stft, amplitude_to_db\n\n# Prepare the STFT\nHOP_LENGTH = 2**4\n# spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n\n# For the test\nspec = pd.read_csv('dataset/spec.csv', index_col=0)\nspec = spec.applymap(complex)\ntime = np.array(normal.index)\naudio = pd.read_csv('dataset/audio.csv', index_col=0).to_numpy().squeeze()\nsfreq = 2205\n\n\n\n\nCode\nfrom librosa.display import specshow\n\n# Convert into decibels\nspec_db = amplitude_to_db(spec)\n\n# Compare the raw audio to the spectrogram of the audio\nfig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\naxs[0].plot(time,audio)\nspecshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH);\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: amplitude_to_db was called on complex input so phase information will be discarded. To suppress this warning, call amplitude_to_db(np.abs(S)) instead.\n  return f(*args, **kwargs)\n\n\n\n\n\n\n\n\nAs you can probably tell, there is a lot more information in a spectrogram compared to a raw audio file. By computing the spectral features, you have a much better idea of what’s going on. As such, there are all kinds of spectral features that you can compute using the spectrogram as a base. In this exercise, you’ll look at a few of these features.\n\n\nCode\nspec = np.array(np.abs(spec))\n\n\n\n\nCode\n# Calculate the spectral centroid and bandwidth for the spectrogram\nbandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\ncentroids = lr.feature.spectral_centroid(S=spec)[0]\n\n\n\n\nCode\n#hide\ntimes_spec = np.array([ 0.        ,  0.00725624,  0.01451247,  0.02176871,  0.02902494,\n        0.03628118,  0.04353741,  0.05079365,  0.05804989,  0.06530612,\n        0.07256236,  0.07981859,  0.08707483,  0.09433107,  0.1015873 ,\n        0.10884354,  0.11609977,  0.12335601,  0.13061224,  0.13786848,\n        0.14512472,  0.15238095,  0.15963719,  0.16689342,  0.17414966,\n        0.1814059 ,  0.18866213,  0.19591837,  0.2031746 ,  0.21043084,\n        0.21768707,  0.22494331,  0.23219955,  0.23945578,  0.24671202,\n        0.25396825,  0.26122449,  0.26848073,  0.27573696,  0.2829932 ,\n        0.29024943,  0.29750567,  0.3047619 ,  0.31201814,  0.31927438,\n        0.32653061,  0.33378685,  0.34104308,  0.34829932,  0.35555556,\n        0.36281179,  0.37006803,  0.37732426,  0.3845805 ,  0.39183673,\n        0.39909297,  0.40634921,  0.41360544,  0.42086168,  0.42811791,\n        0.43537415,  0.44263039,  0.44988662,  0.45714286,  0.46439909,\n        0.47165533,  0.47891156,  0.4861678 ,  0.49342404,  0.50068027,\n        0.50793651,  0.51519274,  0.52244898,  0.52970522,  0.53696145,\n        0.54421769,  0.55147392,  0.55873016,  0.56598639,  0.57324263,\n        0.58049887,  0.5877551 ,  0.59501134,  0.60226757,  0.60952381,\n        0.61678005,  0.62403628,  0.63129252,  0.63854875,  0.64580499,\n        0.65306122,  0.66031746,  0.6675737 ,  0.67482993,  0.68208617,\n        0.6893424 ,  0.69659864,  0.70385488,  0.71111111,  0.71836735,\n        0.72562358,  0.73287982,  0.74013605,  0.74739229,  0.75464853,\n        0.76190476,  0.769161  ,  0.77641723,  0.78367347,  0.79092971,\n        0.79818594,  0.80544218,  0.81269841,  0.81995465,  0.82721088,\n        0.83446712,  0.84172336,  0.84897959,  0.85623583,  0.86349206,\n        0.8707483 ,  0.87800454,  0.88526077,  0.89251701,  0.89977324,\n        0.90702948,  0.91428571,  0.92154195,  0.92879819,  0.93605442,\n        0.94331066,  0.95056689,  0.95782313,  0.96507937,  0.9723356 ,\n        0.97959184,  0.98684807,  0.99410431,  1.00136054,  1.00861678,\n        1.01587302,  1.02312925,  1.03038549,  1.03764172,  1.04489796,\n        1.0521542 ,  1.05941043,  1.06666667,  1.0739229 ,  1.08117914,\n        1.08843537,  1.09569161,  1.10294785,  1.11020408,  1.11746032,\n        1.12471655,  1.13197279,  1.13922902,  1.14648526,  1.1537415 ,\n        1.16099773,  1.16825397,  1.1755102 ,  1.18276644,  1.19002268,\n        1.19727891,  1.20453515,  1.21179138,  1.21904762,  1.22630385,\n        1.23356009,  1.24081633,  1.24807256,  1.2553288 ,  1.26258503,\n        1.26984127,  1.27709751,  1.28435374,  1.29160998,  1.29886621,\n        1.30612245,  1.31337868,  1.32063492,  1.32789116,  1.33514739,\n        1.34240363,  1.34965986,  1.3569161 ,  1.36417234,  1.37142857,\n        1.37868481,  1.38594104,  1.39319728,  1.40045351,  1.40770975,\n        1.41496599,  1.42222222,  1.42947846,  1.43673469,  1.44399093,\n        1.45124717,  1.4585034 ,  1.46575964,  1.47301587,  1.48027211,\n        1.48752834,  1.49478458,  1.50204082,  1.50929705,  1.51655329,\n        1.52380952,  1.53106576,  1.538322  ,  1.54557823,  1.55283447,\n        1.5600907 ,  1.56734694,  1.57460317,  1.58185941,  1.58911565,\n        1.59637188,  1.60362812,  1.61088435,  1.61814059,  1.62539683,\n        1.63265306,  1.6399093 ,  1.64716553,  1.65442177,  1.661678  ,\n        1.66893424,  1.67619048,  1.68344671,  1.69070295,  1.69795918,\n        1.70521542,  1.71247166,  1.71972789,  1.72698413,  1.73424036,\n        1.7414966 ,  1.74875283,  1.75600907,  1.76326531,  1.77052154,\n        1.77777778,  1.78503401,  1.79229025,  1.79954649,  1.80680272,\n        1.81405896,  1.82131519,  1.82857143,  1.83582766,  1.8430839 ,\n        1.85034014,  1.85759637,  1.86485261,  1.87210884,  1.87936508,\n        1.88662132,  1.89387755,  1.90113379,  1.90839002,  1.91564626,\n        1.92290249,  1.93015873,  1.93741497,  1.9446712 ,  1.95192744,\n        1.95918367,  1.96643991,  1.97369615,  1.98095238,  1.98820862,\n        1.99546485,  2.00272109,  2.00997732,  2.01723356,  2.0244898 ,\n        2.03174603,  2.03900227,  2.0462585 ,  2.05351474,  2.06077098,\n        2.06802721,  2.07528345,  2.08253968,  2.08979592,  2.09705215,\n        2.10430839,  2.11156463,  2.11882086,  2.1260771 ,  2.13333333,\n        2.14058957,  2.1478458 ,  2.15510204,  2.16235828,  2.16961451,\n        2.17687075,  2.18412698,  2.19138322,  2.19863946,  2.20589569,\n        2.21315193,  2.22040816,  2.2276644 ,  2.23492063,  2.24217687,\n        2.24943311,  2.25668934,  2.26394558,  2.27120181,  2.27845805,\n        2.28571429,  2.29297052,  2.30022676,  2.30748299,  2.31473923,\n        2.32199546,  2.3292517 ,  2.33650794,  2.34376417,  2.35102041,\n        2.35827664,  2.36553288,  2.37278912,  2.38004535,  2.38730159,\n        2.39455782,  2.40181406,  2.40907029,  2.41632653,  2.42358277,\n        2.430839  ,  2.43809524,  2.44535147,  2.45260771,  2.45986395,\n        2.46712018,  2.47437642,  2.48163265,  2.48888889,  2.49614512,\n        2.50340136,  2.5106576 ,  2.51791383,  2.52517007,  2.5324263 ,\n        2.53968254,  2.54693878,  2.55419501,  2.56145125,  2.56870748,\n        2.57596372,  2.58321995,  2.59047619,  2.59773243,  2.60498866,\n        2.6122449 ,  2.61950113,  2.62675737,  2.63401361,  2.64126984,\n        2.64852608,  2.65578231,  2.66303855,  2.67029478,  2.67755102,\n        2.68480726,  2.69206349,  2.69931973,  2.70657596,  2.7138322 ,\n        2.72108844,  2.72834467,  2.73560091,  2.74285714,  2.75011338,\n        2.75736961,  2.76462585,  2.77188209,  2.77913832,  2.78639456,\n        2.79365079,  2.80090703,  2.80816327,  2.8154195 ,  2.82267574,\n        2.82993197,  2.83718821,  2.84444444,  2.85170068,  2.85895692,\n        2.86621315,  2.87346939,  2.88072562,  2.88798186,  2.8952381 ,\n        2.90249433,  2.90975057,  2.9170068 ,  2.92426304,  2.93151927,\n        2.93877551,  2.94603175,  2.95328798,  2.96054422,  2.96780045,\n        2.97505669,  2.98231293,  2.98956916,  2.9968254 ,  3.00408163,\n        3.01133787,  3.0185941 ,  3.02585034,  3.03310658,  3.04036281,\n        3.04761905,  3.05487528,  3.06213152,  3.06938776,  3.07664399,\n        3.08390023,  3.09115646,  3.0984127 ,  3.10566893,  3.11292517,\n        3.12018141,  3.12743764,  3.13469388,  3.14195011,  3.14920635,\n        3.15646259,  3.16371882,  3.17097506,  3.17823129,  3.18548753,\n        3.19274376,  3.2       ,  3.20725624,  3.21451247,  3.22176871,\n        3.22902494,  3.23628118,  3.24353741,  3.25079365,  3.25804989,\n        3.26530612,  3.27256236,  3.27981859,  3.28707483,  3.29433107,\n        3.3015873 ,  3.30884354,  3.31609977,  3.32335601,  3.33061224,\n        3.33786848,  3.34512472,  3.35238095,  3.35963719,  3.36689342,\n        3.37414966,  3.3814059 ,  3.38866213,  3.39591837,  3.4031746 ,\n        3.41043084,  3.41768707,  3.42494331,  3.43219955,  3.43945578,\n        3.44671202,  3.45396825,  3.46122449,  3.46848073,  3.47573696,\n        3.4829932 ,  3.49024943,  3.49750567,  3.5047619 ,  3.51201814,\n        3.51927438,  3.52653061,  3.53378685,  3.54104308,  3.54829932,\n        3.55555556,  3.56281179,  3.57006803,  3.57732426,  3.5845805 ,\n        3.59183673,  3.59909297,  3.60634921,  3.61360544,  3.62086168,\n        3.62811791,  3.63537415,  3.64263039,  3.64988662,  3.65714286,\n        3.66439909,  3.67165533,  3.67891156,  3.6861678 ,  3.69342404,\n        3.70068027,  3.70793651,  3.71519274,  3.72244898,  3.72970522,\n        3.73696145,  3.74421769,  3.75147392,  3.75873016,  3.76598639,\n        3.77324263,  3.78049887,  3.7877551 ,  3.79501134,  3.80226757,\n        3.80952381,  3.81678005,  3.82403628,  3.83129252,  3.83854875,\n        3.84580499,  3.85306122,  3.86031746,  3.8675737 ,  3.87482993,\n        3.88208617,  3.8893424 ,  3.89659864,  3.90385488,  3.91111111,\n        3.91836735,  3.92562358,  3.93287982,  3.94013605,  3.94739229,\n        3.95464853,  3.96190476,  3.969161  ,  3.97641723,  3.98367347,\n        3.99092971,  3.99818594])\n\n\n\n\nCode\n# Convert spectrogram to decibels for visualization\nspec_db = amplitude_to_db(spec)\n\n# Display these features on top of the spectrogram\nfig, ax = plt.subplots(figsize=(10, 5))\nax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH);\nax.plot(times_spec, centroids);\nax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5);\nax.set(ylim=[None, 6000]);\n\n\nAttributeError: 'QuadMesh' object has no attribute 'plot'"
  },
  {
    "objectID": "posts/Using XGBoost in Pipelines/Using XGBoost in pipelines.html",
    "href": "posts/Using XGBoost in Pipelines/Using XGBoost in pipelines.html",
    "title": "Using XGBoost in pipelines",
    "section": "",
    "text": "Get more out of your XGBoost skills with two end-to-end machine learning pipelines using your models. You’ll learn how to tune the most important XGBoost hyperparameters efficiently within a pipeline, as well as how to use advanced preprocessing techniques.\nThis Using XGBoost in pipelines is part of Datacamp course: Extreme Gradient Boosting with XGBoost\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\n\n\n\n\nPipeline review\n\nTakes a list of 2-tuples (name, pipeline_step) as input\nTuples can contain any arbitrary scikit-learn compatible estimator or transformer object\nPipeline implements fit/predict methods\nCan be used as input estimator into grid/randomized search and cross_val_score methods\n\n\n\n\nNow that you’ve seen what will need to be done to get the housing data ready for XGBoost, let’s go through the process step-by-step.\nFirst, you will need to fill in missing values - as you saw previously, the column LotFrontage has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically.\nThe data has five categorical columns: MSZoning, PavedDrive, Neighborhood, BldgType, and HouseStyle. Scikit-learn has a LabelEncoder function that converts the values in each categorical column into integers. You’ll practice using this here.\n\n\nCode\ndf = pd.read_csv('dataset/ames_unprocessed_data.csv')\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      MSSubClass\n      MSZoning\n      LotFrontage\n      LotArea\n      Neighborhood\n      BldgType\n      HouseStyle\n      OverallQual\n      OverallCond\n      YearBuilt\n      ...\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      Fireplaces\n      GarageArea\n      PavedDrive\n      SalePrice\n    \n  \n  \n    \n      0\n      60\n      RL\n      65.0\n      8450\n      CollgCr\n      1Fam\n      2Story\n      7\n      5\n      2003\n      ...\n      1710\n      1\n      0\n      2\n      1\n      3\n      0\n      548\n      Y\n      208500\n    \n    \n      1\n      20\n      RL\n      80.0\n      9600\n      Veenker\n      1Fam\n      1Story\n      6\n      8\n      1976\n      ...\n      1262\n      0\n      1\n      2\n      0\n      3\n      1\n      460\n      Y\n      181500\n    \n    \n      2\n      60\n      RL\n      68.0\n      11250\n      CollgCr\n      1Fam\n      2Story\n      7\n      5\n      2001\n      ...\n      1786\n      1\n      0\n      2\n      1\n      3\n      1\n      608\n      Y\n      223500\n    \n    \n      3\n      70\n      RL\n      60.0\n      9550\n      Crawfor\n      1Fam\n      2Story\n      7\n      5\n      1915\n      ...\n      1717\n      1\n      0\n      1\n      0\n      3\n      1\n      642\n      Y\n      140000\n    \n    \n      4\n      60\n      RL\n      84.0\n      14260\n      NoRidge\n      1Fam\n      2Story\n      8\n      5\n      2000\n      ...\n      2198\n      1\n      0\n      2\n      1\n      4\n      1\n      836\n      Y\n      250000\n    \n  \n\n5 rows × 21 columns\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\n\n# Fill missing values with 0\ndf.LotFrontage = df.LotFrontage.fillna(0)\n\n# Create a boolean mask for categorical columns\ncategorical_mask = (df.dtypes == 'object')\n\n# Get list of categorical columns names\ncategorical_columns = df.columns[categorical_mask].tolist()\n\n# Print the head of the categorical columns\nprint(df[categorical_columns].head())\n\n# Create LabelEncoder object: le\nle = LabelEncoder()\n\n# Apply LabelEncode to categorical columns\ndf[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n\n# Print the head of the LabelEncoded categorical columns\nprint(df[categorical_columns].head())\nprint(\"\\nNotice how the entries in each categorical column are now encoded numerically. A BldgTpe of 1Fam is encoded as 0, while a HouseStyle of 2Story is encoded as 5\")\n\n\n  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n0       RL      CollgCr     1Fam     2Story          Y\n1       RL      Veenker     1Fam     1Story          Y\n2       RL      CollgCr     1Fam     2Story          Y\n3       RL      Crawfor     1Fam     2Story          Y\n4       RL      NoRidge     1Fam     2Story          Y\n   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n0         3             5         0           5           2\n1         3            24         0           2           2\n2         3             5         0           5           2\n3         3             6         0           5           2\n4         3            15         0           5           2\n\nNotice how the entries in each categorical column are now encoded numerically. A BldgTpe of 1Fam is encoded as 0, while a HouseStyle of 2Story is encoded as 5\n\n\n\n\n\nOkay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6. Is Veenker “greater” than Crawfor and CollgCr? No - and allowing the model to assume this natural ordering may result in poor performance.\nAs a result, there is another step needed: You have to apply a one-hot encoding to create binary, or “dummy” variables. You can do this using scikit-learn’s OneHotEncoder.\n\nWarning: Instead using LabelEncoder, use make_column_transformer\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\n\ndf = pd.read_csv('dataset/ames_unprocessed_data.csv')\n\n# Fill missing values with 0\ndf.LotFrontage = df.LotFrontage.fillna(0)\n\n# Create a boolean mask for categorical columns\ncategorical_mask = (df.dtypes == 'object')\n\n# Get list of categorical columns names\ncategorical_columns = df.columns[categorical_mask].tolist()\n\n# Generate unique list of each categorical columns\nunique_list = [df[c].unique().tolist() for c in categorical_columns]\n\n# Create OneHotEncoder: ohe\nohe = OneHotEncoder(categories=unique_list)\n\n# Create preprocess object for onehotencoding\npreprocess = make_column_transformer(\n    (ohe, categorical_columns),\n    ('passthrough', categorical_mask[~categorical_mask].index.tolist())\n)\n\n# apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\ndf_encoded = preprocess.fit_transform(df)\n\n# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\nprint(df_encoded[:5, :])\n\n# Print the shape fo the original DataFrame\nprint(df.shape)\n\n# Print the shape of the transformed array\nprint(df_encoded.shape)\n\n\n[[1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 1.000e+00 0.000e+00 0.000e+00 6.000e+01 6.500e+01 8.450e+03\n  7.000e+00 5.000e+00 2.003e+03 0.000e+00 1.710e+03 1.000e+00 0.000e+00\n  2.000e+00 1.000e+00 3.000e+00 0.000e+00 5.480e+02 2.085e+05]\n [1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 1.000e+00 0.000e+00 0.000e+00 2.000e+01 8.000e+01 9.600e+03\n  6.000e+00 8.000e+00 1.976e+03 0.000e+00 1.262e+03 0.000e+00 1.000e+00\n  2.000e+00 0.000e+00 3.000e+00 1.000e+00 4.600e+02 1.815e+05]\n [1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 1.000e+00 0.000e+00 0.000e+00 6.000e+01 6.800e+01 1.125e+04\n  7.000e+00 5.000e+00 2.001e+03 1.000e+00 1.786e+03 1.000e+00 0.000e+00\n  2.000e+00 1.000e+00 3.000e+00 1.000e+00 6.080e+02 2.235e+05]\n [1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 1.000e+00 0.000e+00 0.000e+00 7.000e+01 6.000e+01 9.550e+03\n  7.000e+00 5.000e+00 1.915e+03 1.000e+00 1.717e+03 1.000e+00 0.000e+00\n  1.000e+00 0.000e+00 3.000e+00 1.000e+00 6.420e+02 1.400e+05]\n [1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 1.000e+00 0.000e+00 0.000e+00 6.000e+01 8.400e+01 1.426e+04\n  8.000e+00 5.000e+00 2.000e+03 0.000e+00 2.198e+03 1.000e+00 0.000e+00\n  2.000e+00 1.000e+00 4.000e+00 1.000e+00 8.360e+02 2.500e+05]]\n(1460, 21)\n(1460, 62)\n\n\n\n\n\nAlright, one final trick before you dive into pipelines. The two step process you just went through - LabelEncoder followed by OneHotEncoder - can be simplified by using a DictVectorizer.\nUsing a DictVectorizer on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go.\nYour task is to work through this strategy in this exercise!\n\n\nCode\nfrom sklearn.feature_extraction import DictVectorizer\n\n# Convert df into a dictionary: df_dict\ndf_dict = df.to_dict(\"records\")\n\n# Create the DictVectorizer object: dv\ndv = DictVectorizer(sparse=False)\n\n# Apply dv on df: df_encoded\ndf_encoded2 = dv.fit_transform(df_dict)\n\n# Print the resulting first five rows\nprint(df_encoded2[:5, :])\n\n# Print the vocabulary\nprint(dv.vocabulary_)\nprint(\"\\nBesides simplifying the process into one step, DictVectorizer has useful attributes such as vocabulary_ which maps the names of the features to their indices.  \")\n\n\n[[3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 2.000e+00 5.480e+02 1.710e+03 1.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n  8.450e+03 6.500e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.085e+05 2.003e+03]\n [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  1.000e+00 1.000e+00 2.000e+00 4.600e+02 1.262e+03 0.000e+00 0.000e+00\n  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  9.600e+03 8.000e+01 2.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 8.000e+00 6.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.815e+05 1.976e+03]\n [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 1.000e+00 2.000e+00 6.080e+02 1.786e+03 1.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n  1.125e+04 6.800e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n  0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.235e+05 2.001e+03]\n [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 1.000e+00 1.000e+00 6.420e+02 1.717e+03 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n  9.550e+03 6.000e+01 7.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n  0.000e+00 0.000e+00 1.000e+00 1.000e+00 1.400e+05 1.915e+03]\n [4.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 1.000e+00 2.000e+00 8.360e+02 2.198e+03 1.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n  1.426e+04 8.400e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 8.000e+00\n  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n{'MSSubClass': 23, 'MSZoning=RL': 27, 'LotFrontage': 22, 'LotArea': 21, 'Neighborhood=CollgCr': 34, 'BldgType=1Fam': 1, 'HouseStyle=2Story': 18, 'OverallQual': 55, 'OverallCond': 54, 'YearBuilt': 61, 'Remodeled': 59, 'GrLivArea': 11, 'BsmtFullBath': 6, 'BsmtHalfBath': 7, 'FullBath': 9, 'HalfBath': 12, 'BedroomAbvGr': 0, 'Fireplaces': 8, 'GarageArea': 10, 'PavedDrive=Y': 58, 'SalePrice': 60, 'Neighborhood=Veenker': 53, 'HouseStyle=1Story': 15, 'Neighborhood=Crawfor': 35, 'Neighborhood=NoRidge': 44, 'Neighborhood=Mitchel': 40, 'HouseStyle=1.5Fin': 13, 'Neighborhood=Somerst': 50, 'Neighborhood=NWAmes': 43, 'MSZoning=RM': 28, 'Neighborhood=OldTown': 46, 'Neighborhood=BrkSide': 32, 'BldgType=2fmCon': 2, 'HouseStyle=1.5Unf': 14, 'Neighborhood=Sawyer': 48, 'Neighborhood=NridgHt': 45, 'Neighborhood=NAmes': 41, 'BldgType=Duplex': 3, 'Neighborhood=SawyerW': 49, 'Neighborhood=IDOTRR': 38, 'PavedDrive=N': 56, 'Neighborhood=MeadowV': 39, 'BldgType=TwnhsE': 5, 'MSZoning=C (all)': 24, 'Neighborhood=Edwards': 36, 'Neighborhood=Timber': 52, 'PavedDrive=P': 57, 'HouseStyle=SFoyer': 19, 'MSZoning=FV': 25, 'Neighborhood=Gilbert': 37, 'HouseStyle=SLvl': 20, 'BldgType=Twnhs': 4, 'Neighborhood=StoneBr': 51, 'HouseStyle=2.5Unf': 17, 'Neighborhood=ClearCr': 33, 'Neighborhood=NPkVill': 42, 'HouseStyle=2.5Fin': 16, 'Neighborhood=Blmngtn': 29, 'Neighborhood=BrDale': 31, 'Neighborhood=SWISU': 47, 'MSZoning=RH': 26, 'Neighborhood=Blueste': 30}\n\nBesides simplifying the process into one step, DictVectorizer has useful attributes such as vocabulary_ which maps the names of the features to their indices.  \n\n\n\n\n\nNow that you’ve seen what steps need to be taken individually to properly process the Ames housing data, let’s use the much cleaner and more succinct DictVectorizer approach and put it alongside an XGBoostRegressor inside of a scikit-learn pipeline.\n\n\nCode\ndf = pd.read_csv('dataset/ames_unprocessed_data.csv')\nX, y = df.iloc[:, :-1], df.iloc[:, -1]\n\n\n\n\nCode\nfrom sklearn.pipeline import Pipeline\n\n# Fill LotFrontage missing values with 0\nX.LotFrontage = X.LotFrontage.fillna(0)\n\n# Setup the pipeline steps: steps\nsteps = [('ohe_onestep', DictVectorizer(sparse=False)),\n         ('xgb_model', xgb.XGBRegressor())]\n\n# Create the pipeline: xgb_pipeline\nxgb_pipeline = Pipeline(steps)\n\n# Fit the pipeline\nxgb_pipeline.fit(X.to_dict(\"records\"), y)\n\n\nPipeline(steps=[('ohe_onestep', DictVectorizer(sparse=False)),\n                ('xgb_model',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, n_estimators=100,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('ohe_onestep', DictVectorizer(sparse=False)),\n                ('xgb_model',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, n_estimators=100,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])DictVectorizerDictVectorizer(sparse=False)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\n\n\n\n\n\n\nAdditional components introduced for pipelines\n\nsklearn_pandas:\n\nDataFrameMapper - Interoperability between pandas and scikit-learn\nCategoricalImputer - Allow for imputation of categorical variables before conversion to integers\n\nsklearn.preprocessing:\n\nImputer - Native imputation of numerical columns in scikit-learn\n\nsklearn.pipeline:\n\nFeatureUnion - combine multiple pipelines of features into a single pipeline of features\n\n\n\n\n\nIn this exercise, you’ll go one step further by using the pipeline you’ve created to preprocess and cross-validate your model.\n\n\nCode\ndf = pd.read_csv('dataset/ames_unprocessed_data.csv')\nX, y = df.iloc[:, :-1], df.iloc[:, -1]\n\n\n\n\nCode\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\n\n# Fill LotFrontage missing values with 0\nX.LotFrontage = X.LotFrontage.fillna(0)\n\n# Setup the pipeline steps: steps\nsteps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective='reg:squarederror'))]\n\n# Create the pipeline: xgb_pipeline\nxgb_pipeline = Pipeline(steps)\n\n# Cross-validate the model\ncross_val_scores = cross_val_score(xgb_pipeline, X.to_dict('records'), y,\n                                   scoring='neg_mean_squared_error', cv=10)\n\n# Print the 10-fold RMSE\nprint(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))\n\n\n10-fold RMSE:  27683.04157118635\n\n\n\n\n\nYou’ll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The chronic kidney disease dataset contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.\nAs Sergey mentioned in the video, you’ll be introduced to a new library, sklearn_pandas, that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you’ll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\nWe’ve also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict(\"records\") without you having to do it explicitly (and so that it works in a pipeline). Finally, we’ve also provided the list of feature names in kidney_feature_names, the target name in kidney_target_name, the features in X, and the target in y.\nIn this exercise, your task is to apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments input_df=True and df_out=True? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar.\n\n\nCode\nX = pd.read_csv('dataset/chronic_kidney_X.csv')\ny = pd.read_csv('dataset/chronic_kidney_y.csv').to_numpy().ravel()\n\n\n\n\nCode\n#!pip install sklearn_pandas\n\n\nRequirement already satisfied: sklearn_pandas in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.0)\nRequirement already satisfied: scikit-learn>=0.23.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sklearn_pandas) (1.1.2)\nRequirement already satisfied: numpy>=1.18.1 in c:\\users\\dghr201\\appdata\\roaming\\python\\python39\\site-packages (from sklearn_pandas) (1.23.2)\nRequirement already satisfied: pandas>=1.1.4 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sklearn_pandas) (1.4.2)\nRequirement already satisfied: scipy>=1.5.1 in c:\\users\\dghr201\\appdata\\roaming\\python\\python39\\site-packages (from sklearn_pandas) (1.9.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.1.4->sklearn_pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.1.4->sklearn_pandas) (2022.1)\nRequirement already satisfied: six>=1.5 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.1.4->sklearn_pandas) (1.16.0)\nRequirement already satisfied: joblib>=1.0.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.23.0->sklearn_pandas) (1.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.23.0->sklearn_pandas) (3.1.0)\n\n\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\dghr201\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\nWARNING: You are using pip version 21.2.3; however, version 22.3.1 is available.\nYou should consider upgrading via the 'C:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n\n\n\n\nCode\nfrom sklearn_pandas import DataFrameMapper, CategoricalImputer\nfrom sklearn.impute import SimpleImputer\n\n# Check number of nulls in each feature columns\nnulls_per_column = X.isnull().sum()\nprint(nulls_per_column)\n\n# Create a boolean mask for categorical columns\ncategorical_feature_mask = X.dtypes == object\n\n# Get list of categorical column names\ncategorical_columns = X.columns[categorical_feature_mask].tolist()\n\n# Get list of non-categorical column names\nnon_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n\n# Apply numeric imputer\nnumeric_imputation_mapper = DataFrameMapper(\n    [([numeric_feature], SimpleImputer(strategy='median'))\n     for numeric_feature in non_categorical_columns],\n    input_df=True,\n    df_out=True\n)\n\n# Apply categorical imputer\ncategorical_imputation_mapper = DataFrameMapper(\n    [(category_feature, CategoricalImputer())\n     for category_feature in categorical_columns],\n    input_df=True,\n    df_out=True\n)\n\n\nImportError: cannot import name 'CategoricalImputer' from 'sklearn_pandas' (C:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn_pandas\\__init__.py)\n\n\n\n\n\nHaving separately imputed numeric as well as categorical columns, your task is now to use scikit-learn’s FeatureUnion to concatenate their results, which are contained in two separate transformer objects - numeric_imputation_mapper, and categorical_imputation_mapper, respectively.\nJust like with pipelines, you have to pass it a list of (string, transformer) tuples, where the first half of each tuple is the name of the transformer.\n\n\nCode\nfrom sklearn.pipeline import FeatureUnion\n\n# Combine the numeric and categorical transformations\nnumeric_categorical_union = FeatureUnion([\n    (\"num_mapper\", numeric_imputation_mapper),\n    (\"cat_mapper\", categorical_imputation_mapper)\n])\n\n\n\n\n\nIt’s time to piece together all of the transforms along with an XGBClassifier to build the full pipeline!\nBesides the numeric_categorical_union that you created in the previous exercise, there are two other transforms needed: the Dictifier() transform which we created for you, and the DictVectorizer().\nAfter creating the pipeline, your task is to cross-validate it to see how well it performs.\n\n\nCode\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define Dictifier class to turn df into dictionary as part of pipeline\nclass Dictifier(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X) == pd.core.frame.DataFrame:\n            return X.to_dict(\"records\")\n        else:\n            return pd.DataFrame(X).to_dict(\"records\")\n\n\n\n\nCode\n# Create full pipeline\npipeline = Pipeline([\n    (\"featureunion\", numeric_categorical_union),\n    (\"dictifier\", Dictifier()),\n    (\"vectorizer\", DictVectorizer(sort=False)),\n    (\"clf\", xgb.XGBClassifier(max_depth=3))\n])\n\n# Perform cross-validation\ncross_val_scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=3)\n\n# Print avg. AUC\nprint(\"3-fold AUC: \", np.mean(cross_val_scores))\n\n\n\n\n\n\n\n\nAlright, it’s time to bring together everything you’ve learned so far! In this final exercise of the course, you will combine your work from the previous exercises into one end-to-end XGBoost pipeline to really cement your understanding of preprocessing and pipelines in XGBoost.\nYour work from the previous 3 exercises, where you preprocessed the data and set up your pipeline, has been pre-loaded. Your job is to perform a randomized search and identify the best hyperparameters.\n\n\nCode\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Create the parameter grid\ngbm_param_grid = {\n    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n    'clf__max_depth': np.arange(3, 10, 1),\n    'clf__n_estimators': np.arange(50, 200, 50)\n}\n\n# Perform RandomizedSearchCV\nrandomized_roc_auc = RandomizedSearchCV(estimator=pipeline, param_distributions=gbm_param_grid,\n                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n\n# Fit the estimator\nrandomized_roc_auc.fit(X, y)\n\n# Compute metrics\nprint('Score: ', randomized_roc_auc.best_score_)\nprint('Estimator: ', randomized_roc_auc.best_estimator_)\n\n\nNameError: name 'pipeline' is not defined"
  },
  {
    "objectID": "posts/Validating and inspecting time series/Validating and Inspecting Time Series Models.html",
    "href": "posts/Validating and inspecting time series/Validating and Inspecting Time Series Models.html",
    "title": "Validating and Inspecting Time Series Models",
    "section": "",
    "text": "Once you’ve got a model for predicting time series data, you need to decide if it’s a good or a bad model. This chapter coves the basics of generating predictions with models in order to validate them against “test” data.\nThis Validating and Inspecting Time Series Models is part of Datacamp course: Machine Learning for Time Series Data in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (10, 5)\n# plt.style.use('fivethirtyeight')\nplt.rcParams['image.cmap'] = 'inferno'\n\n\n\n\n\nThe past is useful\n\nTimeseries data almost always have information that is shared between timepoints\nInformation in the past can help predict what happens in the future\nOften the features best-suited to predict a timeseries are previous values of the same timeseries\n\nA note on smoothness and auto-correlation\n\nA common question to ask of a timeseries: how smooth is the data.\nAKA, how correlated is a timepoint with its neighboring timepoints (called autocorrelation)\nThe amount of auto-correlation in data will impact your models.\n\n\n\n\nIn machine learning for time series, it’s common to use information about previous time points to predict a subsequent time point.\nIn this exercise, you’ll “shift” your raw data and visualize the results. You’ll use the percent change time series that you calculated in the previous chapter, this time with a very short window. A short window is important because, in a real-world scenario, you want to predict the day-to-day fluctuations of a time series, not its change over a longer window of time.\n\n\nCode\nprices = pd.read_csv('dataset/prices_nyse.csv', index_col=0, parse_dates=True)\nprices = prices[['AAPL']]\n\n# Your custom function\ndef percent_change(series):\n    # Collect all *but* the last value of this window, then the final value\n    previous_values = series[:-1]\n    last_value = series[-1]\n\n    # Calculate the % difference between the last value and the mean of earlier values\n    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n    return percent_change\n\ndef replace_outliers(series):\n    # Calculate the absolute difference of each timepoint from the series mean\n    absolute_differences_from_mean = np.abs(series - np.mean(series))\n\n    # Calculate a mask for the difference that are > 3 standard deviations from zero\n    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n\n    # Replace these values with the median across the data\n    series[this_mask] = np.nanmedian(series)\n    return series\n\n# Apply your custom function and plot\nprices_perc = prices.rolling(20).apply(percent_change)\n\n# Apply your preprocessing functino to the timeseries and plot the results\nprices_perc = prices_perc.apply(replace_outliers)\n\n\n\n\nCode\n# These are the \"time lags\"\nshifts = np.arange(1, 11).astype(int)\n\n# Use a dictionary comprehension to create name: value pairs, one pair per shift\nshifted_data = {\"lag_{}_day\".format(day_shift):\n                prices_perc['AAPL'].shift(day_shift) for day_shift in shifts}\n\n# Convert into a DataFrame for subsequent use\nprices_perc_shifted = pd.DataFrame(shifted_data)\n\n# Plot the first 100 samples of each\nfig, ax = plt.subplots(figsize=(20, 10));\nprices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis, ax=ax);\nprices_perc.iloc[:100].plot(color='r', lw=2, ax=ax);\nax.legend(loc='best');\n\n\n\n\n\n\n\n\nNow that you’ve created time-shifted versions of a single time series, you can fit an auto-regressive model. This is a regression model where the input features are time-shifted versions of the output time series data. You are using previous values of a timeseries to predict current values of the same timeseries (thus, it is auto-regressive).\nBy investigating the coefficients of this model, you can explore any repetitive patterns that exist in a timeseries, and get an idea for how far in the past a data point is predictive of the future.\n\n\nCode\nfrom sklearn.linear_model import Ridge\n\n# Replace missing values with the median for each column\nX = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\ny = prices_perc['AAPL'].fillna(np.nanmedian(prices_perc['AAPL']))\n\n# Fit the model\nmodel = Ridge()\nmodel.fit(X, y)\n\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\n\n\nNow that you’ve fit the model, let’s visualize its coefficients. This is an important part of machine learning because it gives you an idea for how the different features of a model affect the outcome.\nIn this exercise, you will create a function that, given a set of coefficients and feature names, visualizes the coefficient values.\n\n\nCode\ndef visualize_coefficients(coefs, names, ax):\n    # Make a bar plot for the coefficients, including their names on the x-axis\n    ax.bar(names, coefs);\n    ax.set(xlabel='Coefficient name', ylabel='Coefficient value');\n\n    # set formatting so it looks nice\n    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right');\n    plt.tight_layout();\n    return ax\n\n\n\n\nCode\n# Visualize the output data up to \"2011-01\"\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\ny.loc[:'2011-01'].plot(ax=axs[0], ylim=(-0.1, 0.2));\n\n# Run the function to visualize model's coefficients\nvisualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1]);\n\n\n\n\n\n\n\n\nNow, let’s re-run the same procedure using a smoother signal. You’ll use the same percent change algorithm as before, but this time use a much larger window (40 instead of 20). As the window grows, the difference between neighboring timepoints gets smaller, resulting in a smoother signal. What do you think this will do to the auto-regressive model?\n\n\nCode\n# Apply your custom function and plot\nprices_perc = prices.rolling(40).apply(percent_change)\n\n# Apply your preprocessing functino to the timeseries and plot the results\nprices_perc = prices_perc.apply(replace_outliers)\n\n# Use a dictionary comprehension to create name: value pairs, one pair per shift\nshifted_data = {\"lag_{}_day\".format(day_shift):\n                prices_perc['AAPL'].shift(day_shift) for day_shift in shifts}\n\n# Convert into a DataFrame for subsequent use\nprices_perc_shifted = pd.DataFrame(shifted_data)\n\n# Replace missing values with the median for each column\nX = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\ny = prices_perc['AAPL'].fillna(np.nanmedian(prices_perc['AAPL']))\n\n# Fit the model\nmodel = Ridge()\nmodel.fit(X, y)\n\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\n\n\n\n\n\nAs you’ll recall, cross-validation is the process of splitting your data into training and test sets multiple times. Each time you do this, you choose a different training and test set. In this exercise, you’ll perform a traditional ShuffleSplit cross-validation on the company value data from earlier. Later we’ll cover what changes need to be made for time series data. The data we’ll use is the same historical price data for several large companies.\n\n\nCode\nX = pd.read_csv('dataset/x.csv', index_col=0).to_numpy()\ny = pd.read_csv('dataset/y.csv', index_col=0).to_numpy()\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nmodel = LinearRegression()\n\n\n\n\nCode\ndef visualize_predictions(results):\n    fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n\n    # Loop through our model results to visualize them\n    for ii, (prediction, score, indices) in enumerate(results):\n        # Plot the predictions of the model in the order they were generated\n        offset = len(prediction) * ii\n        axs[0].scatter(np.arange(len(prediction)) + offset, prediction,\n                       label='Iteration {}'.format(ii))\n\n        # Plot the predictions of the model according to how time was ordered\n        axs[1].scatter(indices, prediction)\n    axs[0].legend(loc=\"best\")\n    axs[0].set(xlabel=\"Test prediction number\", title=\"Predictions ordered by test prediction number\")\n    axs[1].set(xlabel=\"Time\", title=\"Predictions ordered by time\")\n\n\n\n\nCode\nfrom sklearn.model_selection import ShuffleSplit\ncv = ShuffleSplit(n_splits=10, random_state=1)\n\n# Iterate through CV splits\nresults = []\nfor tr, tt in cv.split(X, y):\n    # Fit the model on training data\n    model.fit(X[tr], y[tr])\n\n    # Generate predictions on the test data, score the predictions, and collect\n    prediction = model.predict(X[tt])\n    score = r2_score(y[tt], prediction)\n    results.append((prediction, score, tt))\n\n# Custom function to quickly visualize predictions\nvisualize_predictions(results)\n\n\n\n\n\nIf you look at the plot to the right, see that the order of datapoints in the test set is scrambled. Let’s see how it looks when we shuffle the data in blocks.\n\n\n\nNow, re-run your model fit using block cross-validation (without shuffling all datapoints). In this case, neighboring time-points will be kept close to one another. How do you think the model predictions will look in each cross-validation loop?\n\n\nCode\n# Create KFold cross-validation object\nfrom sklearn.model_selection import KFold\ncv = KFold(n_splits=10, shuffle=False)\n\n# Iterate through CV splits\nresults = []\nfor tr, tt in cv.split(X, y):\n    # Fit the model on training data\n    model.fit(X[tr], y[tr])\n\n    # Generate predictions on the test data and collect\n    prediction = model.predict(X[tt])\n    results.append((prediction, _, tt))\n\n# Custom function to quickly visualize predictions\nvisualize_predictions(results)\n\n\n\n\n\nThis time, the predictions generated within each CV loop look ‘smoother’ than they were before - they look more like a real time series because you didn’t shuffle the data. This is a good sanity check to make sure your CV splits are correct\n\n\n\nFinally, let’s visualize the behavior of the time series cross-validation iterator in scikit-learn. Use this object to iterate through your data one last time, visualizing the training data used to fit the model on each iteration.\n\n\nCode\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# Create time-series cross-validation object\ncv = TimeSeriesSplit(n_splits=10)\n\n# Iterate through CV splits\nfig, ax = plt.subplots()\nfor ii, (tr, tt) in enumerate(cv.split(X, y)):\n    # Plot the training data on each iteration, to see the behavior of the CV\n    ax.plot(tr, ii + y[tr]);\n\nax.set(title='Training data on each CV iteration', ylabel='CV iteration');\n\n\n\n\n\nNote that the size of the training set grew each time when you used the time series cross-validation object. This way, the time points you predict are always after the timepoints we train on.\n\n\n\n\n\nStationarity\n\nStationarity time series do not change their statistical properties over time\n\nE.g. mean, standard deviation, trends\n\nMost time series are non-stationary to some extent\n\nModel stability\n\nNon-stationary data results in variability in our model\nThe statistical properties the model finds may change with the data\nIn addition, we will be less certain about the correct values of model parameters\nHow can we quantify this?\n\nCross validation to quantify parameter stability\n\nOne approach: use cross-validation\nCalculate model parameters on each iteration\nAssess parameter stability across all CV splits\n\nBootstrapping the mean\n\nBootstrapping is a common way to assess variability\nThe bootstrap:\n\nTake a random sample of data with replacement\nCalculate the mean of the sample\nRepeat this process many times (1000s)\nCalculate the percentiles of the result (usually 2.5, 97.5)\n\nThe result is a 95% confidence interval of the mean of each coefficient.\n\nAssessing model performance stability\n\nIf using the TimeSeriesSplit, can plot the model’s score over time\nThis is useful in finding certain regions of time that hurt the score\nAlso useful to find non-stationary signals\n\n\n\n\nA useful tool for assessing the variability of some data is the bootstrap. In this exercise, you’ll write your own bootstrapping function that can be used to return a bootstrapped confidence interval.\nThis function takes three parameters: a 2-D array of numbers (data), a list of percentiles to calculate (percentiles), and the number of boostrap iterations to use (n_boots). It uses the resample function to generate a bootstrap sample, and then repeats this many times to calculate the confidence interval.\n\n\nCode\nfrom sklearn.utils import resample\n\ndef bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n    \"\"\"Bootstrap a confidence interval for the mean of columns of a 1- or 2-D dataset.\"\"\"\n    # Create our empty array we'll fill with the results\n    if data.ndim == 1:\n        data = data[:, np.newaxis]\n        data = np.atleast_2d(data)\n    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n    for ii in range(n_boots):\n        # Generate random indices for our data *with* replacement, then take the sample mean\n        random_sample = resample(data)\n        bootstrap_means[ii] = random_sample.mean(axis=0)\n\n    # Compute the percentiles of choice for the bootstrapped means\n    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n    return percentiles\n\n\n\n\n\nIn this lesson, you’ll re-run the cross-validation routine used before, but this time paying attention to the model’s stability over time. You’ll investigate the coefficients of the model, as well as the uncertainty in its predictions.\nBegin by assessing the stability (or uncertainty) of a model’s coefficients across multiple CV splits. Remember, the coefficients are a reflection of the pattern that your model has found in the data.\n\n\nCode\nX = pd.read_csv('dataset/stock_12x.csv', index_col=0).to_numpy()\ny = pd.read_csv('dataset/stock_12y.csv', index_col=0).to_numpy()\n\nfeature_names = np.array(['AAPL_lag_1_day', 'YHOO_lag_1_day', 'NVDA_lag_1_day', 'AAPL_lag_2_day',\n       'YHOO_lag_2_day', 'NVDA_lag_2_day', 'AAPL_lag_3_day', 'YHOO_lag_3_day',\n       'NVDA_lag_3_day', 'AAPL_lag_4_day', 'YHOO_lag_4_day', 'NVDA_lag_4_day'])\n\ntimes_scores = pd.DatetimeIndex(['2010-04-05', '2010-04-28', '2010-05-21', '2010-06-16',\n               '2010-07-12', '2010-08-04', '2010-08-27', '2010-09-22',\n               '2010-10-15', '2010-11-09', '2010-12-03', '2010-12-29',\n               '2011-01-24', '2011-02-16', '2011-03-14', '2011-04-06',\n               '2011-05-02', '2011-05-25', '2011-06-20', '2011-07-14',\n               '2011-08-08', '2011-08-31', '2011-09-26', '2011-10-19',\n               '2011-11-11', '2011-12-07', '2012-01-03', '2012-01-27',\n               '2012-02-22', '2012-03-16', '2012-04-11', '2012-05-04',\n               '2012-05-30', '2012-06-22', '2012-07-18', '2012-08-10',\n               '2012-09-05', '2012-09-28', '2012-10-23', '2012-11-19',\n               '2012-12-13', '2013-01-09', '2013-02-04', '2013-02-28',\n               '2013-03-25', '2013-04-18', '2013-05-13', '2013-06-06',\n               '2013-07-01', '2013-07-25', '2013-08-19', '2013-09-12',\n               '2013-10-07', '2013-10-30', '2013-11-22', '2013-12-18',\n               '2014-01-14', '2014-02-07', '2014-03-05', '2014-03-28',\n               '2014-04-23', '2014-05-16', '2014-06-11', '2014-07-07',\n               '2014-07-30', '2014-08-22', '2014-09-17', '2014-10-10',\n               '2014-11-04', '2014-11-28', '2014-12-23', '2015-01-20',\n               '2015-02-12', '2015-03-10', '2015-04-02', '2015-04-28',\n               '2015-05-21', '2015-06-16', '2015-07-10', '2015-08-04',\n               '2015-08-27', '2015-09-22', '2015-10-15', '2015-11-09',\n               '2015-12-03', '2015-12-29', '2016-01-25', '2016-02-18',\n               '2016-03-14', '2016-04-07', '2016-05-02', '2016-05-25',\n               '2016-06-20', '2016-07-14', '2016-08-08', '2016-08-31',\n               '2016-09-26', '2016-10-19', '2016-11-11', '2016-12-07'], name='date')\n\nmodel = LinearRegression()\n\n\n\n\nCode\n# Iterate through CV splits\nn_splits = 100\ncv = TimeSeriesSplit(n_splits)\n\n# Create empty array to collect coefficients\ncoefficients = np.zeros([n_splits, X.shape[1]])\n\nfor ii, (tr, tt) in enumerate(cv.split(X, y)):\n    # Fit the model on training data and collect the coefficients\n    model.fit(X[tr], y[tr])\n    coefficients[ii] = model.coef_\n\n\n\n\nCode\n# Calculate a confidence interval around each coefficient\nbootstrapped_interval = bootstrap_interval(coefficients)\n\n# Plot it\nfig, ax = plt.subplots()\nax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3);\nax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3);\nax.set(title='95% confidence interval for model coefficients');\nplt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right');\n\n\n\n\n\n\n\n\nNow that you’ve assessed the variability of each coefficient, let’s do the same for the performance (scores) of the model. Recall that the TimeSeriesSplit object will use successively-later indices for each test set. This means that you can treat the scores of your validation as a time series. You can visualize this over time in order to see how the model’s performance changes over time.\n\n\nCode\ndef my_pearsonr(est, X, y):\n    # Generate predictions and convert to a vector\n    y_pred = est.predict(X).squeeze()\n\n    # Use the numpy \"corrcoef\" function to calculate a correlation matrix\n    my_corrcoef_matrix = np.corrcoef(y_pred, y.squeeze())\n\n    # Return a single correlation value from the matrix\n    my_corrcoef = my_corrcoef_matrix[1, 0]\n    return my_corrcoef\n\n\n\n\nCode\nfrom sklearn.model_selection import cross_val_score\nfrom functools import partial\n\n# Generate scores for each split to see how the model performs over time\nscores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n\n# Convert to a Pandas Series object\nscores_series = pd.Series(scores, index=times_scores, name='score')\n\n# Bootstrap a rolling confidence interval for the mean score\nscores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5))\nscores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5))\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_39244\\1687367918.py:7: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n  data = data[:, np.newaxis]\n\n\n\n\nCode\n# Plot the results\nfig, ax = plt.subplots()\nscores_lo.plot(ax=ax, label='Lower confidence interval');\nscores_series.plot(ax=ax, label='scores')\nscores_series.rolling(20).mean().plot(ax=ax, label='rolling mean')\nscores_hi.plot(ax=ax, label='Upper confidence interval');\nax.legend();\n\n\n\n\n\n\n\n\nIn this exercise, you will again visualize the variations in model scores, but now for data that changes its statistics over time.\n\n\nCode\n# Pre-initialize window sizes\nwindow_sizes = [25, 50, 75, 100]\n\n# Create an empty DataFrame to collect the stores\nall_scores = pd.DataFrame(index=times_scores)\n\n# Generate scores for each split to see how the model performs over time\nfor window in window_sizes:\n    # Create cross-validation object using a limited lookback window\n    cv = TimeSeriesSplit(n_splits=100, max_train_size=window)\n\n    # Calculate scores across all CV splits and collect them in a DataFrame\n    this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n    all_scores['Length {}'.format(window)] = this_scores\n\n\n\n\nCode\n# Visualize the scores\nax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm);\nax.set(title='Scores for multiple windows', ylabel='Correlation (r)');\n\n\n\n\n\n\n\n\n\n\nTimeseries and machine learning\n\nMany applications of time series + machine learning\nAlways visualize your data first\n\nFeature extraction and classification\n\nSummary statistics for time series classification\nCombining multiple features into a single input matrix\nFeature extraction for time series data\n\nModel fitting and improving data quality\n\nTime series features for regression\nGenerating predictions over time\nCleaning and improving time series data\n\nValidating and assessing our model performance\n\nCross-validation with time series data (don’t shuffle the data!)\nTime series stationary\nAssessing model coefficient and score stability\n\nAdvanced concepts in time series\n\nAdvanced window functions\nSignal processing and filtering details\nSpectral Analysis\n\nAdvanced machine learning\n\nAdvanced time series feature extraction (e.g., tsfresh)\nMore complex model architectures for regression and classification\nProduction-ready pipelines for time series analysis"
  }
]