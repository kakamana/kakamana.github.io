[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Muhammad Asad Kamran is passionate\nMuhammad Asad Kamran has more than 15 years of experience in Software Engineering profession. Having strong hands on for architect, development, consultation & team building/managing experience of enterprise application & their integration with business critical applications, which includes SharePoint/Project Server, Asp.Net MVC & Biztalk complex applications.\nBeing a certified PMP & Prince 2 practitioner, Asad has been managing & mentoring mission critical team which delivered successful & award wining projects worth millions for clients in Middle East, Europe including Telco Operators, Oil & Gas clients, ministries, Insurance giant & multinational Attorney giants. Having numerous success stories of working with key stakeholders to develop architectural framework that aligns strategy, processes, and IT assets with business goals.\nExcellent communication, presentation, and organizational skills. Involved in successful Digital Transformation & Integration projects which provides G2G, G2B, B2B, B2C & G2C e-commerce & Digital services. His core competencies are Collaboration, Messaging, Enterprise solution architecture, Digitization transformation, project management, Agile methodologies & Data analytics."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity Of Michigan | Michigan, USA Masters in applied data science (MADS) | May 2022 - April 2024\nComsats Institute Of Information Technology | Lahore, Pakistan Bachelors Of Computer Science (Software Engineering) | 2002 - 2005"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kakamana’s Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nAssessing model fit\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nSimple Logistic Regression Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nkakamana\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nPredictions and model objects in linear regression\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSimple Linear Regression Modeling\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\nmachine learning\n\n\nlinear regression\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation in a nutshell\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nkakamana\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation and Experimental Design\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nDistribution (pdf, cdf) of iris data\n\n\n\n\n\n\n\npython\n\n\nstatistics\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nkakamana\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nRandom Numbers and Probability\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\nkakamana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSummary Of Statistics\n\n\n\n\n\n\n\npython\n\n\ndatacamp\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\nkakamana\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Correlation and experimental design/Correlation and experimental design.html",
    "href": "posts/Correlation and experimental design/Correlation and experimental design.html",
    "title": "Correlation and Experimental Design",
    "section": "",
    "text": "We will explore how to quantify the strength of a linear relationship between two variables, and explore how confounding variables can affect the relationship between two other variables. we’ll also see how a study’s design can influence its results, change how the data should be analyzed, and potentially affect the reliability of your conclusions\nThis Correlation and Experimental Design is part of Datacamp course: Introduction to Statistic in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport numpy as np\n\n\n\n\n* Correlation coefficient\n    * Quantifies the linear relationship between two variables\n    * Number between -1 and 1\n    * Magnitude corresponds to strength of relationship\n    * Sign (+ or -) corresponds to direction of relationship\n\n* Pearson product-moment correlation(rr)\n\n\n\nHere we’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\nIn this exercise, you’ll examine the relationship between a country’s life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively.\n\n\nCode\nworld_happiness = pd.read_csv('world_happiness.csv', index_col=0)\nworld_happiness.head()\n\n\n\n\n\n\n  \n    \n      \n      country\n      social_support\n      freedom\n      corruption\n      generosity\n      gdp_per_cap\n      life_exp\n      happiness_score\n    \n  \n  \n    \n      1\n      Finland\n      2.0\n      5.0\n      4.0\n      47.0\n      42400\n      81.8\n      155\n    \n    \n      2\n      Denmark\n      4.0\n      6.0\n      3.0\n      22.0\n      48300\n      81.0\n      154\n    \n    \n      3\n      Norway\n      3.0\n      3.0\n      8.0\n      11.0\n      66300\n      82.6\n      153\n    \n    \n      4\n      Iceland\n      1.0\n      7.0\n      45.0\n      3.0\n      47900\n      83.0\n      152\n    \n    \n      5\n      Netherlands\n      15.0\n      19.0\n      12.0\n      7.0\n      50500\n      81.8\n      151\n    \n  \n\n\n\n\n\n\nCode\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x='life_exp', y='happiness_score',data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create scatterplot of happiness_score vs life_exp with trendline\nsns.lmplot(x='life_exp', y='happiness_score',data=world_happiness, ci=None)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Correlation between life_exp and happiness_score\ncor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n\nprint(cor)\n\n\n0.7802249053272062\n\n\n\n\n\n* Correlation only accounts for linear relationships\n* Transformation\n   *  Certain statistical methods rely on variables having a linear relationship\n        * Correlation coefficient\n        * Linear regression\n* Correlation does not imply causation\n    * x is correlated with yy does not mean xx causes y\n\n\n\nWhile the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it’s far from perfect. Here we’ll explore one of the caveats of the correlation coefficient by examining the relationship between a country’s GDP per capita (gdp_per_cap) and happiness score.\n\n\nCode\n# Scatterplot of gdp_per_cap and life_exp\nsns.scatterplot(x='gdp_per_cap',y='life_exp', data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Correlation between gdp_per_cap and life_exp\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n\nprint(cor)\n\n\n0.7019547642148015\n\n\n\n\n\nWhen variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. Here we’ll perform a transformation yourself\n\n\nCode\n# Scatterplot of happiness_score vs. gdp_per_cap\nsns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n\n\n\n0.7279733012222975\n\n\n\n\nCode\n# Create log_gdp_per_cap column\nworld_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n\n# Scatterplot of log_gdp_per_cap and happiness_score\nsns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness);\nplt.show()\n\n# Calculate correlation\ncor =  world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\nprint(\"\\n The relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP\")\n\n\n\n\n\n0.8043146004918288\n\n The relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP\n\n\n\n\n\nA new column has been added to world_happiness called grams_sugar_per_day, which contains the average amount of sugar eaten per person per day in each country. We’ll examine the effect of a country’s average sugar consumption on its happiness score.\n\n\nCode\nworld_happiness = pd.read_csv('world_happiness_add_sugar.csv', index_col=0)\nworld_happiness\n\n\n\n\n\n\n  \n    \n      \n      country\n      social_support\n      freedom\n      corruption\n      generosity\n      gdp_per_cap\n      life_exp\n      happiness_score\n      grams_sugar_per_day\n    \n    \n      Unnamed: 0\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      Finland\n      2\n      5\n      4.0\n      47\n      42400\n      81.8\n      155\n      86.8\n    \n    \n      2\n      Denmark\n      4\n      6\n      3.0\n      22\n      48300\n      81.0\n      154\n      152.0\n    \n    \n      3\n      Norway\n      3\n      3\n      8.0\n      11\n      66300\n      82.6\n      153\n      120.0\n    \n    \n      4\n      Iceland\n      1\n      7\n      45.0\n      3\n      47900\n      83.0\n      152\n      132.0\n    \n    \n      5\n      Netherlands\n      15\n      19\n      12.0\n      7\n      50500\n      81.8\n      151\n      122.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129\n      Yemen\n      100\n      147\n      83.0\n      155\n      2340\n      68.1\n      5\n      77.9\n    \n    \n      130\n      Rwanda\n      144\n      21\n      2.0\n      90\n      2110\n      69.1\n      4\n      14.1\n    \n    \n      131\n      Tanzania\n      131\n      78\n      34.0\n      49\n      2980\n      67.7\n      3\n      28.0\n    \n    \n      132\n      Afghanistan\n      151\n      155\n      136.0\n      137\n      1760\n      64.1\n      2\n      24.5\n    \n    \n      133\n      Central African Republic\n      155\n      133\n      122.0\n      113\n      794\n      52.9\n      1\n      22.4\n    \n  \n\n133 rows × 9 columns\n\n\n\n\n\nCode\n# Scatterplot of grams_sugar_per_day and happiness_score\nsns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor =  world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n\n\n\n0.6939100021829634\n\n\n\n\n\n\n* Vocabulary\n    * Experiment aims to answer: What is the effect of the treatment on the response?\n        * Treatment: explanatory / independent variable\n        * Response: response / dependent variable\n    * E.g.: What is the effect of an advertisement on the number of products purchased?\n        * Treatment: advertisement\n        * Response: number of products purchased\n            * Controlled experiments\n            * Participants are assigned by researchers to either treatment group or control group\n            * Treatment group sees advertisement\n            * Control group does not\n            * Group should be comparable so that causation can be inferred\n            * If groups are not comparable, this could lead to confounding (bias)\n* Gold standard of experiment\n    * Randomized controlled trial\n        * Participants are assigned to treatment/control randomly, not based on any other characteristics\n        C* hoosing randomly helps ensure that groups are comparable\n    * Placebo\n        * Resembles treatement, but has no effect\n        * Participants will not know which group they're in\n    * Double-blind trial\n        * Person administering the treatment/running the study doesn't know whether the treatment is real or a placebo\n        * Prevents bias in the response and/or analysis of results\n    * Fewopportunities for bias = more reliable conclusion about causation\n* Observational studies\n    * Participants are not assigned randomly to groups\n        * Participants assign themselves, usually based on pre-existing characteristics\n    * Many research questions are not conductive to a controlled experiment\n        * Cannot force someone to smoke or have a disease\n    * Establish association, not causation\n        * Effects can be confounded by factors that got certain people into the control or treatment group\n        * There are ways to control for confounders to get more reliable conclusions about association\n            * Longitudinal vs. cross-sectional studies\n    * Longitudinal study\n        * Participants are followed over a period of time to examine effect of treatment on response\n        * Effect of age on height is not confounded by generation\n        * More expensive, results take longer\n    * Cross-sectional study\n        * Data on participants is collected from a single snapshot in time\n        * Effect of age on height is confounded by generation\n        * Cheaper, fater, more convenient"
  },
  {
    "objectID": "posts/Correlation in a nutshell/Correlation in a nutshell.html",
    "href": "posts/Correlation in a nutshell/Correlation in a nutshell.html",
    "title": "Correlation in a nutshell",
    "section": "",
    "text": "In this article we will explore basically a linear relationship between two variables, its possible quantification (magnitude & direction). We will also touch high level of confounding & caveats of correlation. This article use exploration of study for mammals sleeping habits & world happiness\nThis is my learning experience of data science through DataCamp\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nCode\ndf = pd.read_csv('mammals.csv')\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      species\n      body_wt\n      brain_wt\n      non_dreaming\n      dreaming\n      total_sleep\n      life_span\n      gestation\n      predation\n      exposure\n      danger\n    \n  \n  \n    \n      0\n      Africanelephant\n      6654.000\n      5712.0\n      NaN\n      NaN\n      3.3\n      38.6\n      645.0\n      3\n      5\n      3\n    \n    \n      1\n      Africangiantpouchedrat\n      1.000\n      6.6\n      6.3\n      2.0\n      8.3\n      4.5\n      42.0\n      3\n      1\n      3\n    \n    \n      2\n      ArcticFox\n      3.385\n      44.5\n      NaN\n      NaN\n      12.5\n      14.0\n      60.0\n      1\n      1\n      1\n    \n    \n      3\n      Arcticgroundsquirrel\n      0.920\n      5.7\n      NaN\n      NaN\n      16.5\n      NaN\n      25.0\n      5\n      2\n      3\n    \n    \n      4\n      Asianelephant\n      2547.000\n      4603.0\n      2.1\n      1.8\n      3.9\n      69.0\n      624.0\n      3\n      5\n      4\n    \n  \n\n\n\n\n\n\nCode\ndf.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 62 entries, 0 to 61\nData columns (total 11 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   species       62 non-null     object \n 1   body_wt       62 non-null     float64\n 2   brain_wt      62 non-null     float64\n 3   non_dreaming  48 non-null     float64\n 4   dreaming      50 non-null     float64\n 5   total_sleep   58 non-null     float64\n 6   life_span     58 non-null     float64\n 7   gestation     58 non-null     float64\n 8   predation     62 non-null     int64  \n 9   exposure      62 non-null     int64  \n 10  danger        62 non-null     int64  \ndtypes: float64(7), int64(3), object(1)\nmemory usage: 5.5+ KB\n\n\n\n\n\nThe sleep time of 39 species of mammals distributed over 13 orders is analyzed in regards to their distribution over the 13 orders. There are 62 observations across 11 variables.\nspecies : Mammal species\nbody_wt : Mammal’s total body weight (kg)\nbrain_wt : Mammal’s brain weight (kg)\nnon_dreaming : Sleep hours without dreaming\ndreaming : Sleep hours spent dreaming\ntotal_sleep : Total number of hours of sleep\nlife_span : Life span (in years)\ngestation : Days during gestation / pregnancy\nThe likelihood that a mammal will be preyed upon. 1 = least likely to be preyed on. 5 = most likely to be preyed upon.\nexposure : How exposed a mammal is during sleep. 1 = least exposed (e.g., sleeps in a well-protected den). 5 = most exposed.\nA measure of how much danger the mammal faces. This index is based upon Predation and Exposure. 1 = least danger from other animals. 5 = most danger from other animals.\n\n\nCode\ndf.isnull().sum()\n\n\nspecies          0\nbody_wt          0\nbrain_wt         0\nnon_dreaming    14\ndreaming        12\ntotal_sleep      4\nlife_span        4\ngestation        4\npredation        0\nexposure         0\ndanger           0\ndtype: int64\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Body Weight Distribution')\nsns.histplot(df['body_wt'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Body Weight Distribution'}, xlabel='body_wt', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Body Wight')\nsns.barplot(x='body_wt', y='species', data=df.sort_values('body_wt',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Body Wight'}, xlabel='body_wt', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"body_wt\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c10a9430>\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Brain Weight Distribution')\nsns.histplot(df['brain_wt'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Brain Weight Distribution'}, xlabel='brain_wt', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Brain Wight')\nsns.barplot(x='brain_wt', y='species', data=df.sort_values('brain_wt',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Brain Wight'}, xlabel='brain_wt', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"brain_wt\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c5b40bb0>\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(16,6))\nplt.title('Mammals Life Span Distribution')\nsns.histplot(df['life_span'] , kde=True)\n\n\n<AxesSubplot:title={'center':'Mammals Life Span Distribution'}, xlabel='life_span', ylabel='Count'>\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(20,8))\nplt.title('Top 10 Mammals Life Span')\nsns.barplot(x='life_span', y='species', data=df.sort_values('life_span',ascending = False)[:10], palette='pastel')\n\n\n<AxesSubplot:title={'center':'Top 10 Mammals Life Span'}, xlabel='life_span', ylabel='species'>\n\n\n\n\n\n\n\nCode\nsns.jointplot(data=df, x=\"life_span\", y=\"total_sleep\", kind=\"hex\",  height=8)\n\n\n<seaborn.axisgrid.JointGrid at 0x217c6460d60>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Predation Total Sleep Visualization')\nsns.countplot(x='predation',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"predation\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"predation\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"predation\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3907231976.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"predation\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='predation', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Exposure Total Sleep Visualization')\nsns.countplot(x='exposure',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"exposure\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"exposure\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"exposure\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3542283944.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"exposure\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='exposure', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\n\n\nCode\nfigure, axes = plt.subplots(2, 2, sharex=True, figsize=(18,10))\nfigure.suptitle('Danger Total Sleep Visualization')\nsns.countplot(x='danger',data=df,palette='pastel', ax=axes[0][0])\nsns.boxplot(x=\"danger\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[0][1])\nsns.violinplot(x=\"danger\", y=\"total_sleep\", data=df, palette='pastel', ax=axes[1][0])\nsns.stripplot(x=\"danger\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_42120\\3554697531.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x=\"danger\", y=\"total_sleep\", data=df,jitter=True, palette='pastel', ax=axes[1][1])\n\n\n<AxesSubplot:xlabel='danger', ylabel='total_sleep'>\n\n\n\n\n\n\n\n\nx = explanatory / independent variables y = response / dependent variable\n\n\nCode\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x='total_sleep', y='dreaming',data=df)\n\n# Show plot\nplt.title('Sleeping habits')\nplt.ylabel(\"rem sleep per day(hour\")\nplt.xlabel(\"total sleep per day(hour\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.lmplot(x=\"total_sleep\", y=\"dreaming\", data=df, ci=None)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\ndf['total_sleep'].corr(df['dreaming'])\n\n\n0.7270869571641637\n\n\n\n\nCode\ndf['dreaming'].corr(df['total_sleep'])\n\n\n0.7270869571641637\n\n\n\n\n\n\n\nCode\n# Create a scatterplot of gestation vs. total_sleep and show\nsns.scatterplot(x='gestation', y='total_sleep',data=df)\n\n# Show plot\nplt.title('High negative correlation')\nplt.ylabel(\"gestation\")\nplt.xlabel(\"total sleep per day(hour\")\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.scatterplot(x='total_sleep', y='dreaming',data=df)\nplt.title(\"low positive correlation\")\nplt.show()"
  },
  {
    "objectID": "posts/Distribution (pdf, cdf) of iris dataset/Distribution (pdf, cdf) of iris data set.html",
    "href": "posts/Distribution (pdf, cdf) of iris dataset/Distribution (pdf, cdf) of iris data set.html",
    "title": "Distribution (pdf, cdf) of iris data",
    "section": "",
    "text": "Lets explore distribution functions pdf and cdf using Iris data set\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings;\nwarnings.filterwarnings('ignore');\n\n\n\n\nCode\niris=pd.read_csv('iris.csv')\niris.head()\n\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      type\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n    \n  \n\n\n\n\n\n\nCode\niris.shape\n\n\n(150, 5)\n\n\n\n\nCode\niris.columns\n\n\nIndex(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'type'], dtype='object')\n\n\n\n\nCode\niris['type'].value_counts()\n\n\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\nName: type, dtype: int64\n\n\n\n\nCode\nsetosa=iris[iris['type']==\"Iris-setosa\"]\nsetosa['petal_length'].value_counts()\n\n\n1.5    14\n1.4    12\n1.3     7\n1.6     7\n1.7     4\n1.2     2\n1.9     2\n1.1     1\n1.0     1\nName: petal_length, dtype: int64\n\n\n\n\n\n\nCode\niris.plot(kind='scatter',x='sepal_length',y='sepal_width');\nplt.show()\n\n\n\n\n\n\n\nCode\n#here we plot the scatter diagram with colour coding\nsns.set_style('whitegrid')\nsns.FacetGrid(iris,hue=\"type\",aspect = 2).map(plt.scatter,\"sepal_length\",\"sepal_width\").add_legend()\nplt.show()\n\n\n\n\n\n\n\n\nFor cross-referencing\n\n\nCode\nplt.close()\nsns.set_style(\"whitegrid\")\nsns.pairplot(iris,hue=\"type\",size=3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsetosa=iris.loc[iris[\"type\"]==\"Iris-setosa\"]\nversicolor=iris.loc[iris[\"type\"]==\"Iris-versicolor\"]\nvirginica=iris.loc[iris[\"type\"]==\"Iris-virginica\"]\n\n\n\n\nCode\nplt.plot(setosa[\"petal_length\"],np.zeros_like(setosa['petal_length']), 'o')\nplt.plot(versicolor[\"petal_length\"],np.zeros_like(versicolor['petal_length']), 'o')\nplt.plot(virginica[\"petal_length\"],np.zeros_like(virginica['petal_length']), 'o')\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.distplot(iris[iris['type']== 'Iris-setosa']['petal_length'])\n\n\n<AxesSubplot:xlabel='petal_length', ylabel='Density'>\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\",aspect = 2).map(sns.distplot, \"petal_length\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect = 2).map(sns.distplot, \"petal_width\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect=2).map(sns.distplot, \"sepal_length\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(iris, hue=\"type\", aspect=2).map(sns.distplot, \"sepal_width\") \\\n   .add_legend();\nplt.show();\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = False)\n\nprint(\"histogram counts\\n\",counts)\npdf = counts/(sum(counts))\nprint(\"Sum of count is\\n\",sum(counts))\nprint(\"bin edges \\n\",bin_edges)\nprint(\"PDF below \\n\",pdf);\n\nplt.gca().legend(('Pdf'))\nplt.title('PDF and PDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.plot(bin_edges[1:],pdf)\n\n\nhistogram counts\n [ 1  1  2  7 12 14  7  4  0  2]\nSum of count is\n 50\nbin edges \n [1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\nPDF below \n [0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n\n\n[<matplotlib.lines.Line2D at 0x1a5ae102100>]\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = False)\n\nprint(\"histogram counts\\n\",counts)\npdf = counts/(sum(counts))\nprint(\"Sum of count is\\n\",sum(counts))\nprint(\"bin edges \\n\",bin_edges)\nprint(\"PDF is below  \\n\",pdf);\n\ncdf = np.cumsum(pdf)\nprint(\"CDF is below\\n\",cdf)\nplt.gca().legend(('Cdf'))\nplt.title('CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\n\nplt.plot(bin_edges[1:],cdf)\n\n\nhistogram counts\n [ 1  1  2  7 12 14  7  4  0  2]\nSum of count is\n 50\nbin edges \n [1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\nPDF is below  \n [0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\nCDF is below\n [0.02 0.04 0.08 0.22 0.46 0.74 0.88 0.96 0.96 1.  ]\n\n\n[<matplotlib.lines.Line2D at 0x1a5ae1721c0>]\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\n\nprint(counts)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\n\n#compute CDF\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\nplt.gca().legend(('Pdf','Cdf'))\nplt.title('PDF and CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.show();\n\n\n[0.22222222 0.22222222 0.44444444 1.55555556 2.66666667 3.11111111\n 1.55555556 0.88888889 0.         0.44444444]\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\n\n\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=20,\n                                 density = True)\npdf = counts/(sum(counts))\nplt.plot(bin_edges[1:],pdf);\nplt.gca().legend(('Pdf','Cdf','bin edges'))\nplt.title('PDF and CDF For iris_setosa')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\nplt.show();\n\n\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n\n\n\n\n\n\n\n\n\n\nCode\ncounts, bin_edges = np.histogram(setosa['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\n\n# virginica\ncounts, bin_edges = np.histogram(virginica['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\n\n#versicolor\ncounts, bin_edges = np.histogram(versicolor['petal_length'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\nprint(pdf);\nprint(bin_edges)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:], cdf)\n\nplt.title('PDF and CDF For iris_versicolor')\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Percentage\")\n\nplt.show();\n\n\n[0.02 0.02 0.04 0.14 0.24 0.28 0.14 0.08 0.   0.04]\n[1.   1.09 1.18 1.27 1.36 1.45 1.54 1.63 1.72 1.81 1.9 ]\n[0.02 0.1  0.24 0.08 0.18 0.16 0.1  0.04 0.02 0.06]\n[4.5  4.74 4.98 5.22 5.46 5.7  5.94 6.18 6.42 6.66 6.9 ]\n[0.02 0.04 0.06 0.04 0.16 0.14 0.12 0.2  0.14 0.08]\n[3.   3.21 3.42 3.63 3.84 4.05 4.26 4.47 4.68 4.89 5.1 ]\n\n\n\n\n\n\n\n\n\n\nCode\n#Mean, Variance, Std-deviation,\nprint(\"Means:\")\nprint(np.mean(setosa[\"petal_length\"]))\n#Mean with an outlier.\nprint(np.mean(np.append(setosa[\"petal_length\"],50)));\nprint(np.mean(virginica[\"petal_length\"]))\nprint(np.mean(versicolor[\"petal_length\"]))\n\nprint(\"\\nStd-dev:\");\nprint(np.std(setosa[\"petal_length\"]))\nprint(np.std(virginica[\"petal_length\"]))\nprint(np.std(versicolor[\"petal_length\"]))\n\n\nMeans:\n1.464\n2.4156862745098038\n5.5520000000000005\n4.26\n\nStd-dev:\n0.17176728442867112\n0.546347874526844\n0.4651881339845203\n\n\n\n\n\n\n\nCode\n#Median, Quantiles, Percentiles, IQR.\nprint(\"\\nMedians:\")\nprint(np.median(setosa[\"petal_length\"]))\n#Median with an outlier\nprint(np.median(np.append(setosa[\"petal_length\"],50)));\nprint(np.median(virginica[\"petal_length\"]))\nprint(np.median(versicolor[\"petal_length\"]))\n\n\nprint(\"\\nQuantiles:\")\nprint(np.percentile(setosa[\"petal_length\"],np.arange(0, 100, 25)))\nprint(np.percentile(virginica[\"petal_length\"],np.arange(0, 100, 25)))\nprint(np.percentile(versicolor[\"petal_length\"], np.arange(0, 100, 25)))\n\nprint(\"\\n90th Percentiles:\")\nprint(np.percentile(setosa[\"petal_length\"],90))\nprint(np.percentile(virginica[\"petal_length\"],90))\nprint(np.percentile(versicolor[\"petal_length\"], 90))\n\nfrom statsmodels import robust\nprint (\"\\nMedian Absolute Deviation\")\nprint(robust.mad(setosa[\"petal_length\"]))\nprint(robust.mad(virginica[\"petal_length\"]))\nprint(robust.mad(versicolor[\"petal_length\"]))\n\n\n\nMedians:\n1.5\n1.5\n5.55\n4.35\n\nQuantiles:\n[1.    1.4   1.5   1.575]\n[4.5   5.1   5.55  5.875]\n[3.   4.   4.35 4.6 ]\n\n90th Percentiles:\n1.7\n6.31\n4.8\n\nMedian Absolute Deviation\n0.14826022185056031\n0.6671709983275211\n0.5189107764769602\n\n\n\n\n\n\n\nCode\nsns.boxplot(x='type',y='petal_length', data=iris)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.violinplot(x=\"type\", y=\"petal_length\", data=iris, size=8)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nsns.jointplot(x=\"petal_length\", y=\"petal_width\", data=setosa, kind=\"kde\");\nplt.show();"
  },
  {
    "objectID": "posts/Predictions and model objects/Predictions and model objects.html",
    "href": "posts/Predictions and model objects/Predictions and model objects.html",
    "title": "Predictions and model objects in linear regression",
    "section": "",
    "text": "This article explores how linear regression models can be used to predict Taiwanese house prices and Facebook advert clicks. Our regression skills will also be developed through the use of hands-on model objects, as well as the concept of “regression to the mean” and how to transform variables within a dataset.\nThis Simple Linear Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nPredictions can be made using statistical models like linear regression. In other words, you specify each explanatory variable, feed it into the model, and get a prediction.\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847\n    \n  \n\n\n\n\n\n\nCode\n# Create the explanatory_data\nexplanatory_data = pd.DataFrame({'n_convenience': np.arange(0,10)})\n\n# Print it\nprint(explanatory_data)\n\n\n   n_convenience\n0              0\n1              1\n2              2\n3              3\n4              4\n5              5\n6              6\n7              7\n8              8\n9              9\n\n\n\n\nCode\nmdl_price_vs_conv=ols(\"price_twd_msq ~ n_convenience\",data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Create explanatory_data\nexplanatory_data = pd.DataFrame({'n_convenience': np.arange(0, 11)})\n\n# Use mdl_price_vs_conv to predict with explanatory_data, call it price_twd_msq\nprice_twd_msq = mdl_price_vs_conv.predict(explanatory_data)\n\n# Print it\nprint(price_twd_msq)\n\n\n0      8.224237\n1      9.022317\n2      9.820397\n3     10.618477\n4     11.416556\n5     12.214636\n6     13.012716\n7     13.810795\n8     14.608875\n9     15.406955\n10    16.205035\ndtype: float64\n\n\n\n\nCode\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_conv.predict(explanatory_data))\n\n# Print the result\nprint(prediction_data)\n\n\n    n_convenience  price_twd_msq\n0               0       8.224237\n1               1       9.022317\n2               2       9.820397\n3               3      10.618477\n4               4      11.416556\n5               5      12.214636\n6               6      13.012716\n7               7      13.810795\n8               8      14.608875\n9               9      15.406955\n10             10      16.205035\n\n\n\n\n\nThe prediction DataFrame you created contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values\n\n\nCode\n# Create a new figure, fig\nfig = plt.figure()\n\nsns.regplot(x=\"n_convenience\",\n            y=\"price_twd_msq\",\n            data=taiwan_real_estate,\n            ci=None)\n# Add a scatter plot layer to the regplot\nsns.scatterplot(x='n_convenience',y='price_twd_msq',data=prediction_data,color='red',marker='s')\n\n# Show the layered plot\nplt.show()\nprint(\"\\n the predicted points lie on the trend lin\")\n\n\n\n\n\n\n the predicted points lie on the trend lin\n\n\n\n\n\nThe model object created by ols() contains many elements. In order to perform further analysis on the model results, we need to extract its useful bits. The model coefficients, the fitted values, and the residuals are perhaps the most important pieces of the linear model object.\n\n\nCode\n# Print the model parameters of mdl_price_vs_conv\nprint(mdl_price_vs_conv.params)\n\n\nIntercept        8.224237\nn_convenience    0.798080\ndtype: float64\n\n\n\n\nCode\n# Print the fitted values of mdl_price_vs_conv\nprint(mdl_price_vs_conv.fittedvalues)\n\n\n0      16.205035\n1      15.406955\n2      12.214636\n3      12.214636\n4      12.214636\n         ...    \n409     8.224237\n410    15.406955\n411    13.810795\n412    12.214636\n413    15.406955\nLength: 414, dtype: float64\n\n\n\n\nCode\n# Print the residuals of mdl_price_vs_conv\nprint(mdl_price_vs_conv.resid)\n\n\n0     -4.737561\n1     -2.638422\n2      2.097013\n3      4.366302\n4      0.826211\n         ...   \n409   -3.564631\n410   -0.278362\n411   -1.526378\n412    3.670387\n413    3.927387\nLength: 414, dtype: float64\n\n\n\n\nCode\n# Print a summary of mdl_price_vs_conv\nprint(mdl_price_vs_conv.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          price_twd_msq   R-squared:                       0.326\nModel:                            OLS   Adj. R-squared:                  0.324\nMethod:                 Least Squares   F-statistic:                     199.3\nDate:                Thu, 12 Jan 2023   Prob (F-statistic):           3.41e-37\nTime:                        17:11:19   Log-Likelihood:                -1091.1\nNo. Observations:                 414   AIC:                             2186.\nDf Residuals:                     412   BIC:                             2194.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         8.2242      0.285     28.857      0.000       7.664       8.784\nn_convenience     0.7981      0.057     14.118      0.000       0.687       0.909\n==============================================================================\nOmnibus:                      171.927   Durbin-Watson:                   1.993\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1417.242\nSkew:                           1.553   Prob(JB):                    1.78e-308\nKurtosis:                      11.516   Cond. No.                         8.87\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nUsing the model coefficients, you can manually calculate predictions. It’s better to use .predict() when making predictions in real life, but doing it manually is helpful for reassuring yourself that predictions aren’t magic.\nFor simple linear regressions, the predicted value is the intercept plus the slope times the explanatory variable.\nresponse = intercept + slope * explanatory\n\n\nCode\n# Get the coefficients of mdl_price_vs_conv\ncoeffs = mdl_price_vs_conv.params\n\n# Get the intercept\nintercept = coeffs[0]\n\n# Get the slope\nslope = coeffs[1]\n\n# Manually calculate the predictions\nprice_twd_msq = intercept + slope * explanatory_data\nprint(price_twd_msq)\n\n# Compare to the results from .predict()\nprint(price_twd_msq.assign(predictions_auto=mdl_price_vs_conv.predict(explanatory_data)))\n\n\n    n_convenience\n0        8.224237\n1        9.022317\n2        9.820397\n3       10.618477\n4       11.416556\n5       12.214636\n6       13.012716\n7       13.810795\n8       14.608875\n9       15.406955\n10      16.205035\n    n_convenience  predictions_auto\n0        8.224237          8.224237\n1        9.022317          9.022317\n2        9.820397          9.820397\n3       10.618477         10.618477\n4       11.416556         11.416556\n5       12.214636         12.214636\n6       13.012716         13.012716\n7       13.810795         13.810795\n8       14.608875         14.608875\n9       15.406955         15.406955\n10      16.205035         16.205035\n\n\n\n\n\n\nResponse value = fitted value + residual\n“The stuff one can explain” + “the stuff once couldn’t explain”\nResiduals exist due to problems in model and fundamental randomness\nExtreme cases are often due to randomness\nRegression to mean indicated extreme cases don’t persist over time\n\n\n\nCode\nsp500_yearly_returns=pd.read_csv(\"dataset/sp500_yearly_returns.csv\")\nsp500_yearly_returns.head()\n\n\n\n\n\n\n  \n    \n      \n      symbol\n      return_2018\n      return_2019\n    \n  \n  \n    \n      0\n      AAPL\n      -0.053902\n      0.889578\n    \n    \n      1\n      MSFT\n      0.207953\n      0.575581\n    \n    \n      2\n      AMZN\n      0.284317\n      0.230278\n    \n    \n      3\n      FB\n      -0.257112\n      0.565718\n    \n    \n      4\n      GOOGL\n      -0.008012\n      0.281762\n    \n  \n\n\n\n\n\n\nCode\n# Create a new figure, fig\nfig = plt.figure()\n\n# Plot the first layer: y = x\nplt.axline(xy1=(0,0), slope=1, linewidth=2, color=\"green\")\n\n# Add scatter plot with linear regression trend line\nsns.regplot(x='return_2018',y='return_2019',data=sp500_yearly_returns,ci=None,line_kws={'color':'black'})\n\n# Set the axes so that the distances along the x and y axes look the same\nplt.axis(\"equal\")\n\n# Show the plot\nplt.show()\nprint('\\n The regression trend line looks very different to the y equals x line. As the financial advisors say, \"Past performance is no guarantee of future results.\"')\n\n\n\n\n\n\n The regression trend line looks very different to the y equals x line. As the financial advisors say, \"Past performance is no guarantee of future results.\"\n\n\n\n\n\nLet’s quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019\n\n\nCode\n# Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns\nmdl_returns = ols(\"return_2019 ~ return_2018\",data=sp500_yearly_returns).fit()\n\n# Print the parameters\nprint(mdl_returns.params)\n\n# Create a DataFrame with return_2018 at -1, 0, and 1\nexplanatory_data = pd.DataFrame({'return_2018':[-1,0,1]})\n\n# Use mdl_returns to predict with explanatory_data\nprint(mdl_returns.predict(explanatory_data))\n\nprint(\"\\n Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019\")\n\n\nIntercept      0.321321\nreturn_2018    0.020069\ndtype: float64\n0    0.301251\n1    0.321321\n2    0.341390\ndtype: float64\n\n Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019\n\n\n\n\n\nWhen there is no straight-line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both. Let’s transform the explanatory variable.\nWe’ll look at the Taiwan real estate dataset again, but we’ll use the distance to the nearest MRT (metro) station as the explanatory variable. By taking the square root, you’ll shorten the distance to the metro station for commuters.\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\nplt.figure()\n\n# Plot using the transformed variable\nsns.regplot(x='sqrt_dist_to_mrt_m',y='price_twd_msq',data=taiwan_real_estate)\nplt.show()\n\n\n\n\n\n\n\nCode\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\nprint(mdl_price_vs_dist.params)\n\n\nIntercept             16.709799\nsqrt_dist_to_mrt_m    -0.182843\ndtype: float64\n\n\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\nexplanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n\n# Create prediction_data by adding a column of predictions to explantory_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n)\n\n# Print the result\nprint(prediction_data)\n\n\n   sqrt_dist_to_mrt_m  dist_to_mrt_m  price_twd_msq\n0                 0.0              0      16.709799\n1                10.0            100      14.881370\n2                20.0            400      13.052942\n3                30.0            900      11.224513\n4                40.0           1600       9.396085\n5                50.0           2500       7.567656\n6                60.0           3600       5.739227\n7                70.0           4900       3.910799\n8                80.0           6400       2.082370\n\n\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\n# Use this explanatory data\nexplanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n\n# Use mdl_price_vs_dist to predict explanatory_data\nprediction_data = explanatory_data.assign(\n    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n)\n\nfig = plt.figure()\nsns.regplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n\n# Add a layer of your prediction points\nsns.scatterplot(data=prediction_data, x='sqrt_dist_to_mrt_m', y='price_twd_msq', color='red')\nplt.show()\n\nprint(\"\\n By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate mode\")\n\n\n\n\n\n\n By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate mode\n\n\n\n\n\nThe response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. That is, you “back transform” the predictions\n\n\nCode\nad_conversion=pd.read_csv('dataset/ad_conversion.csv')\nad_conversion.head()\n\n\n\n\n\n\n  \n    \n      \n      spent_usd\n      n_impressions\n      n_clicks\n    \n  \n  \n    \n      0\n      1.43\n      7350\n      1\n    \n    \n      1\n      1.82\n      17861\n      2\n    \n    \n      2\n      1.25\n      4259\n      1\n    \n    \n      3\n      1.29\n      4133\n      1\n    \n    \n      4\n      4.77\n      15615\n      3\n    \n  \n\n\n\n\n\n\nCode\n# Create qdrt_n_impressions and qdrt_n_clicks\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion['n_impressions'] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion['n_clicks'] ** 0.25\n\nplt.figure()\n\n# Plot using the transformed variables\nsns.regplot(x='qdrt_n_impressions',y='qdrt_n_clicks',data=ad_conversion,ci=None)\nplt.show()\n\n\n\n\n\n\n\nCode\n# From previous step\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n\n# Run a linear regression of your transformed variables\nmdl_click_vs_impression = ols('qdrt_n_clicks ~ qdrt_n_impressions', data=ad_conversion).fit()\nprint(mdl_click_vs_impression.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          qdrt_n_clicks   R-squared:                       0.945\nModel:                            OLS   Adj. R-squared:                  0.944\nMethod:                 Least Squares   F-statistic:                 1.590e+04\nDate:                Thu, 12 Jan 2023   Prob (F-statistic):               0.00\nTime:                        17:11:20   Log-Likelihood:                 193.90\nNo. Observations:                 936   AIC:                            -383.8\nDf Residuals:                     934   BIC:                            -374.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.0717      0.017      4.171      0.000       0.038       0.106\nqdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n==============================================================================\nOmnibus:                       11.447   Durbin-Watson:                   0.568\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\nSkew:                          -0.216   Prob(JB):                      0.00490\nKurtosis:                       2.707   Cond. No.                         52.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# From previous steps\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n\nmdl_click_vs_impression = ols(\"qdrt_n_clicks ~ qdrt_n_impressions\", data=ad_conversion, ci=None).fit()\n\n# Use this explanatory data\nexplanatory_data = pd.DataFrame({\"qdrt_n_impressions\": np.arange(0, 3e6+1, 5e5) ** .25,\n                                 \"n_impressions\": np.arange(0, 3e6+1, 5e5)})\n\n# Complete prediction_data\nprediction_data = explanatory_data.assign(\n    qdrt_n_clicks = mdl_click_vs_impression.predict(explanatory_data)\n)\n\n# Print the result\nprint(prediction_data)\nprint(\"\\n Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your result\")\n\n\n   qdrt_n_impressions  n_impressions  qdrt_n_clicks\n0            0.000000            0.0       0.071748\n1           26.591479       500000.0       3.037576\n2           31.622777      1000000.0       3.598732\n3           34.996355      1500000.0       3.974998\n4           37.606031      2000000.0       4.266063\n5           39.763536      2500000.0       4.506696\n6           41.617915      3000000.0       4.713520\n\n Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your result\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\base\\model.py:127: ValueWarning: unknown kwargs ['ci']\n  warnings.warn(msg, ValueWarning)\n\n\n\n\n\nIn the previous section, we transformed the response variable, ran a regression, and made predictions. However, we are not yet finished! We will need to perform a back-transformation in order to interpret and visualize your predictions correctly.\n\n\nCode\n# Back transform qdrt_n_clicks\nprediction_data[\"n_clicks\"] = prediction_data['qdrt_n_clicks'] ** 4\nprint(prediction_data)\n\n\n   qdrt_n_impressions  n_impressions  qdrt_n_clicks    n_clicks\n0            0.000000            0.0       0.071748    0.000026\n1           26.591479       500000.0       3.037576   85.135121\n2           31.622777      1000000.0       3.598732  167.725102\n3           34.996355      1500000.0       3.974998  249.659131\n4           37.606031      2000000.0       4.266063  331.214159\n5           39.763536      2500000.0       4.506696  412.508546\n6           41.617915      3000000.0       4.713520  493.607180\n\n\n\n\nCode\n# Plot the transformed variables\nfig = plt.figure()\nsns.regplot(x=\"qdrt_n_impressions\", y=\"qdrt_n_clicks\", data=ad_conversion, ci=None)\n\n# Add a layer of your prediction points\nsns.scatterplot(data=prediction_data, x='qdrt_n_impressions', y='qdrt_n_clicks', color='red')\nplt.show()\nprint(\"\\n Notice that your back-transformed predictions nicely follow the trend line and allow you to make more accurate predictions\")\n\n\n\n\n\n\n Notice that your back-transformed predictions nicely follow the trend line and allow you to make more accurate predictions"
  },
  {
    "objectID": "posts/Quantifying model fit/Quantifying model fit.html",
    "href": "posts/Quantifying model fit/Quantifying model fit.html",
    "title": "Assessing model fit",
    "section": "",
    "text": "What questions to ask your model to determine its fit. We will discuss how to quantify how well a linear regression model fits, how to diagnose problems with the model using visualizations, and how each observation impacts the model.\nThis Assessing model fit is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n\n\n\nAnalyze and assess the accuracy of model predictions.\nCoefficient of determination: R-squared (1 is the best, 0 is as good as randomness).\nThe proportion of variance in the response variable that is predictable (explainable) by the explanatory variable. This information indicates whether the model at hand is effective in resuming our data or not. Data, context, and the way we transform variables heavily impact r-squared interpretation.\nAccessible inside .summary() or .rsquared\nResidual standard error (RSE)\nThe residual is the difference between the predicted and observed response values (the distance). It has the same unit as the response.\nMSE = RSE**2 RSE = np.sqrt(MSE)\nAccessible with .mse_resid()\nRSE is calculated manually by taking the square of each residual. The degrees of freedom are calculated (# of observations minus # of model coefficients). Then we take the square root of the sum divided by the deg_freedom.\nRoot mean square error\nUnlike MSE, we do not remove degrees of freedom (we divide only by the number of observations).\n\n\n\nA coefficient of determination measures how well the linear regression line fits the observed values. It is equal to the square root of the correlation between the explanatory and response variables in a simple linear regression.\n\n\nCode\n# fetch data for which model is created\nad_conversion=pd.read_csv('dataset/ad_conversion.csv')\nad_conversion.head()\n\n\n\n\n\n\n  \n    \n      \n      spent_usd\n      n_impressions\n      n_clicks\n    \n  \n  \n    \n      0\n      1.43\n      7350\n      1\n    \n    \n      1\n      1.82\n      17861\n      2\n    \n    \n      2\n      1.25\n      4259\n      1\n    \n    \n      3\n      1.29\n      4133\n      1\n    \n    \n      4\n      4.77\n      15615\n      3\n    \n  \n\n\n\n\n\n\nCode\n# click vs impression model\nmdl_click_vs_impression_orig = ols('n_clicks ~ n_impressions' , data = ad_conversion).fit()\n# Print a summary of mdl_click_vs_impression_orig\nprint(mdl_click_vs_impression_orig.summary())\n\n# Create qdrt_n_impressions and qdrt_n_clicks\nad_conversion[\"qdrt_n_impressions\"] = ad_conversion['n_impressions'] ** 0.25\nad_conversion[\"qdrt_n_clicks\"] = ad_conversion['n_clicks'] ** 0.25\n\n# qdrnt click vs impression model\nmdl_click_vs_impression_trans = ols('qdrt_n_clicks ~ qdrt_n_impressions  ' , data = ad_conversion).fit()\n# Print a summary of mdl_click_vs_impression_trans\nprint(mdl_click_vs_impression_trans.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               n_clicks   R-squared:                       0.892\nModel:                            OLS   Adj. R-squared:                  0.891\nMethod:                 Least Squares   F-statistic:                     7683.\nDate:                Fri, 13 Jan 2023   Prob (F-statistic):               0.00\nTime:                        09:10:19   Log-Likelihood:                -4126.7\nNo. Observations:                 936   AIC:                             8257.\nDf Residuals:                     934   BIC:                             8267.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         1.6829      0.789      2.133      0.033       0.135       3.231\nn_impressions     0.0002   1.96e-06     87.654      0.000       0.000       0.000\n==============================================================================\nOmnibus:                      247.038   Durbin-Watson:                   0.870\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            13215.277\nSkew:                          -0.258   Prob(JB):                         0.00\nKurtosis:                      21.401   Cond. No.                     4.88e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.88e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          qdrt_n_clicks   R-squared:                       0.945\nModel:                            OLS   Adj. R-squared:                  0.944\nMethod:                 Least Squares   F-statistic:                 1.590e+04\nDate:                Fri, 13 Jan 2023   Prob (F-statistic):               0.00\nTime:                        09:10:19   Log-Likelihood:                 193.90\nNo. Observations:                 936   AIC:                            -383.8\nDf Residuals:                     934   BIC:                            -374.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              0.0717      0.017      4.171      0.000       0.038       0.106\nqdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n==============================================================================\nOmnibus:                       11.447   Durbin-Watson:                   0.568\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\nSkew:                          -0.216   Prob(JB):                      0.00490\nKurtosis:                       2.707   Cond. No.                         52.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# Print the coeff of determination for mdl_click_vs_impression_orig\nprint(mdl_click_vs_impression_orig.rsquared)\n\n# Print the coeff of determination for mdl_click_vs_impression_trans\nprint(mdl_click_vs_impression_trans.rsquared)\n\nprint(\"\\n The transformed model has a higher coefficient of determination than the original model, suggesting that it gives a better fit to the data.\")\n\n\n0.8916134973508041\n0.9445272817143905\n\n The transformed model has a higher coefficient of determination than the original model, suggesting that it gives a better fit to the data.\n\n\n\n\n\nThe residual standard error (RSE) measures the typical residual size. Predictions are measured by how wrong they can be. The data fits better with smaller numbers, with zero being perfect\n\n\nCode\n# Calculate mse_orig for mdl_click_vs_impression_orig\nmse_orig = mdl_click_vs_impression_orig.mse_resid\n\n# Calculate rse_orig for mdl_click_vs_impression_orig and print it\nrse_orig = np.sqrt(mse_orig)\nprint(\"RSE of original model: \", rse_orig)\n\n# Calculate mse_trans for mdl_click_vs_impression_trans\nmse_trans = mdl_click_vs_impression_trans.mse_resid\n\n# Calculate rse_trans for mdl_click_vs_impression_trans and print it\nrse_trans = np.sqrt(mse_trans)\nprint(\"RSE of transformed model: \", rse_trans)\n\n\nRSE of original model:  19.905838862478134\nRSE of transformed model:  0.19690640896875722\n\n\n\n\n\nIf the model is well fitted, the residuals should be normally distributed along the line/curve, and the mean should be zero. In addition, it indicates when the fitted residuals are positive or negative (above/below the straight line).\nResidual VS fitted values chart\nTrends can be visualized using this tool. The best accuracy is achieved by following the y=0 line. There is a problem if the curve goes all over the place.\nsns.residplot()\nQ-Q Plot\nThe best conditions are validated if the points track along a straight line and are normally distributed. Otherwise, they are not.\nqqplot() (from statsmodels.api import qqplot)\nSquare root of Standardized Residuals VS fitted values, Scale-location plot\nAs the fitted values change, the residuals change in size and whether they become smaller or larger. If it bounces all over or is irregular, it means residuals tend to vary randomly or in an inconsistent manner as fitted values change.\n\n\n\nLet’s draw diagnostic plots using the Taiwan real estate dataset and the model of house prices versus number of convenience stores.\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847\n    \n  \n\n\n\n\n\n\nCode\nmdl_price_vs_conv=ols(\"price_twd_msq ~ n_convenience\",data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Plot the residuals vs. fitted values\nsns.residplot(x='n_convenience', y='price_twd_msq', data=taiwan_real_estate, lowess=True)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Import qqplot\nfrom statsmodels.api import qqplot\n\n# Create the Q-Q plot of the residuals\nqqplot(data=mdl_price_vs_conv.resid, fit=True, line=\"45\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Preprocessing steps\nmodel_norm_residuals = mdl_price_vs_conv.get_influence().resid_studentized_internal\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# Create the scale-location plot\nsns.regplot(x=mdl_price_vs_conv.fittedvalues, y=model_norm_residuals_abs_sqrt, ci=None, lowess=True)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Sqrt of abs val of stdized residuals\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\nprint(\"Above three diagnostic plots are excellent for sanity-checking the quality of your models\")\n\n\nAbove three diagnostic plots are excellent for sanity-checking the quality of your models"
  },
  {
    "objectID": "posts/Quantifying model fit/Quantifying model fit.html#extracting-leverage-and-influence",
    "href": "posts/Quantifying model fit/Quantifying model fit.html#extracting-leverage-and-influence",
    "title": "Assessing model fit",
    "section": "Extracting leverage and influence",
    "text": "Extracting leverage and influence\nLets find leverage and influence for taiwan real estate data\n\n\nCode\n# Create sqrt_dist_to_mrt_m\ntaiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n\n# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\nmdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n\n\n\n\nCode\n# Create summary_info\nsummary_info = mdl_price_vs_dist.get_influence().summary_frame()\nprint(summary_info.head(n=10))\n\n\n   dfb_Intercept  dfb_sqrt_dist_to_mrt_m       cooks_d  standard_resid  \\\n0      -0.094893                0.073542  4.648246e-03       -1.266363   \n1      -0.013981                0.008690  1.216711e-04       -0.262996   \n2       0.025510               -0.009963  6.231096e-04        0.688143   \n3       0.055525               -0.021686  2.939394e-03        1.494602   \n4      -0.000932                0.000518  6.055123e-07       -0.019716   \n5      -0.012257                0.029560  7.976174e-04        0.544490   \n6       0.000592               -0.000187  3.896928e-07        0.017531   \n7       0.010115               -0.006428  6.232088e-05        0.185284   \n8      -0.087118                0.126666  9.060428e-03        0.915959   \n9       0.009041               -0.033610  1.378024e-03       -0.818660   \n\n   hat_diag  dffits_internal  student_resid    dffits  \n0  0.005764        -0.096418      -1.267294 -0.096489  \n1  0.003506        -0.015599      -0.262699 -0.015582  \n2  0.002625         0.035302       0.687703  0.035279  \n3  0.002625         0.076673       1.496850  0.076789  \n4  0.003106        -0.001100      -0.019692 -0.001099  \n5  0.005352         0.039940       0.544024  0.039906  \n6  0.002530         0.000883       0.017509  0.000882  \n7  0.003618         0.011164       0.185067  0.011151  \n8  0.021142         0.134614       0.915780  0.134587  \n9  0.004095        -0.052498      -0.818332 -0.052477  \n\n\n\n\nCode\n# Add the hat_diag column to taiwan_real_estate, name it leverage\ntaiwan_real_estate[\"leverage\"] = summary_info['hat_diag']\n\n# Sort taiwan_real_estate by leverage in descending order and print the head\nprint(taiwan_real_estate.sort_values(by='leverage', ascending=False).head())\n\n\n     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \\\n347       6488.021              1        15 to 30       3.388805   \n116       6396.283              1        30 to 45       3.691377   \n249       6306.153              1        15 to 30       4.538578   \n255       5512.038              1        30 to 45       5.264750   \n8         5512.038              1        30 to 45       5.688351   \n\n     sqrt_dist_to_mrt_m  leverage  \n347           80.548253  0.026665  \n116           79.976765  0.026135  \n249           79.411290  0.025617  \n255           74.243101  0.021142  \n8             74.243101  0.021142  \n\n\n\n\nCode\n# Add the cooks_d column to taiwan_real_estate, name it cooks_dist\ntaiwan_real_estate['cooks_dist'] = summary_info['cooks_d']\n\n# Sort taiwan_real_estate by cooks_dist in descending order and print the head.\nprint(taiwan_real_estate.sort_values(\"cooks_dist\", ascending=False).head())\n\n\n     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \\\n270       252.5822              1         0 to 15      35.552194   \n148      3780.5900              0        15 to 30      13.645991   \n228      3171.3290              0         0 to 15      14.099849   \n220       186.5101              9        30 to 45      23.691377   \n113       393.2606              6         0 to 15       2.299546   \n\n     sqrt_dist_to_mrt_m  leverage  cooks_dist  \n270           15.892835  0.003849    0.115549  \n148           61.486503  0.012147    0.052440  \n228           56.314554  0.009332    0.035384  \n220           13.656870  0.004401    0.025123  \n113           19.830799  0.003095    0.022813"
  },
  {
    "objectID": "posts/Random numbers and probability/Random numbers and probability.html",
    "href": "posts/Random numbers and probability/Random numbers and probability.html",
    "title": "Random Numbers and Probability",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nRandom Numbers and Probability\nThis Random Numbers and Probability is part of Datacamp course: Introduction to Statistic in Python\nHere we’ll explore how to generate random samples and measure chance using probability.We will work with real-world sales data to calculate the probability of a salesperson being successful. Finally, we will try to use the binomial distribution to model events with binary outcomes.\nThis is my learning experience of data science through DataCamp\n\nMeasuring chance\n\\[ P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total # of possible outcomes}} \\]\nSampling with replacement vs sampling without replacement\nsampling without replacement, in which a subset of the observations are selected randomly, and once an observation is selected it cannot be selected again. sampling with replacement, in which a subset of observations are selected randomly, and an observation may be selected more than once\n\n\nCalculating probabilities\nYou’re in charge of the sales team, and it’s time for performance reviews, starting with Amir. As part of the review, you want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals.\nRecall that the probability of an event can be calculated by\n\\[ P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total # of possible outcomes}} \\]\n\n\nCode\namir_deals = pd.read_csv('amir_deals.csv', index_col=0)\namir_deals.head()\n\n\n\n\n\n\n  \n    \n      \n      product\n      client\n      status\n      amount\n      num_users\n    \n  \n  \n    \n      1\n      Product F\n      Current\n      Won\n      7389.52\n      19\n    \n    \n      2\n      Product C\n      New\n      Won\n      4493.01\n      43\n    \n    \n      3\n      Product B\n      New\n      Won\n      5738.09\n      87\n    \n    \n      4\n      Product I\n      Current\n      Won\n      2591.24\n      83\n    \n    \n      5\n      Product E\n      Current\n      Won\n      6622.97\n      17\n    \n  \n\n\n\n\n\n\nCode\ncounts = amir_deals['product'].value_counts()\nprint(counts)\n\n# Calculate probability of picking a deal with each product\nprobs = counts / len(amir_deals['product'])\nprint(probs)\n\n\nProduct B    62\nProduct D    40\nProduct A    23\nProduct C    15\nProduct F    11\nProduct H     8\nProduct I     7\nProduct E     5\nProduct N     3\nProduct G     2\nProduct J     2\nName: product, dtype: int64\nProduct B    0.348315\nProduct D    0.224719\nProduct A    0.129213\nProduct C    0.084270\nProduct F    0.061798\nProduct H    0.044944\nProduct I    0.039326\nProduct E    0.028090\nProduct N    0.016854\nProduct G    0.011236\nProduct J    0.011236\nName: product, dtype: float64\n\n\n\n\nSampling deals\nIn the previous exercise, you counted the deals Amir worked on. Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement.\nAdditionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed before sampling from the deals.\n\n\nCode\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals without replacement\nsample_without_replacement = amir_deals.sample(5,replace=False)\nprint(sample_without_replacement)\n\n\n       product   client status   amount  num_users\n128  Product B  Current    Won  2070.25          7\n149  Product D  Current    Won  3485.48         52\n78   Product B  Current    Won  6252.30         27\n105  Product D  Current    Won  4110.98         39\n167  Product C      New   Lost  3779.86         11\n\n\n\n\nCode\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals with replacement\nsample_with_replacement = amir_deals.sample(5,replace=True)\nprint(sample_with_replacement)\n\n\n       product   client status   amount  num_users\n163  Product D  Current    Won  6755.66         59\n132  Product B  Current    Won  6872.29         25\n88   Product C  Current    Won  3579.63          3\n146  Product A  Current    Won  4682.94         63\n146  Product A  Current    Won  4682.94         63\n\n\n\n\nDiscrete distributions\n\nProbability distribution\n\nDescribe probability of each possible outcome in a scenario\nExpected value: mean of probability distribution\n\nLaw of large number (LLN): as size of sample increases, sample mean will approach expected value.\n\n\n\nCreating a probability distribution\nRestaurant management wants to optimize seating space based on the size of the groups that come most often to a new restaurant. One night, 10 groups of people are waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. This exercise examines the probability of picking groups of different sizes.\nRemember that expected value can be calculated by multiplying each possible outcome with its corresponding probability and taking the sum\n\n\nCode\nrestaurant_groups = pd.read_csv('restaurant_groups.csv')\nrestaurant_groups.head()\n\n\n\n\n\n\n  \n    \n      \n      group_id\n      group_size\n    \n  \n  \n    \n      0\n      A\n      2\n    \n    \n      1\n      B\n      4\n    \n    \n      2\n      C\n      6\n    \n    \n      3\n      D\n      2\n    \n    \n      4\n      E\n      2\n    \n  \n\n\n\n\n\n\nCode\n# Create a histogram of restaurant_groups and show plot\nrestaurant_groups['group_size'].hist(bins=[2, 3, 4, 5, 6])\n\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nCode\n# Create probability distribution\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\nprint(size_dist)\n\n# Expected value\nexpected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\nprint(expected_value)\n\n# Subset groups of size 4 or more\ngroups_4_or_more = size_dist[size_dist['group_size'] >=4]\n\n# Sum the probabilities of groups_4_or_more\nprob_4_or_more = groups_4_or_more['prob'].sum()\nprint(prob_4_or_more)\n\n\n   group_size  prob\n0           2   0.6\n1           4   0.2\n2           6   0.1\n3           3   0.1\n2.9000000000000004\n0.30000000000000004\n\n\n\n\nContinuous distributions\nData back-ups\nYour company’s sales software backs itself up automatically, but no one knows exactly when the back-ups take place. It is known, however, that back-ups occur every 30 minutes. Amir updates the client data after sales meetings at random times. When will his newly-entered data be backed up? Answer Amir’s questions using your new knowledge of continuous uniform distributions\n\n\nCode\nfrom scipy.stats import uniform\n\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 = uniform.cdf(5, min_time, max_time)\nprint(prob_less_than_5)\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\nprint(prob_greater_than_5)\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - \\\n                        uniform.cdf(10, min_time, max_time)\nprint(prob_between_10_and_20)\n\n\n0.16666666666666666\n0.8333333333333334\n0.3333333333333333\n\n\n\n\nSimulating wait times\nTo give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\n\n\nCode\nnp.random.seed(334)\n\n# Generates 1000 wait times between 0 and 30 mins\nwait_times = uniform.rvs(min_time, max_time, 1000)\nprint(wait_times[:10])\n\n# Create a histogram of simulated times and show plot\nplt.hist(wait_times);\nprint (\"Unless Amir figures out exactly what time each backup happens, he won't be able to time his data entry so it gets backed up sooner, but it looks like he'll wait about 15 minutes on average\\n\")\n\n\n[ 7.144097    0.97455866  3.72802787  5.11644319  8.70602482 24.69140099\n 23.98012075  3.19592668 25.1985306  17.89048629]\nUnless Amir figures out exactly what time each backup happens, he won't be able to time his data entry so it gets backed up sooner, but it looks like he'll wait about 15 minutes on average\n\n\n\n\n\n\n\n\nThe binomial distribution\n\nBinomial distribution\n\nProbability distribution of number of successes in a sequence of independent trials\nDescribed by n and p\n\nn: total number of trials\np: probability of success\n\nExpected value: n * p\nIndependence: The binomial distribution is a probability distribution of number of successes in a sequence of independent trials\n\n\n\n\nSimulating sales deals\nAssume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance.\n\n\nCode\n# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate a single deal\nprint(binom.rvs(1, 0.3, size=1))\n\n# Simulate 1 week of 3 deals\nprint(binom.rvs(3,0.3,size=1))\n\n# Simulate 52 weeks of 3 deals\ndeals = binom.rvs(3,0.3,size=52)\n\n# Print mean deals won per week\nprint(np.mean(deals))\n\nprint('\\nIn this simulated year, Amir won 0.83 deals on average each week')\n\n\n[1]\n[0]\n0.8461538461538461\n\nIn this simulated year, Amir won 0.83 deals on average each week\n\n\n\n\nCalculating binomial probabilities\nJust as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution.\n\n\nCode\n# Probability of closing 3 out of 3 deals\nprob_3 = binom.pmf(3,3,0.3)\nprint(prob_3)\n\n# Probability of closing <= 1 deal out of 3 deals\nprob_less_than_or_equal_1 = binom.cdf(1,3,0.3)\nprint(prob_less_than_or_equal_1)\n\n# Probability of closing > 1 deal out of 3 deals\nprob_greater_than_1 =1- binom.cdf(1,3,0.3)\nprint(prob_greater_than_1)\n\nprint(\"\\nAmir has about a 22% chance of closing more than one deal in a week.\")\n\n\n0.026999999999999996\n0.784\n0.21599999999999997\n\nAmir has about a 22% chance of closing more than one deal in a week.\n\n\n\n\nHow many sales will be won?\nNow Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by n*p\n\n\nCode\n# Expected number won with 30% win rate\nwon_30pct = 3 * 0.3\nprint(won_30pct)\n\n# Expected number won with 25% win rate\nwon_25pct = 3 * 0.25\nprint(won_25pct)\n\n# Expected number won with 35% win rate\nwon_35pct = 3 * 0.35\nprint(won_35pct)\n\nprint('\\nIf Amirs win rate goes up by 5%, he can expect to close more than 1 deal on average each week')\n\n\n0.8999999999999999\n0.75\n1.0499999999999998\n\nIf Amirs win rate goes up by 5%, he can expect to close more than 1 deal on average each week"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html",
    "title": "Simple Linear Regression Modeling",
    "section": "",
    "text": "We will learn the basics of this popular statistical model, what regression is, and how linear and logistic regressions differ. We’ll then learn how to fit simple linear regression models with numeric and categorical explanatory variables, and how to describe the relationship between the response and explanatory variables using model coefficients\nThis Simple Linear Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntaiwan_real_estate=pd.read_csv(\"dataset/taiwan_real_estate2.csv\")\ntaiwan_real_estate.head()\n\n\n\n\n\n\n  \n    \n      \n      dist_to_mrt_m\n      n_convenience\n      house_age_years\n      price_twd_msq\n    \n  \n  \n    \n      0\n      84.87882\n      10\n      30 to 45\n      11.467474\n    \n    \n      1\n      306.59470\n      9\n      15 to 30\n      12.768533\n    \n    \n      2\n      561.98450\n      5\n      0 to 15\n      14.311649\n    \n    \n      3\n      561.98450\n      5\n      0 to 15\n      16.580938\n    \n    \n      4\n      390.56840\n      5\n      0 to 15\n      13.040847"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#fitting-a-linear-regression",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#fitting-a-linear-regression",
    "title": "Simple Linear Regression Modeling",
    "section": "Fitting a linear regression",
    "text": "Fitting a linear regression\nStraight lines are defined by two things:\n\nIntercept: The y value at the point when x is zero.\nSlope: The amount the y value increases if you increase x by one.\nEquation: y = intercept + slope ∗ x"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#linear-regression-with-ols",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#linear-regression-with-ols",
    "title": "Simple Linear Regression Modeling",
    "section": "Linear regression with ols()",
    "text": "Linear regression with ols()\nWhile sns.regplot() can display a linear regression trend line, it doesn’t give you access to the intercept and slope as variables, or allow you to work with the model results as variables. That means that sometimes you’ll need to run a linear regression yourself.\n\n\nCode\n# Import the ols function\nfrom statsmodels.formula.api import ols\n\n# Create the model object\nmdl_price_vs_conv = ols(\"price_twd_msq ~ n_convenience\", data=taiwan_real_estate)\n\n# Fit the model\nmdl_price_vs_conv = mdl_price_vs_conv.fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_conv.params)\n\n\nIntercept        8.224237\nn_convenience    0.798080\ndtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#categorical-explanatory-variables",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#categorical-explanatory-variables",
    "title": "Simple Linear Regression Modeling",
    "section": "Categorical explanatory variables",
    "text": "Categorical explanatory variables\nVariables that categorize observations are known as categorical variables. Known as levels, they have a limited number of values. Gender is a categorical variable that can take two levels: Male or Female.\nNumbers are required for regression analysis. It is therefore necessary to make the results interpretable when a categorical variable is included in a regression model.\nA set of binary variables is created by recoding categorical variables. The recoding process creates a contrast matrix table by “dummy coding”\nThere are two type of data variables: * Quantitative data: refers to amount * Data collected quantitatively represents actual amounts that can be added, subtracted, divided, etc. Quantitative variables can be: * discrete (integer variables): count of individual items in record e.g. No. of players * continuous (ratio variables): continuous / non-finite value measurements e.g. distance, age etc * Categorical: refers to grouping There are three types of categorical variables: * binary: yes / no e.g. head/tail of coin flip * nominal: group with no rank or order b/w them e.g. color, brand, species etc * ordinal: group that can be ranked in specific order e.g. rating scale in survey result"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#visualizing-numeric-vs.-categorical",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#visualizing-numeric-vs.-categorical",
    "title": "Simple Linear Regression Modeling",
    "section": "Visualizing numeric vs. categorical",
    "text": "Visualizing numeric vs. categorical\nIf the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn’t make sense. Instead, a good option is to draw a histogram for each category.\n\n\nCode\n# Histograms of price_twd_msq with 10 bins, split by the age of each house\nsns.displot(data=taiwan_real_estate,\n         x='price_twd_msq',\n         col='house_age_years',\n         bins=10)\n\n# Show the plot\nplt.show()\nprint(\"\\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest.\")\n\n\n\n\n\n\nIt appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest."
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#using-categories-to-calculate-means",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#using-categories-to-calculate-means",
    "title": "Simple Linear Regression Modeling",
    "section": "Using categories to calculate means",
    "text": "Using categories to calculate means\nUsing summary statistics for each category is a good way to explore categorical variables further. Using a categorical variable, you can calculate the mean and median of your response variable. Therefore, you can compare each category in more detail.\n\n\nCode\n# Calculate the mean of price_twd_msq, grouped by house age\nmean_price_by_age = taiwan_real_estate.groupby('house_age_years')['price_twd_msq'].mean()\n\n# Print the result\nprint(mean_price_by_age)\n\n\nhouse_age_years\n0 to 15     12.637471\n15 to 30     9.876743\n30 to 45    11.393264\nName: price_twd_msq, dtype: float64"
  },
  {
    "objectID": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#is-coefficient-of-linear-regression-model-is-mean-of-each-category",
    "href": "posts/Simple linear regression modeling/Simple Linear Regression Modeling.html#is-coefficient-of-linear-regression-model-is-mean-of-each-category",
    "title": "Simple Linear Regression Modeling",
    "section": "Is coefficient of linear regression model is mean of each category?",
    "text": "Is coefficient of linear regression model is mean of each category?\nWhile calculating linear regression with categorical explanatory variable, means of each category will also coefficient of linear regression but this hold true in case with only one categorical variable. Lets verify this\n\n\nCode\n# Create the model, fit it\nmdl_price_vs_age = ols(\"price_twd_msq ~ house_age_years\", data=taiwan_real_estate).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_age.params)\n\n\nIntercept                      12.637471\nhouse_age_years[T.15 to 30]    -2.760728\nhouse_age_years[T.30 to 45]    -1.244207\ndtype: float64\n\n\n\n\nCode\n# Update the model formula to remove the intercept\nmdl_price_vs_age0 = ols(\"price_twd_msq ~ house_age_years + 0\", data=taiwan_real_estate).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_price_vs_age0.params)\nprint(\"\\n The coefficients of the model are just the means of each category you calculated previously. Fantastic job! \")\n\n\nhouse_age_years[0 to 15]     12.637471\nhouse_age_years[15 to 30]     9.876743\nhouse_age_years[30 to 45]    11.393264\ndtype: float64\n\n The coefficients of the model are just the means of each category you calculated previously. Fantastic job!"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html",
    "title": "Simple Logistic Regression Modeling",
    "section": "",
    "text": "Get a better understanding of logistic regression models. We will analyze real-world data to predict the likelihood of a customer closing their bank account in terms of probabilities of success and odds ratios, and quantify the performance of your model using confusion matrices.\nThis Simple Logistic Regression Modeling is part of Datacamp course: Introduction to Regression with statsmodels in Python\nThis is my learning experience of data science through DataCamp\n\n\nCode\n# Import numpy with alias np\nimport numpy as np\n# Import seaborn with alias sns\nimport pandas as pd\nimport seaborn as sns\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n# Import the ols function\nfrom statsmodels.formula.api import ols"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression",
    "title": "Simple Logistic Regression Modeling",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nHow would the response variable be affected if it were binary or logical? It can be Yes/No, 1/0, Blue/Red, etc.\nFor categorical responses, a logistic regression model is another type of generalized linear model.\nAn S curve is drawn to represent the response. Probabilities can be considered to be the fitted values between 0 and 1."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#exploring-the-explanatory-variables",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#exploring-the-explanatory-variables",
    "title": "Simple Logistic Regression Modeling",
    "section": "Exploring the explanatory variables",
    "text": "Exploring the explanatory variables\nIn the case of a logical response variable, all points lie on the y=0 and y=1 lines, making it difficult to determine what is occurring. It was unclear how the explanatory variable was distributed on each line before you saw the trend line. A histogram of the explanatory variable, grouped by the response, can be used to resolve this problem.\nThese histograms will be used to gain an understanding of the financial services churn dataset\n\n\nCode\nchurn = pd.read_csv('dataset/churn.csv')\nprint(churn.head())\n\n\n   has_churned  time_since_first_purchase  time_since_last_purchase\n0            0                  -1.089221                 -0.721322\n1            0                   1.182983                  3.634435\n2            0                  -0.846156                 -0.427582\n3            0                   0.086942                 -0.535672\n4            0                  -1.166642                 -0.672640\n\n\n\n\nCode\n# Create the histograms of time_since_last_purchase split by has_churned\nsns.displot(data=churn,x='time_since_last_purchase',col='has_churned')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Redraw the plot with time_since_first_purchase\nsns.displot(data=churn,x='time_since_first_purchase', col='has_churned')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create the histograms of time_since_last_purchase split by has_churned\nsns.distplot(churn['time_since_last_purchase'])\n\n# Show the plot\nplt.show()\n\nprint(churn['has_churned'].unique())\n\nfor x in churn['has_churned'].unique():\n    values = churn.time_since_last_purchase[churn['has_churned'] == x]\n    sns.distplot(values, hist=False, rug=False)\n\ng = sns.FacetGrid(churn, col='has_churned', hue='has_churned')\np1 = g.map(sns.distplot, 'time_since_last_purchase', hist=True, rug=True).add_legend()\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(churn['time_since_last_purchase'])\n\n\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\1505514929.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\n\n\n[0 1]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\nCode\n# Create the histograms of time_since_first_purchase split by has_churned\nsns.distplot(churn['time_since_first_purchase'])\n\n# Show the plot\nplt.show()\n\nprint(churn['has_churned'].unique())\n\nfor x in churn['has_churned'].unique():\n    values = churn.time_since_last_purchase[churn['has_churned'] == x]\n    sns.distplot(values, hist=False, rug=False)\n\ng = sns.FacetGrid(churn, col='has_churned', hue='has_churned')\np1 = g.map(sns.distplot, 'time_since_first_purchase', hist=True, rug=True).add_legend()\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(churn['time_since_first_purchase'])\n\n\n\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\nC:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_29076\\3605684052.py:11: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(values, hist=False, rug=False)\n\n\n[0 1]\n\n\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\axisgrid.py:848: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(*plot_args, **plot_kwargs)"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#visualizing-liner-and-logistic-model",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#visualizing-liner-and-logistic-model",
    "title": "Simple Logistic Regression Modeling",
    "section": "Visualizing liner and logistic model",
    "text": "Visualizing liner and logistic model\nA logistic regression model can be drawn using regplot() in the same manner as a linear regression without you having to concern yourself with the modeling code. Try drawing both trend lines side by side to see how linear and logistic regressions make different predictions. From the linear model, you should see a linear trend (straight line), whereas from the logistic model, you should see a logistic trend (S-shaped).\n\n\nCode\n# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(data=churn,x='time_since_first_purchase',y='has_churned'\n            ,line_kws={\"color\": \"red\"})\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(x=\"time_since_first_purchase\",\n            y=\"has_churned\",\n            data=churn,\n            ci=None,\n            line_kws={\"color\": \"red\"})\n\n# Draw a logistic regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\nsns.regplot(data=churn,x='time_since_first_purchase', y='has_churned',ci=None,logistic=True,line_kws={\"color\": \"blue\"})\n\nplt.show()\n\nprint(\"\\n The two models give similar predictions in some places, but notice the slight curve in the logistic model trend.\")\n\n\n\n\n\n\n The two models give similar predictions in some places, but notice the slight curve in the logistic model trend."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression-with-logit",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#logistic-regression-with-logit",
    "title": "Simple Logistic Regression Modeling",
    "section": "Logistic regression with logit()",
    "text": "Logistic regression with logit()\nLogistic regression requires another function from statsmodels.formula.api: logit(). It takes the same arguments as ols(): a formula and data argument. You then use .fit() to fit the model to the data.\n\n\nCode\n# Import logit\nfrom statsmodels.formula.api import logit\n\n# Fit a logistic regression of churn vs. length of relationship using the churn dataset\nmdl_churn_vs_relationship = logit('has_churned ~ time_since_first_purchase', data=churn).fit()\n\n# Print the parameters of the fitted model\nprint(mdl_churn_vs_relationship.params)\n\n\nOptimization terminated successfully.\n         Current function value: 0.679663\n         Iterations 4\nIntercept                   -0.015185\ntime_since_first_purchase   -0.354795\ndtype: float64"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#predictions-and-odds-ratios",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#predictions-and-odds-ratios",
    "title": "Simple Logistic Regression Modeling",
    "section": "Predictions and odds ratios",
    "text": "Predictions and odds ratios\nOdds ratios\nTaking the probability that something will happen and dividing it by the probability that it will not happen. It is equal to (P/(1-P)). Probability in favor of / against. The data cannot be compared with the original data, but can instead be plotted using a special chart. This unit represents the probability of … occurring (3 times the probability of…). It is easy to interpret, the data cannot be altered easily, and it is precise.\nLog odds ratio\nIt is a nice property of odds ratios that they can be passed into a log() = linear regression. Data changes that are easy to interpret and precise.\nMost likely Outcome\nAccording to logistic regression, we discuss the rounded most likely outcome (response > 0.5 chance of churning, etc.) since response values can be interpreted as probabilities. This data is very easy to interpret, easy to change, and not precise (rounded).\nProbability\nOriginal data. Easy to interpret, not easy to change data on the fly, and precise.\nProbabilities\nWe will examine each of the four main ways of expressing a logistic regression model’s prediction in the following four exercises. Since the response variable is either “yes” or “no”, you can predict the probability of a “yes”. These probabilities will be calculated and visualized here.\n\n\nCode\nexplanatory_data = pd.DataFrame({'time_since_first_purchase': np.arange(-1.5, 4, .35)})\n\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n  has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n)\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned\n0                      -1.50     0.626448\n1                      -1.15     0.596964\n2                      -0.80     0.566762\n3                      -0.45     0.536056\n4                      -0.10     0.505074\n\n\n\n\nCode\n# Create prediction_data\nprediction_data = explanatory_data.assign(\n    has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n)\n\nfig = plt.figure()\n\n# Create a scatter plot with logistic trend line\nsns.regplot(x='time_since_first_purchase', y='has_churned', data=churn, ci=None, logistic=True)\n\n# Overlay with prediction_data, colored red\nsns.scatterplot(x='time_since_first_purchase',y='has_churned',\ndata=prediction_data, color='red')\n\nplt.show()"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#most-likely-outcome",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#most-likely-outcome",
    "title": "Simple Logistic Regression Modeling",
    "section": "Most likely outcome",
    "text": "Most likely outcome\nA non-technical audience may appreciate you not discussing probabilities and simply explaining the most likely outcome. Thus, instead of stating that there is a 60% chance of a customer leaving, you state that churn is the most likely outcome. There is a trade-off here between easier interpretation and nuance.\n\n\nCode\n# Update prediction data by adding most_likely_outcome\nprediction_data[\"most_likely_outcome\"] = np.round(prediction_data['has_churned'])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome\n0                      -1.50     0.626448                  1.0\n1                      -1.15     0.596964                  1.0\n2                      -0.80     0.566762                  1.0\n3                      -0.45     0.536056                  1.0\n4                      -0.10     0.505074                  1.0\n\n\n\n\nCode\n# Update prediction data by adding most_likely_outcome\nprediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n\nfig = plt.figure()\n\n# Create a scatter plot with logistic trend line (from previous exercise)\nsns.regplot(x=\"time_since_first_purchase\",\n            y=\"has_churned\",\n            data=churn,\n            ci=None,\n            logistic=True)\n\n# Overlay with prediction_data, colored red\nsns.scatterplot(x='time_since_first_purchase', y='most_likely_outcome', data=prediction_data, color='red')\n\nplt.show()\nprint(\"\\nThe most likely outcome is that you will master logistic regression! Providing the most likely response is a great way to share the model results with a non-technical audience.\")\n\n\n\n\n\n\nThe most likely outcome is that you will master logistic regression! Providing the most likely response is a great way to share the model results with a non-technical audience."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#odds-ratio",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#odds-ratio",
    "title": "Simple Logistic Regression Modeling",
    "section": "Odds ratio",
    "text": "Odds ratio\nAn odds ratio is a measure of the probability of something occurring compared to the probability that it will not occur. Often, this is easier to understand than probabilities, particularly when making decisions regarding choices. If, for example, a customer has a 20% chance of churning, it may be more intuitive to state “the chances of them not churning are four times higher than the chances of them churning.”.\n\n\nCode\n# Update prediction data with odds_ratio\nprediction_data[\"odds_ratio\"] =  prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio\n0                      -1.50     0.626448                  1.0    1.677003\n1                      -1.15     0.596964                  1.0    1.481166\n2                      -0.80     0.566762                  1.0    1.308199\n3                      -0.45     0.536056                  1.0    1.155431\n4                      -0.10     0.505074                  1.0    1.020502\n\n\n\n\nCode\n# Update prediction data with odds_ratio\nprediction_data[\"odds_ratio\"] = prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n\nfig = plt.figure()\n\n# Create a line plot of odds_ratio vs time_since_first_purchase\nsns.lineplot(x='time_since_first_purchase', y='odds_ratio',data=prediction_data)\n\n# Add a dotted horizontal line at odds_ratio = 1\nplt.axhline(y=1, linestyle=\"dotted\")\n\nplt.show()\n\nprint(\"\\nOdds ratios provide an alternative to probabilities that make it easier to compare positive and negative responses\")\n\n\n\n\n\n\nOdds ratios provide an alternative to probabilities that make it easier to compare positive and negative responses"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#log-odds-ratio",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#log-odds-ratio",
    "title": "Simple Logistic Regression Modeling",
    "section": "Log odds ratio",
    "text": "Log odds ratio\nThe disadvantage of probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. It is therefore difficult to understand what happens to the prediction when the explanatory variable is changed. The logarithm of the odds ratio (the “log odds ratio” or “logit”) does exhibit a linear relationship between predicted response and explanatory variable. As the explanatory variable changes, the response metric does not change significantly - only linearly.\nFor visualization purposes, it is usually better to plot the odds ratio and apply a log transformation to the y-axis scale since the actual values of log odds ratio are less intuitive than (linear) odds ratio.\n\n\nCode\n# Update prediction data with log_odds_ratio\nprediction_data[\"log_odds_ratio\"] = np.log(prediction_data['odds_ratio'])\n\n# Print the head\nprint(prediction_data.head())\n\n\n   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio  \\\n0                      -1.50     0.626448                  1.0    1.677003   \n1                      -1.15     0.596964                  1.0    1.481166   \n2                      -0.80     0.566762                  1.0    1.308199   \n3                      -0.45     0.536056                  1.0    1.155431   \n4                      -0.10     0.505074                  1.0    1.020502   \n\n   log_odds_ratio  \n0        0.517008  \n1        0.392830  \n2        0.268651  \n3        0.144473  \n4        0.020295  \n\n\n\n\nCode\n# Update prediction data with log_odds_ratio\nprediction_data[\"log_odds_ratio\"] = np.log(prediction_data[\"odds_ratio\"])\n\nfig = plt.figure()\n\n# Update the line plot: log_odds_ratio vs. time_since_first_purchase\nsns.lineplot(x=\"time_since_first_purchase\",\n             y=\"log_odds_ratio\",\n             data=prediction_data)\n\n# Add a dotted horizontal line at log_odds_ratio = 0\nplt.axhline(y=0, linestyle=\"dotted\")\n\nplt.show()\nprint(\"\\nThe linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about.\")\n\n\n\n\n\n\nThe linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about."
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#section",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#section",
    "title": "Simple Logistic Regression Modeling",
    "section": "",
    "text": "Quantifying logistic regression fit\nResid plot, QQplot & Scale location plot are less useful in the case of logistic regression. Instead, we can use confusion matrices to analyze the fit performance. With True/False positive & negative outcomes. We can also compute metrics based on various ratios.\nAccuracy : proportion of correct predictions. Higher better.\nTN+TP / (TN+FN+FP+TP)\nSensitivity : proportions of observations where the actual response was true and where the model also predicted it was true. Higher better.\nTP / (FN + TP)\nSpecificity : proportions of observations where the actual was false and where the model also predicted it was false. Higher better.\nTN / (TN + FP) Calculating the confusion matrix\nA confusion matrix (occasionally called a confusion table) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.\nTrue positive: The customer churned and the model predicted they would.\nFalse positive: The customer didn't churn, but the model predicted they would.\nTrue negative: The customer didn't churn and the model predicted they wouldn't.\nFalse negative: The customer churned, but the model predicted they wouldn't.\n\n\nCode\n# Get the actual responses\nactual_response = churn[\"has_churned\"]\n\n# Get the predicted responses\npredicted_response = np.round(mdl_churn_vs_relationship.predict())\n\n# Create outcomes as a DataFrame of both Series\noutcomes = pd.DataFrame({\"actual_response\": actual_response,\n                         \"predicted_response\": predicted_response})\n\n# Print the outcomes\nprint(outcomes.value_counts(sort = False))\n\n\nactual_response  predicted_response\n0                0.0                   112\n                 1.0                    88\n1                0.0                    76\n                 1.0                   124\ndtype: int64\n\n\n\n\nCode\nconf_matrix = pd.crosstab(outcomes['actual_response'], outcomes['predicted_response'], rownames=['Actual'], colnames=['Predicted'])\nprint(conf_matrix)\n\n\nPredicted  0.0  1.0\nActual             \n0          112   88\n1           76  124"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#drawing-a-mosaic-plot-of-the-confusion-matrix",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#drawing-a-mosaic-plot-of-the-confusion-matrix",
    "title": "Simple Logistic Regression Modeling",
    "section": "Drawing a mosaic plot of the confusion matrix",
    "text": "Drawing a mosaic plot of the confusion matrix\nWhile calculating the performance matrix might be fun, it would become tedious if you needed multiple confusion matrices of different models. Luckily, the .pred_table() method can calculate the confusion matrix for you.\nAdditionally, you can use the output from the .pred_table() method to visualize the confusion matrix, using the mosaic() function.\n\n\nCode\n# Import mosaic from statsmodels.graphics.mosaicplot\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# Calculate the confusion matrix conf_matrix\nconf_matrix = mdl_churn_vs_relationship.pred_table()\n\n# Print it\nprint(conf_matrix)\n\n# Draw a mosaic plot of conf_matrix\nmosaic(conf_matrix)\nplt.show()\n\n\n[[112.  88.]\n [ 76. 124.]]"
  },
  {
    "objectID": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#measuring-logistic-model-performance",
    "href": "posts/Simple Logistic Regression Modeling/Simple Logistic Regression Modeling.html#measuring-logistic-model-performance",
    "title": "Simple Logistic Regression Modeling",
    "section": "Measuring logistic model performance",
    "text": "Measuring logistic model performance\nAs you know by now, several metrics exist for measuring the performance of a logistic regression model. In this last exercise, you’ll manually calculate accuracy, sensitivity, and specificity.\nAccuracy is the proportion of predictions that are correct.\naccuracy = (TN + TP) / TN + FN + FP + TP\nSensitivity is the proportion of true observations that are correctly predicted by the model as being true\nsensitivity = TP / (TP + FN)\nspecificity is the proportion of false observations that are correctly predicted by the model as being false.\nspecificity = TN / (TN + FP)\n\n\nCode\n# Extract TN, TP, FN and FP from conf_matrix\nTN = conf_matrix[0,0]\nTP = conf_matrix[1,1]\nFN = conf_matrix[1,0]\nFP = conf_matrix[0,1]\n\n# Calculate and print the accuracy\naccuracy = (TN + TP) / (TN + FN + FP + TP)\nprint(\"accuracy: \", accuracy)\n\n# Calculate and print the sensitivity\nsensitivity = TP / (TP + FN)\nprint(\"sensitivity: \", sensitivity)\n\n# Calculate and print the specificity\nspecificity = TN / (TN + FP)\nprint(\"specificity: \", specificity)\n\nprint(\"\\n Using these metrics, it becomes much easier to interpret and compare logistic regression models.\")\n\n\naccuracy:  0.59\nsensitivity:  0.62\nspecificity:  0.56\n\n Using these metrics, it becomes much easier to interpret and compare logistic regression models."
  },
  {
    "objectID": "posts/Summary of statistics/Summary Of Statistics.html",
    "href": "posts/Summary of statistics/Summary Of Statistics.html",
    "title": "Summary Of Statistics",
    "section": "",
    "text": "Datacamp course: Introduction to Statistic in Python\nThe study of statistics involves collecting, analyzing, and interpreting data. You can use it to bring the future into focus and infer answers to tons of questions. How many calls will your support team receive, and how many jeans sizes should you manufacture to fit 95% of the population? Statistical skills are developed in this course, which teaches you how to calculate averages, plot relationships between numeric values, and calculate correlations. In addition, you’ll learn how to conduct a well-designed study using Python to draw your own conclusions.\nCourse Takeaways:\n\nSummary Statistics\nRandom Numbers & Probability\nMore Distributions and the Central Limit Theorem\nCorrelation and Experimental Design\n\n\n\nStatistics - what is it?\n\nStatistics is the practice and study of collecting and analyzing data\nA summary statistic is a fact about or a summary of some data\n\nHow can statistics be used?\n\nDoes a product have a high likelihood of being purchased? People are more likely to purchase the product if they are familiar with it\nIs there an alternative payment system available?\nCan you tell me how many occupants your hotel will have? In what ways can you optimize occupancy?\nTo meet the needs of 95% of the population, how many sizes of jeans should be manufactured?\nCan the same number of each size be produced?\nA/B tests: Which advertisement is more effective in motivating the purchase of a product?\n\n\n\n\nDescriptive: To describe & summarize data e.g. 25% ride bike, 35% take bus ride & 50% drive to work\nInferential : Use sample data to make inferences about a larger population e.g. what percent of people drive to work?\n\n\n\n\n\nNumeric (quantitative)\nContinuous (measured)\n\nairplance speed\ntime spent waiting\n\nDiscrete (counted)\n\nnumber of devices\nnumber of people\n\nCategorical (qualitative)\nNominal (unordered)\n\nsingle / married\ncountry of residence\n\nOrdinal (ordered) agree, disagree, strongly diagree\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\nCode\nfood_consumption=pd.read_csv('food_consumption.csv')\n\n\n\n\nCode\n# Filter for Belgium\nbe_consumption = food_consumption[food_consumption['country']=='Belgium']\n\n# Filter for USA\nusa_consumption = food_consumption[food_consumption['country']=='USA']\n\n# Calculate mean and median consumption in Belgium\nprint(np.mean(be_consumption['consumption']))\nprint(np.median(be_consumption['consumption']))\n\n# Calculate mean and median consumption in USA\nprint(np.mean(usa_consumption['consumption']))\nprint(np.median(usa_consumption['consumption']))\n\n\n42.13272727272727\n12.59\n44.650000000000006\n14.58\n\n\n\n\nCode\n# Subset for Belgium and USA only\nbe_and_usa = food_consumption[(food_consumption['country']=='Belgium') | (food_consumption['country']=='USA')]\n\n# Group by country, select consumption column, and compute mean and median\nprint(be_and_usa.groupby('country')['consumption'].agg([np.mean,np.median]))\n\n\n              mean  median\ncountry                   \nBelgium  42.132727   12.59\nUSA      44.650000   14.58\n\n\nMean vs Median\n\n\nCode\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Subset for food_category equals rice\nrice_consumption = food_consumption[food_consumption['food_category']=='rice']\n\n# Histogram of co2_emission for rice and show plot\nplt.hist(rice_consumption['co2_emission'])\nplt.show()\n\n\n\n\n\n\n\nCode\n# Calculate mean and median of co2_emission with .agg()\nprint(rice_consumption['co2_emission'].agg([np.mean,np.median]))\n\n\nmean      37.591615\nmedian    15.200000\nName: co2_emission, dtype: float64\n\n\n\n\n\n\nVariance: Average distance from each data point to the data’s mean\nStandard Deviation\n\n\n\nCode\n# Calculate the quartiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,5)))\n\n# Calculate the quintiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,6)))\n\n# Calculate the deciles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'],np.linspace(0,1,11)))\n\n\n[   0.        5.21     16.53     62.5975 1712.    ]\n[   0.       3.54    11.026   25.59    99.978 1712.   ]\n[0.00000e+00 6.68000e-01 3.54000e+00 7.04000e+00 1.10260e+01 1.65300e+01\n 2.55900e+01 4.42710e+01 9.99780e+01 2.03629e+02 1.71200e+03]\n\n\n\n\n\nA variable’s variance and standard deviation are two of the most common ways to measure its spread, and you will practice calculating them in this exercise. Spread informs expectations. In other words, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10, they might sell 40 products one day, and one or two the next. Predictions require information like this.\n\n\nCode\n# Print variance and sd of co2_emission for each food_category\nprint(food_consumption.groupby('food_category')['co2_emission'].agg([np.var,np.std]))\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Create histogram of co2_emission for food_category 'beef'\nplt.hist(food_consumption[food_consumption['food_category']=='beef']['co2_emission'])\n# Show plot\nplt.show()\n\n# Create histogram of co2_emission for food_category 'eggs'\nplt.hist(food_consumption[food_consumption['food_category']=='eggs']['co2_emission'])\n# Show plot\nplt.show()\n\n\n                        var         std\nfood_category                          \nbeef           88748.408132  297.906710\ndairy          17671.891985  132.935669\neggs              21.371819    4.622966\nfish             921.637349   30.358481\nlamb_goat      16475.518363  128.356996\nnuts              35.639652    5.969895\npork            3094.963537   55.632396\npoultry          245.026801   15.653332\nrice            2281.376243   47.763754\nsoybeans           0.879882    0.938020\nwheat             71.023937    8.427570\n\n\n\n\n\n\n\n\nFinding outliers using IQR\nOutliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that’s less influenced by outliers. IQR is also often used to find outliers. If a value is less than Q1−1.5×IQRQ1−1.5×IQR or greater than Q3+1.5×IQRQ3+1.5×IQR, it’s considered an outlier.\n\n\nCode\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\nprint(emissions_by_country)\n\n\ncountry\nAlbania      1777.85\nAlgeria       707.88\nAngola        412.99\nArgentina    2172.40\nArmenia      1109.93\n              ...   \nUruguay      1634.91\nVenezuela    1104.10\nVietnam       641.51\nZambia        225.30\nZimbabwe      350.33\nName: co2_emission, Length: 130, dtype: float64\n\n\n\n\nCode\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quantiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n\n# Subset emissions_by_country to find outliers\noutliers = emissions_by_country[(emissions_by_country > upper) | (emissions_by_country < lower)]\nprint(outliers)\n\n\ncountry\nArgentina    2172.4\nName: co2_emission, dtype: float64\n\n\n\n\n\n\nIn this chapter, you’ll learn how to generate random samples and measure chance using probability. You’ll work with real-world sales data to calculate the probability of a salesperson being successful. Finally, you’ll use the binomial distribution to model events with binary outcomes"
  }
]